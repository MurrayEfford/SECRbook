[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The SECR book",
    "section": "",
    "text": "Foreword\nThis book is about the methods for describing animal populations that have come to be called ‘spatially explicit capture–recapture’ or simply ‘spatial capture–recapture’. We use ‘SECR’ as a general label for these data and models.\nSECR data are observations of marked animals at known locations. The observations are from a well-defined regime of spatial sampling, most often with traps, cameras, or some other type of passive detector. SECR models are used to estimate parameters of the animal population, particularly the population density. We focus on ‘closed’ populations whose composition does not change during sampling.",
    "crumbs": [
      "Foreword"
    ]
  },
  {
    "objectID": "index.html#sec-why",
    "href": "index.html#sec-why",
    "title": "The SECR book",
    "section": "Why SECR?",
    "text": "Why SECR?\nNon-spatial capture–recapture methods are highly developed and powerful (Cooch & White, 2023; Otis et al., 1978; Williams et al., 2002). SECR plugs some gaps in non-spatial methods (particularly with respect to density estimation), and has some unexpected benefits:\n\n\nFreedom from edge effects\nEstimation of density with non-spatial capture–recapture is dogged by uncertain edge effects. SECR explicitly accounts for edge effects so density estimates are unbiased.\n\nReduced individual heterogeneity\nUnmodelled individual heterogeneity of detection is a universal source of bias in capture–recapture (e.g., Laake & Collier, 2024). Spatial sampling is a potent source of heterogeneity, due to differential access to detectors. SECR models this component of heterogeneity, which then ceases to be a problem.\n\nScaleable detection model\nThe detection model in SECR is built from components describing the interaction between a single individual and a single detector. Parameter estimates can therefore be used to simulate sampling with novel detector configurations.\n\nCoherent adjustment for effort\nKnown variation in effort, including incomplete use of a detector array, can be modelled without ad hoc covariates.\n\nSpatial pattern (covariates)\nSECR allows density to be modelled as a function of continuous spatial covariates.",
    "crumbs": [
      "Foreword"
    ]
  },
  {
    "objectID": "index.html#why-this-book",
    "href": "index.html#why-this-book",
    "title": "The SECR book",
    "section": "Why this book?",
    "text": "Why this book?\nThe literature of SECR has grown beyond the attention spans and time budgets of most users. Major SECR publications are Efford (2004), Borchers & Efford (2008), Royle et al. (2014), Borchers & Fewster (2016), Sutherland et al. (2019) and Turek et al. (2021).\nThis book provides both a gentle introduction, in the spirit of Cooch & White (2023), and more in-depth treatment of important topics. It is software oriented and therefore unashamedly partial and incomplete. Much of the content is drawn from earlier papers and the documentation of R (R Core Team, 2024) packages. Some topics are yet to be included (e.g., acoustic data) but documentation may be found on the DENSITY website. Others such as partial identity models (Augustine et al., 2018) have yet to be considered at all.\nSECR has become popular for the potential benefits noted above. But are the results reliable? Understanding the real-world performance of SECR is an active research area with its own questions. Are particular field data adequate? Are results robust when assumptions are not strictly met? How can we design better studies? We assemble the evidence in a form that we hope will be useful to practitioners.",
    "crumbs": [
      "Foreword"
    ]
  },
  {
    "objectID": "index.html#organisation",
    "href": "index.html#organisation",
    "title": "The SECR book",
    "section": "Organisation",
    "text": "Organisation\nPart I introduces the concepts of SECR and walks the reader through a simple example. Part II establishes the necessary theory. Part III provides substantial new material on the performance of SECR: Which assumptions really matter? and How should studies be designed? Part IV is a practical guide to SECR modelling with the R package secr. Appendices provide detail on specialised topics such as area and transect searches, spatial mark-resight and non-Euclidean distances.\nWe expect that most readers will start with Part I and thereafter jump to topics of interest. Cross-references are provided to fill in relevant detail that may have been missed.",
    "crumbs": [
      "Foreword"
    ]
  },
  {
    "objectID": "index.html#software",
    "href": "index.html#software",
    "title": "The SECR book",
    "section": "Software",
    "text": "Software\n The R package secr (Efford, 2025) provides most of the functionality we will need. It performs maximum likelihood estimation for a range of closed-population SECR models. The Windows application DENSITY (Efford et al., 2004) has been superceded, although its graphical interface can still come in handy.\nBayesian approaches using Markov chain Monte Carlo (MCMC) methods are a flexible, but generally slower, alternative to maximum likelihood (1.7.4 Estimation). The R package nimbleSCR promises to make MCMC methods for SECR more accessible and faster.\nAdd-on packages extend the capability of secr:\n\n\n\nsecrlinear enables the estimation of linear density (e.g., animals per km) for populations in linear habitats such as stream networks (secrlinear-vignette.pdf).\n\n\n\n\nipsecr fits models by simulation and inverse prediction, rather than maximum likelihood; this is a rigorous way to analyse data from single-catch traps (ipsecr-vignette.pdf). \n\n\n\n\nsecrdesign enables the assessment of alternative study designs by Monte Carlo simulation; scenarios may differ in detector (trap) layout, sampling intensity, and other characteristics (secrdesign-vignette.pdf).\n\n\n\n\nopenCR implements the open-population models of Efford & Schofield (2020).\n\nThese packages are available from the CRAN repository – just open R and type install.packages('xxxx') where xxxx is the package name.\nOther R packages for SECR may be found outside CRAN. A distinct maximum-likelihood implementation by Sutherland et al. (2019) is available on GitHub (https://github.com/jaroyle/oSCR). Open-population packages by Ben Augustine (OpenPopScr) and Richard Glennie (openpopscr) are also available on GitHub.",
    "crumbs": [
      "Foreword"
    ]
  },
  {
    "objectID": "index.html#recommended-citation",
    "href": "index.html#recommended-citation",
    "title": "The SECR book",
    "section": "Recommended citation",
    "text": "Recommended citation\n(to be finalised)\nEfford, M. G. (2025) Spatially explicit capture–recapture. A handbook of statistical methods. Zenodo https://doi.org/10.5281/zenodo.XXXX\nThe book is available online at https://murrayefford.github.io/SECRbook/.\nSee here for the most recent pdf.\nThe Quarto source files, including R code, are at https://github.com/murrayefford/SECRbook.",
    "crumbs": [
      "Foreword"
    ]
  },
  {
    "objectID": "index.html#feedback",
    "href": "index.html#feedback",
    "title": "The SECR book",
    "section": "Feedback",
    "text": "Feedback\nThis is work in progress. If you find an error or would like to make a suggestion, please raise an issue on GitHub or contact the author directly.",
    "crumbs": [
      "Foreword"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "The SECR book",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nBrian Gerber, Joanne Potts and Gurutzeta Guillera-Arroita gave helpful comments on early versions of some chapters. Matt Schofield provided encouragement, answered some theoretical questions, and reviewed several chapters. Thanks to John Boulanger for his collaboration on study design and analysis over many projects.\nThanks for data -\n\nKen Burnham: snowshoe hares 2  Simple example\n\nJared Laufenburg et al. & Great Smoky Mountains NP: black bears 12  Habitat mask\n\nKevin Young: horned lizards Appendix D — Area and transect search\n\nGarth Mowat: Selkirk grizzly bears Appendix F — Non-Euclidean distances\n\nChris Sutherland: non-Euclidean simulation Appendix F — Non-Euclidean distances\n\n\nSee also the links in Appendix L — Datasets.\nCopyright © 2025 Murray Efford\n\n\n\n\n\n\n\n\nThis book is licensed under the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International license.\nMurray Efford\nDunedin, February 2025\n\n\n\n\nAugustine, B. C., Royle, J. A., Kelly, M. J., Satter, C. B., Alonso, R. S., Boydston, E. E., & Crooks, K. R. (2018). Spatial capture–recapture with partial identity: An application to camera traps. The Annals of Applied Statistics, 12(1). https://doi.org/10.1214/17-aoas1091\n\n\nBorchers, D. L., & Efford, M. G. (2008). Spatially explicit maximum likelihood methods for capture-recapture studies. Biometrics, 64, 377–385.\n\n\nBorchers, D. L., & Fewster, R. (2016). Spatial capture–recapture models. Statistical Science, 31(2). https://doi.org/10.1214/16-sts557\n\n\nCooch, E., & White, G. (Eds.). (2023). Program MARK: A gentle introduction. (23rd ed.). http://www.phidot.org/software/mark/docs/book/\n\n\nEfford, M. G. (2004). Density estimation in live-trapping studies. Oikos, 106, 598–610.\n\n\nEfford, M. G. (2025). secr: Spatially explicit capture-recapture models. https://CRAN.R-project.org/package=secr\n\n\nEfford, M. G., Dawson, D. K., & Robbins, C. S. (2004). DENSITY: Software for analysing capture-recapture data from passive detector arrays. Animal Biodiversity and Conservation, 27, 217–228.\n\n\nEfford, M. G., & Schofield, M. R. (2020). A spatial open-population capture-recapture model. Biometrics, 76, 392–402.\n\n\nKendeigh, S. C. (1944). Measurement of bird populations. Ecological Monographs, 14, 67–106. https://doi.org/10.2307/1961632\n\n\nLaake, J. L., & Collier, B. A. (2024). Understanding implications of detection heterogeneity in wildlife abundance estimation. Journal of Wildlife Management, 88, e22516. https://doi.org/10.1002/jwmg.22516\n\n\nOtis, D. L., Burnham, K. P., White, G. C., & Anderson, D. R. (1978). Statistical inference from capture data on closed animal populations. Wildlife Monographs, 62.\n\n\nR Core Team. (2024). R: A language and environment for statistical computing. R Foundation for Statistical Computing. https://www.R-project.org/\n\n\nRoyle, J. A., Chandler, R. B., Sollmann, R., & Gardner, B. (2014). Spatial capture-recapture. Academic Press.\n\n\nSutherland, C., Royle, J. A., & Linden, D. W. (2019). oSCR: A spatial capture-recapture R package for inference about spatial ecological processes. Ecography, 42, 1459–1469. https://doi.org/10.1111/ecog.04551\n\n\nTurek, D., Milleret, C., Ergon, T., Brøseth, H., Dupont, P., Bischof, R., & Valpine, P. de. (2021). Efficient estimation of large‐scale spatial capture–recapture models. Ecosphere, 12(2). https://doi.org/10.1002/ecs2.3385\n\n\nWilliams, B. K., Nichols, J. D., & Conroy, M. J. (2002). Analysis and management of animal populations. Academic Press.",
    "crumbs": [
      "Foreword"
    ]
  },
  {
    "objectID": "01-basics.html",
    "href": "01-basics.html",
    "title": "1  Concepts and terminology",
    "section": "",
    "text": "1.1 Motivation\nOnly a brave ecologist would make this claim today. Measuring animal populations is now justified in more practical terms - the urgent need for evidence to track biodiversity decline or to manage endangered or pest species or those we wish to harvest sustainably. But we rather like Kendeigh’s formulation, to which could be added the lure of the statistical challenges. This book has no more to say about the diverse reasons for measuring populations: we will focus instead on a particular toolkit.\nPopulations of some animal species can be censused by direct observation, but many species are elusive or cryptic. Surveying these species requires indirect methods, often using passive devices such as traps or cameras that accumulate records over time. Passive devices encounter animals as they move around. The scale of that movement is unknown, which creates uncertainty regarding the population that is sampled. Furthermore, the number of observed individuals increases indefinitely as more and more peripheral individuals are encountered.\nSECR cuts through this problem by modelling the spatial scale of detection to obtain an unbiased estimate of the stationary density of individuals.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Concepts and terminology</span>"
    ]
  },
  {
    "objectID": "01-basics.html#motivation",
    "href": "01-basics.html#motivation",
    "title": "1  Concepts and terminology",
    "section": "",
    "text": "“Measurement of the size of animal populations is a full-time job and should be treated as a worthy end in itself.”\nS. Charles Kendeigh (1944)\n\n\n   \n\n\nPhoto: L.L.Getz",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Concepts and terminology</span>"
    ]
  },
  {
    "objectID": "01-basics.html#state-and-observation-models",
    "href": "01-basics.html#state-and-observation-models",
    "title": "1  Concepts and terminology",
    "section": "\n1.2 State and observation models",
    "text": "1.2 State and observation models\nIt helps to think of SECR in two parts: the state model and the observation model (Borchers et al., 2002). The state model describes the biological reality we wish to describe, the animal population. The observation model represents the sampling process by which we learn about the population. The observation model matters only as the lens through which we can achieve a clear view of the population. We treat the state model (spatial population) and observation model (detection process) independently, with rare exceptions (Appendix H).",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Concepts and terminology</span>"
    ]
  },
  {
    "objectID": "01-basics.html#spatial-population",
    "href": "01-basics.html#spatial-population",
    "title": "1  Concepts and terminology",
    "section": "\n1.3 Spatial population",
    "text": "1.3 Spatial population\n An animal population in SECR is a spatial point pattern. Each point represents the location of an animal’s activity centre, often abbreviated AC.\nStatistically, we think of a particular point pattern, a particular distribution of AC, as just one possible outcome of a random process. Run the process again and AC will land in different places and even differ in number. We equate the intensity of the random process with the ecological parameter ‘population density’. This formulation is more challenging than the usual “density = number divided by area”, but it opens the door to rigorous statistical treatment.\nThe usual process model is a 2-dimensional Poisson process (Fig. 1.1). By fitting the SECR model we can estimate the intensity surface of the Poisson process that we represent by D(\\mathbf x) where \\mathbf x stands for the x,y coordinates of a point. Density may be homogeneous (a flat surface, with constant D(\\mathbf x)) or inhomogeneous (spatially varying intensity).\nAn inhomogeneous intensity surface is considered to depend on a vector of parameters \\phi, hence D(\\mathbf x; \\phi). For constant density \\phi is a single value.\n\n\n\n\n\n\nActivity centre vs home range centre\n\n\n\n‘Activity centre’ is often used in preference to ‘home range centre’ because it appears more neutral. ‘Home range’ implies a particular pattern of behaviour: spatial familiarity and repeated use in contrast to nomadism. However, SECR assumes the very pattern of behaviour (persistent use) that distinguishes a home range, and it is safe to use ‘activity centre’ and ‘home range centre’ interchangeably in this context.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Concepts and terminology</span>"
    ]
  },
  {
    "objectID": "01-basics.html#detectors",
    "href": "01-basics.html#detectors",
    "title": "1  Concepts and terminology",
    "section": "\n1.4 Detectors",
    "text": "1.4 Detectors\n\nSECR uses sampling devices (‘detectors’) placed at known locations. We need to recognise individuals whenever they are detected. The accumulated detections of each known individual are its ‘detection history’. Device types differ according to how they affect animal behaviour and the data they collect; each type corresponds to a probability model (Section 3.4).\nDetection may be entirely passive and non-invasive if individuals carry unique natural marks (e.g., pelage patterns) or their DNA can be sampled. Devices that record detections passively are “proximity detectors”. Proximity detectors may be split according to the distribution of the number of detections per animal per occasion (binary, Poisson, or binomial), with binary being the most common1.\nAnimals without natural marks must be marked individually on their first detection. This implies capture and handling. Only devices that hold an animal until it is released can truly be called ‘traps’.2 The probability model for trapping must allow for exclusivity: an animal can be found at only one trap on any occasion, and some traps (‘single-catch’ traps) also exclude other individuals after the first. \n\n\nTable 1.1: Examples of SECR sampling devices\n\n\n\n\n\n\n\n\n\nDevice\nMarks\nDetector type\nExample\n\n\n\nAutomatic camera\nnatural marks: stripes and spots\nproximity\ntiger, Royle et al. (2009)\n\n\n\nHair snag\nmicrosatellite DNA\nproximity\ngrizzly bear, Mowat & Strobeck (2000)\n\n\n\nCage trap\nnumbered ear tag\nsingle-catch trap\nbrushtail possum, Efford et al. (2005)\n\n\n\nUgglan trap\nnumbered ear tag\nmulti-catch trap\nfield vole, Ergon & Gardner (2013)\n\n\n\nMist net\nnumbered leg band\nmulti-catch trap\nred-eyed vireo, Borchers & Efford (2008)",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Concepts and terminology</span>"
    ]
  },
  {
    "objectID": "01-basics.html#sampling-across-time",
    "href": "01-basics.html#sampling-across-time",
    "title": "1  Concepts and terminology",
    "section": "\n1.5 Sampling across time",
    "text": "1.5 Sampling across time\nFor most vertebrates we expect population turnover (births, deaths, AC movement) on a time scale of months or years. Population change is often negligible over shorter spans (days or weeks, depending on the species, time of year etc.). Sampling over shorter spans can therefore treat the size and composition of a population as fixed: it is said to be ‘closed’. This greatly simplifies analysis. We assume closure except when considering breaches of assumptions. \nA set of samples from a closed population comprises a sampling ‘session’. For trap-type detectors there must be multiple ‘occasions’ within a sampling session to obtain recaptures. For proximity-type detectors the role of occasions is more subtle, and data may usually be collapsed to animal- and detector-specific counts. The spatial pattern of binary detections alone is sufficient to obtain an estimate of density (Efford et al., 2009).\n\n\n\n\n\n\nContinuous time\n\n\n\nSome devices such as automatic cameras record data in continuous time. Detection events are stamped with the clock time and date, rather than assigned to discrete occasions. SECR models may be written for continuous time data, and these models have mathematical elegance. They find practical application in some niche cases (e.g. Distiller & Borchers, 2015).\n\n\nIt is not necessary for all detectors to be used on all occasions. Incomplete usage (and other variation in effort per occasion – Efford et al., 2013) may be recorded for each detector and allowed for in the analysis.\nData collected across multiple sessions potentially include the loss of some individuals and recruitment of others. An open population model is the natural way to go (e.g., Efford & Schofield, 2020). However, the complexity of open-population models can be avoided if sessions are treated as independent in a ‘multi-session’ closed population analysis.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Concepts and terminology</span>"
    ]
  },
  {
    "objectID": "01-basics.html#data-structure",
    "href": "01-basics.html#data-structure",
    "title": "1  Concepts and terminology",
    "section": "\n1.6 Data structure",
    "text": "1.6 Data structure\n Data for a single SECR session comprise a 3-dimensional rectangular array with dimensions corresponding to known animals, sampling occasions, and detectors. Data in each cell of the array are usually binary (0/1) but may be integer counts &gt; 1 (e.g., if binary data have been collapsed by occasion). In secr, an R object of class ‘capthist’ holds data in this form, along with the coordinates of the detectors in its ‘traps’ attribute. The user constructs a capthist object from text or spreadsheet input using data entry functions described in Chapter 9.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Concepts and terminology</span>"
    ]
  },
  {
    "objectID": "01-basics.html#model-fitting",
    "href": "01-basics.html#model-fitting",
    "title": "1  Concepts and terminology",
    "section": "\n1.7 Model fitting",
    "text": "1.7 Model fitting\nA SECR model combines a model for the point process (the state model) and a model for distance-dependent detection (the observation model). Unbiased estimates of population density (and other parameters) are obtained by jointly fitting the state and observation models.\n\n1.7.1 Distance-dependent detection\nIn order to estimate density from a sample we must account for the sampling process. The process is inherently spatial: each animal is more likely to be detected near its AC, and less likely to be detected far away. Sampling filters the geographic locations of animals as indicated in Fig. 1.1.\n\n\n\n\n\n\n\nFigure 1.1: Distance-dependent detection of uniformly distributed activity centres (open circles; filled if captured)\n\n\n\n\n\nThe true locations of animals are not known, and therefore the distance-dependent probabilities cannot be calculated directly. The model is fitted by marginalising (integrating over) animal locations.\nDistance-dependent detection is represented by a ‘detection function’ with intercept, scale, and possibly shape, determined by parameters to be estimated3.\n\n\n\n\n\n\n\nFigure 1.2: Some detection functions\n\n\n\n\n\n1.7.2 Habitat\n\nSECR models include a map of potential habitat near the detectors. Here ‘potential’ means ‘feasible locations for the AC of detected animals’. Excluded are sites that are known a priori to be unoccupied, and sites that are so distant that an animal centred there has negligible chance of detection.\nThe habitat map is called a ‘habitat mask’ in secr and a ‘state space’ in the various Bayesian implementations. It is commonly formed by adding a constant-width buffer around the detectors. For computational convenience the map is discretized as many small pixels. Spatial covariates (vegetation type, elevation, etc.) may be attached to each pixel for use in density models. The choice of buffer width and pixel size are considered later.\n\n1.7.3 Link functions\n \nA simple SECR model has three parameters: density D, and the intercept g_0 and spatial scale \\sigma of the detection function. Valid values of each parameter are restricted to part of the real number line (positive values for D and \\sigma, values between zero and one for g_0). A straightforward way to constrain estimates to valid values is to conduct maximization of the likelihood on a transformed (‘link’) scale: at each evaluation the parameter value is back transformed to the natural scale. The link function for all commonly used parameters defaults to ‘log’ (for positive values) except for g0 which defaults to ‘logit’ (for values between zero and one).\n\n\nTable 1.2: Link functions\n\n\n\nName\nFunction\nInverse\n\n\n\nlog\ny = \\log(x)\n\\exp(y)\n\n\nlogit\ny = \\log[p/(1-p)]\n1 / [1 + \\exp(-y)]\n\n\nidentity\ny=x\ny\n\n\ncloglog\ny = \\log(-\\log(1-p))\n1 -\\exp(-\\exp(y))\n\n\n\n\n\n\nWorking on a link scale is especially useful when the parameter is itself a function of covariates. For example, \\log (D) = \\beta_0 + \\beta_1 c \\; for a log-linear function of a spatially varying covariate c. The coefficients \\beta_0 and \\beta_1 are estimated in place of D per se.\nWe sometimes follow MARK (e.g., Cooch & White, 2023) and use ‘beta parameters’ for coefficients on the link scale and ‘real parameters’ for the core parameters (D, g_0, \\lambda_0, \\sigma) on the natural scale.\n\n1.7.4 Estimation\n\nThere are several ways to estimate the parameters of the SECR probability model, all of them computer-intensive. We focus on numerical maximization of the log likelihood (Borchers & Efford (2008) and Chapter 3). The likelihood integrates over the unknown locations of the animals’ activity centres. This is achieved in practice by summing over cells in a discretized map of the habitat.\nIn outline, a function to compute the log likelihood from a vector of beta parameters is passed, along with the data, to an optimization function. Optimization is iterative. For illustration, Fig. 1.3 shows the sequence of likelihood evaluations with two maximization algorithms when the parameter vector consists of only the intercept and spatial scale of detection. Optimization returns the maximized log likelihood, a vector of parameter values at the maximum, and the Hessian matrix from which the variance-covariance matrix of the estimates may be obtained.\n\n\n\n\n\n\n\nFigure 1.3: Numerical maximization of conditional likelihood by two methods; triangle – initial values; filled circle – estimates. Both converge on the same estimates (dashed lines). Newton-Raphson is the default method in secr.\n\n\n\n\nBayesian methods make use of algorithms that sample from a Markov chain (MCMC) to approximate the posterior distribution of the parameters. MCMC for abundance estimation faces the special problem that an unknown number of individuals, at unknown locations, remain undetected. The issue is addressed by data augmentation (Royle & Young, 2008) or using a semi-complete data likelihood (King et al., 2016). \nEarly anxiety about the suitability of MLE and asymptotic variances for SECR with small samples appears to have been mistaken. Royle et al. (2009) believed that “… the practical validity of these procedures cannot be asserted in most situations involving small samples”. This has not been borne out by subsequent simulations. The concluding discussion of Gerber & Parmenter (2015) is pertinent. Palmero et al. (2023) reported that Bayesian methods provide more precise estimates of density, but this appears to be an artefact: it is a mistake to compare estimates from MLE models with random N(A) with estimates from Bayesian models with fixed N(A). There is a further risk that the chosen Bayesian priors constrain the estimates.\n\n\nThe choice between Bayesian and frequentist (MLE) methods is now an issue of convenience for most users:\n\nMLE provides fast and repeatable results for established models with a small to medium number of parameters.\nBayesian methods have an advantage for novel models and possibly for those with many parameters.\n\nA further method, simulation and inverse prediction, has a niche use for data from single-catch traps (Efford et al., 2004; Efford, 2023). \n\n\n\nFigure 1.1: Distance-dependent detection of uniformly distributed activity centres (open circles; filled if captured)\nFigure 1.2: Some detection functions\nFigure 1.3: Numerical maximization of conditional likelihood by two methods; triangle – initial values; filled circle – estimates. Both converge on the same estimates (dashed lines). Newton-Raphson is the default method in secr.\n\n\n\nBorchers, D. L., Buckland, S. T., & Zucchini, W. (2002). Estimating animal abundance. In Statistics for Biology and Health. Springer London. https://doi.org/10.1007/978-1-4471-3708-5\n\n\nBorchers, D. L., & Efford, M. G. (2008). Spatially explicit maximum likelihood methods for capture-recapture studies. Biometrics, 64, 377–385.\n\n\nCooch, E., & White, G. (Eds.). (2023). Program MARK: A gentle introduction. (23rd ed.). http://www.phidot.org/software/mark/docs/book/\n\n\nDistiller, G., & Borchers, D. L. (2015). A spatially explicit capture–recapture estimator for single‐catch traps. Ecology and Evolution, 5(21), 5075–5087. https://doi.org/10.1002/ece3.1748\n\n\nEfford, M. G. (2023). ipsecr: An R package for awkward spatial capture–recapture data. Methods in Ecology and Evolution, 14(5), 1182–1189. https://doi.org/10.1111/2041-210x.14088\n\n\nEfford, M. G., Borchers, D. L., & Mowat, G. (2013). Varying effort in capture-recapture studies. Methods in Ecology and Evolution, 4, 629–636.\n\n\nEfford, M. G., Dawson, D. K., & Borchers, D. L. (2009). Population density estimated from locations of individuals on a passive detector array. Ecology, 90, 2676–2682.\n\n\nEfford, M. G., Dawson, D. K., & Robbins, C. S. (2004). DENSITY: Software for analysing capture-recapture data from passive detector arrays. Animal Biodiversity and Conservation, 27, 217–228.\n\n\nEfford, M. G., & Schofield, M. R. (2020). A spatial open-population capture-recapture model. Biometrics, 76, 392–402.\n\n\nEfford, M. G., Warburton, B., Coleman, M. C., & Barker, R. J. (2005). A field test of two methods for density estimation. Wildlife Society Bulletin, 33, 731–738.\n\n\nErgon, T., & Gardner, B. (2013). Separating mortality and emigration: Modelling space use, dispersal and survival with robust‐design spatial capture–recapture data. Methods in Ecology and Evolution, 5(12), 1327–1336. https://doi.org/10.1111/2041-210x.12133\n\n\nGerber, B. D., & Parmenter, R. R. (2015). Spatial capture–recapture model performance with known small‐mammal densities. Ecological Applications, 25(3), 695–705. https://doi.org/10.1890/14-0960.1\n\n\nKendeigh, S. C. (1944). Measurement of bird populations. Ecological Monographs, 14, 67–106. https://doi.org/10.2307/1961632\n\n\nKing, R., McClintock, B. T., Kidney, D., & Borchers, D. (2016). Capture–recapture abundance estimation using a semi-complete data likelihood approach. The Annals of Applied Statistics, 10, 264–285. https://doi.org/10.1214/15-aoas890\n\n\nMowat, G., & Strobeck, C. (2000). Estimating population size of grizzly bears using hair capture, DNA profiling, and mark-recapture analysis. Journal of Wildlife Management, 64, 183–193.\n\n\nPalmero, S., Premier, J., Kramer‐Schadt, S., Monterroso, P., & Heurich, M. (2023). Sampling variables and their thresholds for the precise estimation of wild felid population density with camera traps and spatial capture–recapture methods. Mammal Review, 53(4), 223–237. https://doi.org/10.1111/mam.12320\n\n\nRoyle, J. A., Karanth, K. U., Gopalaswamy, A. M., & Kumar, N. S. (2009). Bayesian inference in camera trapping studies for a class of spatial capture–recapture models. Ecology, 90(11), 3233–3244. https://doi.org/10.1890/08-1481.1\n\n\nRoyle, J. A., & Young, K. V. (2008). A hierarchical model for spatial capture-recapture data. Ecology, 89, 2281–2289.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Concepts and terminology</span>"
    ]
  },
  {
    "objectID": "01-basics.html#footnotes",
    "href": "01-basics.html#footnotes",
    "title": "1  Concepts and terminology",
    "section": "",
    "text": "In the secr software, type ‘proximity’ refers specifically to binary proximity detectors.↩︎\nConfusingly, secr uses ‘traps’ as a generic name for R objects holding detector coordinates and other information. This software-specific jargon should be avoided in publications.↩︎\nAll detection functions have intercept (g_0, \\lambda_0) and scale (\\sigma) parameters; some such as the hazard rate function have a further parameter that controls some aspect of shape.↩︎",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Concepts and terminology</span>"
    ]
  },
  {
    "objectID": "02-example.html",
    "href": "02-example.html",
    "title": "2  Simple example",
    "section": "",
    "text": "2.1 Input data\nThe raw data are in two text files, the capture file and the trap layout file. Data from Otis et al. (1978) have been transformed for secr (code in secr-tutorial.pdf).\ncode to download ‘hareCH6capt.txt’ and ‘hareCH6trap.txt’fnames &lt;- c(\"hareCH6capt.txt\", \"hareCH6trap.txt\")\nurl &lt;- paste0('https://www.otago.ac.nz/density/examples/', fnames)\ndownload.file(url, fnames, method = \"libcurl\")\nThe capture file “hareCH6capt.txt” has one line per capture and four columns (header lines are commented out and are not needed). Here we display the first 6 lines. The first column is a session label derived from the original study name; here there is only one session.\n# Burnham and Cushwa snowshoe hare captures\n# Session ID Occasion Detector \nwickershamunburne  1 2 0201\nwickershamunburne 19 1 0501\nwickershamunburne 72 5 0601\nwickershamunburne 73 6 0601\n...\nThe trap layout file “hareCH6trap.txt” has one row per trap and columns for the detector label and x- and y-coordinates. We display the first 6 lines. The detector label is used to link captures to trap sites. Coordinates can relate to any rectangular coordinate system; secr will assume distances are in metres. These coordinates simply describe a 10 \\times 10 square grid with spacing 61 m.\n# Burnham and Cushwa snowshoe hare trap layout\n# Detector  x  y \n0101 0 0\n0201 60.96 0\n0301 121.92 0\n0401 182.88 0\n...\nAfter opening R, we load secr and read the data files to construct a capthist object. The detectors are single-catch traps (maximum of one capture per animal per occasion and one capture per trap per occasion).\nlibrary(secr)\nhareCH6 &lt;- read.capthist(\"hareCH6capt.txt\", \"hareCH6trap.txt\", detector = \"single\")\n\nNo errors found :-)\nThe capthist object hareCH6 now contains almost all the information needed to fit a model.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple example</span>"
    ]
  },
  {
    "objectID": "02-example.html#input-data",
    "href": "02-example.html#input-data",
    "title": "2  Simple example",
    "section": "",
    "text": "Tip\n\n\n\nDo not use unprojected geographic coordinates (latitude and longitude). Section C.2.1 shows how to transform geographic coordinates to rectangular coordinates (e.g., UTM).",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple example</span>"
    ]
  },
  {
    "objectID": "02-example.html#check-data",
    "href": "02-example.html#check-data",
    "title": "2  Simple example",
    "section": "\n2.2 Check data",
    "text": "2.2 Check data\nFirst review a summary of the data. See ?summary.capthist for definitions of the summary statistics n, u, f etc.\n\nsummary(hareCH6)\n\nObject class       capthist \nDetector type      single \nDetector number    100 \nAverage spacing    60.96 m \nx-range            0 548.64 m \ny-range            0 548.64 m \n\nCounts by occasion \n                    1   2   3   4   5   6 Total\nn                  16  28  20  26  23  32   145\nu                  16  24   9   9   6   4    68\nf                  25  22  13   5   1   2    68\nM(t+1)             16  40  49  58  64  68    68\nlosses              0   0   0   0   0   0     0\ndetections         16  28  20  26  23  32   145\ndetectors visited  16  28  20  26  23  32   145\ndetectors used    100 100 100 100 100 100   600\n\n\nThese are spatial data so we learn a lot by mapping them. The plot method for capthist objects has some handy arguments; set tracks = TRUE to join consecutive captures of each individual.\n\npar(mar = c(1,1,3,1))  # reduce margins\nplot (hareCH6, tracks = TRUE)\n\n\n\n\n\n\nFigure 2.1: Snowshoe hare spatial capture data. Trap sites (red crosses) are 61 m apart. Grid lines (grey) are 100 m apart (use arguments gridlines and gridspace to suppress the grid or vary its spacing). Colours help distinguish individuals, but some are recycled.\n\n\n\n\nThe most important insight is that individuals tend to be recaptured near their site of first capture. This is expected when the individuals of a species occupy home ranges. In SECR models the tendency for detections to be localised is reflected in the spatial scale parameter \\sigma. Good estimation of \\sigma and density D requires spatial recaptures (i.e. captures at sites other than the site of first capture).\nSuccessive trap-revealed movements can be extracted with the moves function and summarised with hist:\n\ncode for movement plotm &lt;- unlist(moves(hareCH6))\npar(mar = c(3.2,4,2,1), mgp = c(2.1,0.6,0))  # reduce margins\nhist(m, breaks = seq(0,500,61), xlab = \"Movement  m\", main = \"\")\n\n\n\n\n\n\nFigure 2.2: Trap-revealed movements of snowshoe hares\n\n\n\n\nAbout 30% of trap-revealed movements were of &gt;100 m (Fig. 2.2; try also plot(ecdf(m))), so we can be sure that peripheral hares stood a good chance of being trapped even if their home ranges were centred well outside the area plotted in Fig. 2.1.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple example</span>"
    ]
  },
  {
    "objectID": "02-example.html#fit-a-simple-model",
    "href": "02-example.html#fit-a-simple-model",
    "title": "2  Simple example",
    "section": "\n2.3 Fit a simple model ",
    "text": "2.3 Fit a simple model \nNext we fit the simplest possible SECR model with function secr.fit. The buffer argument determines the habitat extent - we take a stab at this and check it later. Setting trace = FALSE suppresses printing of intermediate likelihood evaluations; it doesn’t hurt to leave it out. We save the fitted model with the name ‘fit’. Fitting is much faster if we use parallel processing in multiple threads - the number will depend on your machine, but ncores = 7 is OK in Windows with a quad-core processor.\n\nfit &lt;- secr.fit (hareCH6, buffer = 250, trace = FALSE, ncores = 7)\n\nWarning: multi-catch likelihood used for single-catch traps\n\n\nA warning is generated. The data are from single-catch traps, but there is no usable theory for likelihood-based estimation from single-catch traps. This is not the obstacle it might seem, because simulations seem to show that the alternative likelihood for multi-catch traps may be used without biasing the density estimates (Efford et al., 2009). It is safe to ignore the warning for now. The issue arises later as a breach of the independence assumption.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple example</span>"
    ]
  },
  {
    "objectID": "02-example.html#output",
    "href": "02-example.html#output",
    "title": "2  Simple example",
    "section": "\n2.4 Output",
    "text": "2.4 Output\nThe output from secr.fit is an R object of class ‘secr’. If you investigate the structure of fit with str(fit) it will seem to be a mess: it is a list with more than 25 components, none of which contains the final estimates you are looking for.\nTo examine model output or extract particular results you should use one of the functions defined for the purpose. Technically, these are S3 methods for the class ‘secr’. The key methods are print, plot, AIC, coef, vcov, and predict. Append ‘.secr’ when seeking help e.g. ?print.secr.\nTyping the name of the fitted model at the R prompt invokes the print method for secr objects and displays a more useful report.\n\nfit\n\n\nsecr.fit(capthist = hareCH6, buffer = 250, trace = FALSE, ncores = 7)\nsecr 5.2.1, 17:45:21 04 Feb 2025\n\nDetector type      single \nDetector number    100 \nAverage spacing    60.96 m \nx-range            0 548.64 m \ny-range            0 548.64 m \n\nN animals       :  68  \nN detections    :  145 \nN occasions     :  6 \nMask area       :  104.595 ha \n\nModel           :  D~1 g0~1 sigma~1 \nFixed (real)    :  none \nDetection fn    :  halfnormal\nDistribution    :  poisson \nN parameters    :  3 \nLog likelihood  :  -607.988 \nAIC             :  1221.98 \nAICc            :  1222.35 \n\nBeta parameters (coefficients) \n           beta   SE.beta       lcl       ucl\nD      0.382529 0.1299502  0.127831  0.637227\ng0    -2.723792 0.1609315 -3.039212 -2.408372\nsigma  4.224580 0.0653229  4.096549  4.352610\n\nVariance-covariance matrix of beta parameters \n                D          g0       sigma\nD      0.01688706 -0.00173067 -0.00162336\ng0    -0.00173067  0.02589895 -0.00737559\nsigma -0.00162336 -0.00737559  0.00426708\n\nFitted (real) parameters evaluated at base levels of covariates \n       link   estimate SE.estimate        lcl        ucl\nD       log  1.4659871  0.19131247  1.1363609  1.8912284\ng0    logit  0.0615839  0.00930045  0.0456855  0.0825365\nsigma   log 68.3457775  4.46931251 60.1324252 77.6809731\n\n\nThe report comprises these sections that you should locate in the preceding output:\n\nfunction call and time stamp\nsummary of the data\ndescription of the model, including the maximized log likelihood, Akaike’s Information Criterion AIC\nestimates of model coefficients (‘beta’ parameters)\nestimates of variance-covariance matrix of the coefficients\nestimates of the ‘real’ parameters\n\nThe last three items are generated by the coef, vcov and predict methods respectively. The final table of estimates is the most interesting, but it is derived from the other two. For our simple model there is one beta parameter for each real parameter1. The estimated density is 1.47 hares per hectare, 95% confidence interval 1.14–1.89 hares per hectare2.\nThe other two real parameters jointly determine the detection function that you can easily plot with 95% confidence limits:\n\npar(mar = c(5,4.5,2,1))  # adjust white margins\nplot(fit, limits = TRUE)\n\n\n\n\n\n\nFigure 2.3: Fitted halfnormal detection function, with 95% confidence limits",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple example</span>"
    ]
  },
  {
    "objectID": "02-example.html#revisiting-buffer-width",
    "href": "02-example.html#revisiting-buffer-width",
    "title": "2  Simple example",
    "section": "\n2.5 Revisiting buffer width",
    "text": "2.5 Revisiting buffer width\nChoosing a buffer width is a common stumbling block. We used buffer = 250 without any explanation. Here it is. As far as we know, the snowshoe hare traps were surrounded by suitable habitat. We limit our attention to the area immediately around the traps by specifying a habitat buffer. The buffer argument is a short-cut for defining the potential habitat (area of integration); the alternative is to provide a habitat mask in the mask argument of secr.fit. Buffers and habitat masks are covered at length in Chapter 12.\nBuffer width is not critical as long as it is wide enough that animals at the edge have effectively zero chance of appearing in our sample, so that increasing the buffer has negligible effect on estimates. For half-normal detection (the default) a buffer of 4\\sigma is usually enough3. We check the present model with the function esaPlot. The estimated density4 has easily reached a plateau at the chosen buffer width (dashed red line):\n\npar(mar = c(5,4,2,1))  # adjust white margins\nesaPlot(fit)\nabline(v = 250, lty = 2, col = 'red')\n\n\n\n\n\n\nFigure 2.4: Post hoc evaluation of buffer width using esaPlot()",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple example</span>"
    ]
  },
  {
    "objectID": "02-example.html#overall-probability-of-detection",
    "href": "02-example.html#overall-probability-of-detection",
    "title": "2  Simple example",
    "section": "\n2.6 Overall probability of detection",
    "text": "2.6 Overall probability of detection\nAs a final flourish, in Fig. 2.5 we plot contours of the overall probability of detection p_\\cdot(\\mathbf x; \\theta) as a function of AC location \\mathbf x, given the fitted model. The white line is the outer edge of the automatic mask generated by secr.fit with a 250-m buffer.\n\ncode for pdot plottr &lt;- traps(hareCH6)   # just the traps\ndp &lt;- detectpar(fit)   # extract detection parameters from simple model\nmask300 &lt;- make.mask(tr, nx = 128, buffer = 300)  \ncovariates(mask300)$pd &lt;- pdot(mask300, tr, detectpar = dp, \n     noccasions = 6)\npar(mar = c(1,1,1,5))  # adjust white margin\nplot(mask300, cov = 'pd', dots = FALSE, border = 1, inset = 0.1, \n     title = 'p.(x)')\nplot(tr, add = TRUE)   # over plot trap locations\npdotContour(tr, nx = 128, detectfn = 'HN', detectpar = dp, \n     noccasions = 6, add = TRUE)\nplotMaskEdge(make.mask(tr, 250, type = 'trapbuffer'), add = TRUE, \n     col = 'white')\n\n\n\n\n\n\nFigure 2.5: Contour plot of overall detection probability p_\\cdot(\\mathbf x; \\theta).\n\n\n\n\n\ncode for distribution of pdotcovariates(mask300)$pd &lt;- pdot(mask300, tr, detectpar = dp, \n    noccasions = 6)\ncovariates(mask300)$d &lt;- distancetotrap(mask300, tr)\ncovariates(mask300)$dclass &lt;- cut(distancetotrap(mask300, tr),\n    c(0,100,200,400))\npar(mar = c(4,4,2,2))\nf &lt;- c(0,0.1,0.2,0.3,0.4,0.5)\nnm &lt;- nrow(mask300)\nhist(covariates(mask300)$pd, xlim = c(0,1), ylim = c(0,nm/2), \n     breaks = seq(0,1,0.05), col = 'forestgreen', axes=F,\n     ylab = \"Fraction of mask population\",\n     xlab = \"Overall probability of detection\", main = \"\")\naxis(1)\naxis(2, at = nm * f, labels = f)\nfor (i in 0:9)\nhist(covariates(mask300)$pd[covariates(mask300)$pd&gt;i/10], breaks =\n      seq(0,1,0.05), add = T, col = terrain.colors(10)[i+1])\n\n\n\n\n\n\nFigure 2.6: Distribution of overall detection probability p_\\cdot(\\mathbf x; \\theta) for AC within the area of Fig. 2.5.\n\n\n\n\n\n\n\nFigure 2.1: Snowshoe hare spatial capture data. Trap sites (red crosses) are 61 m apart. Grid lines (grey) are 100 m apart (use arguments gridlines and gridspace to suppress the grid or vary its spacing). Colours help distinguish individuals, but some are recycled.\nFigure 2.2: Trap-revealed movements of snowshoe hares\nFigure 2.3: Fitted halfnormal detection function, with 95% confidence limits\nFigure 2.4: Post hoc evaluation of buffer width using esaPlot()\nFigure 2.5: Contour plot of overall detection probability p_\\cdot(\\mathbf x; \\theta).\nFigure 2.6: Distribution of overall detection probability p_\\cdot(\\mathbf x; \\theta) for AC within the area of Fig. 2.5.\n\n\n\nEfford, M. G., Borchers, D. L., & Byrom, A. E. (2009). Density estimation by spatially explicit capture-recapture: Likelihood-based methods. In D. L. Thomson, E. G. Cooch, & M. J. Conroy (Eds.), Modeling demographic processes in marked populations (pp. 255–269). Springer.\n\n\nKendeigh, S. C. (1944). Measurement of bird populations. Ecological Monographs, 14, 67–106. https://doi.org/10.2307/1961632\n\n\nOtis, D. L., Burnham, K. P., White, G. C., & Anderson, D. R. (1978). Statistical inference from capture data on closed animal populations. Wildlife Monographs, 62.",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple example</span>"
    ]
  },
  {
    "objectID": "02-example.html#footnotes",
    "href": "02-example.html#footnotes",
    "title": "2  Simple example",
    "section": "",
    "text": "We can get from beta parameter estimates to real parameter estimates by applying the inverse of the link function e.g. \\hat D = \\exp(\\hat \\beta_D), and similarly for confidence limits; standard errors require a delta-method approximation (Lebreton et al. 1992).↩︎\nOne hectare (ha) is 10000 m2 or 0.01 km2.↩︎\nThis is not just the tail probability of a normal deviate; think about how the probability of an individual being detected at least once changes with (i) the duration of sampling (ii) the density of detector array.↩︎\nThese are Horvitz-Thompson-like estimates of density obtained by dividing the observed number of individuals n by effective sampling areas (Borchers and Efford 2008) computed as the cumulative sum over mask cells ordered by distance from the traps. The algorithm treats the detection parameters as known and fixed.↩︎",
    "crumbs": [
      "Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple example</span>"
    ]
  },
  {
    "objectID": "03-theory.html",
    "href": "03-theory.html",
    "title": "3  Likelihood-based SECR",
    "section": "",
    "text": "3.1 Notation\nWe use notation and terminology from Borchers & Efford (2008), with minor variations and extensions from Efford et al. (2009), Efford (2011) and elsewhere (Table 3.1).",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Likelihood-based SECR</span>"
    ]
  },
  {
    "objectID": "03-theory.html#notation",
    "href": "03-theory.html#notation",
    "title": "3  Likelihood-based SECR",
    "section": "",
    "text": "Table 3.1: Mathematical notation for SECR\n\n\n\n\n\n\n\n\n\n\nCategory\nSymbol\nMeaning\n\n\n\n\nGeneral\n\n\n\n\n\nAC\nactivity centre\n\n\n\n\\mathbf x\npoint (x,y) in the plane\n\n\nData\n\n\n\n\n\nn\nnumber of individuals detected\n\n\n\nS\nnumber of sampling occasions\n\n\n\nK\nnumber of detectors\n\n\n\n\\omega_i\nspatial detection history of the i-th animal\n\n\n\n\\Omega\nset of all detection histories \\omega_i, i = 1..n\n\n\nParameters\n\n\n\n\n\nD(\\mathbf x; \\phi)1\nintensity at \\mathbf x of AC Poisson point process\n\n\n\n\\phi\nparameter vector for AC point process\n\n\n\n\\theta\nvector of detection parameters (minimally (g_0, \\sigma) or (\\lambda_0,\\sigma))\n\n\n\ng_0\nintercept of distance-probability-of-detection function\n\n\n\n\\lambda_0\nintercept of distance-hazard-of-detection function\n\n\n\n\\sigma\nspatial scale parameter of distance-detection function\n\n\nModel\n\n\n\n\n\nd_k(\\mathbf x)\ndistance of point \\mathbf x from detector k\n\n\n\n\\lambda(d)\nhazard of detection at distance d (distance-hazard-of-detection function)\n\n\n\ng(d)\nprobability of detection at distance d (distance-probability-of-detection function)\n\n\n\np_\\cdot(\\mathbf x; \\theta)\nprobability that an individual with AC at \\mathbf x is detected at least once\n\n\n\nh_{isk}(\\mathbf x; \\theta)\nhazard of detection at detector k for animal i on occasion s\n\n\n\np_{isk}(\\mathbf x; \\theta)\nprobability of detection corresponding to h_{isk}(\\mathbf x; \\theta)\n\n\n\np_{k}(\\mathbf x; \\theta)\np_{isk}(\\mathbf x; \\theta) constant across individuals i and occasions s\n\n\n\n\\Lambda(\\phi, \\theta)\nexpected number of detected individuals n\n\n\nHabitat\n\n\n\n\n\nA\npotential habitat (‘habitat mask’, ‘state space’) A \\subset R^2\n\n\n\n|A|\narea of A\n\n\n\nN(A)\nnumber of AC in A",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Likelihood-based SECR</span>"
    ]
  },
  {
    "objectID": "03-theory.html#Likelihood",
    "href": "03-theory.html#Likelihood",
    "title": "3  Likelihood-based SECR",
    "section": "3.2 Likelihood",
    "text": "3.2 Likelihood\n\nParameters of the state model (\\phi) and the detection model (\\theta) are estimated jointly by maximizing the logarithm of the likelihood:\n\nL(\\phi, \\theta | \\Omega ) = \\mathrm{Pr}(n| \\phi, \\theta) \\; \\mathrm{Pr}(\\Omega | n, \\phi, \\theta).\n\\tag{3.1}\nWhen density is constant across space, \\phi drops out of the rightmost term, which then relates to the detection (observation) model alone, and maximization of this component gives unbiased estimates of \\theta (see Conditional likelihood).\n\n3.2.1 Number of individuals\n If AC follow an inhomogeneous Poisson process then n has a Poisson distribution with parameter \n\\Lambda(\\phi, \\theta) = \\int_{R^2} D(\\mathbf x; \\phi) \\, p_\\cdot(\\mathbf x;\\theta) \\, d\\mathbf x,\n\\tag{3.2} where D(\\mathbf x; \\phi) is the density at \\mathbf x and p_\\cdot(\\mathbf x|\\theta) is the overall probability of detecting an AC at \\mathbf x (see Section 3.4 and Chapter 4). Thus \\mathrm{Pr}(n | \\phi, \\theta) = \\Lambda^n \\exp (-\\Lambda) / n!.\nIf the population size in a defined area A is considered to be fixed rather than Poisson then the distribution of n is binomial with size N(A).\n\n\n3.2.2 Detection histories\nIn general we have \n\\mathrm{Pr} (\\Omega | n, \\phi, \\theta) = \\binom {n}{n_1,...,n_C}\n\\prod_{i=1}^n \\mathrm{Pr}( \\omega_i | \\omega_i&gt;0, \\phi, \\theta),\n\\tag{3.3}\nwhere \\omega_i&gt;0 indicates a non-empty detection history. The multinomial coefficient uses the frequencies n_1,...,n_C of each of the C observed histories. The coefficient is a constant not involving parameters and it can be omitted without changing the model fit (consistent inclusion or exclusion is needed for valid likelihood-based comparisons such as those using AIC).\nWe do not know the true AC locations, but they can be integrated2 out of the likelihood using an expression for their spatial distribution, i.e. \n\\mathrm{Pr}( \\omega_i | \\omega_i&gt;0, \\phi, \\theta) = \\int_{R^2} \\mathrm{Pr}( \\omega_i | \\omega_i&gt;0, \\theta, \\mathbf x) \\, f(\\mathbf x | \\omega_i&gt;0, \\phi, \\theta) \\; d\\mathbf x\n\\tag{3.4} where f(\\mathbf x| \\omega_i&gt;0, \\phi, \\theta) is the conditional density of an AC given that the individual was detected. The conditional density is given by \nf(\\mathbf x| \\omega_i&gt;0, \\phi, \\theta) = \\frac{D(\\mathbf x ; \\phi) p_\\cdot(\\mathbf x ; \\theta)}{\\Lambda(\\phi,\\theta)}.\n\\tag{3.5}",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Likelihood-based SECR</span>"
    ]
  },
  {
    "objectID": "03-theory.html#distance-dependent-detection",
    "href": "03-theory.html#distance-dependent-detection",
    "title": "3  Likelihood-based SECR",
    "section": "3.3 Distance-dependent detection",
    "text": "3.3 Distance-dependent detection\n\nThe key idea of SECR is that the probability of detecting a particular animal at a particular detector on one occasion can be represented as a function of the distance between its AC and the detector. The function should decline effectively to zero at large distances. Distances are not observed directly, and we rely on functions of somewhat arbitrary shape. Fortunately, the estimates are not very sensitive to the choice. Detection functions are covered in detail in Chapter 10. Either probability g(d) or hazard \\lambda(d) may be modelled as a function of distance. A halfnormal form is commonly used (e.g., g(d) = g_0 \\exp (-d^2/2/\\sigma^2)). The shapes of, e.g., halfnormal g(d) and halfnormal \\lambda_0(d) are only subtly different, but \\lambda(d) is preferred because it lends itself to mathematical manipulation and occurs more widely in the literature (often with different notation).\nThe function \\lambda(d) may be transformed into a probability with g(d) = 1 - \\exp[-\\lambda(d)] and the reverse transformation is \\lambda(d) = -\\log[1-g(d)]. The intercept parameter g_0 has been replaced by \\lambda_0; although the name \\sigma is retained for the spatial scale parameter this is not exactly interchangeable between the models.",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Likelihood-based SECR</span>"
    ]
  },
  {
    "objectID": "03-theory.html#sec-pointdetectors",
    "href": "03-theory.html#sec-pointdetectors",
    "title": "3  Likelihood-based SECR",
    "section": "3.4 Detector types",
    "text": "3.4 Detector types\n \nThe SECR data \\omega_i for each detected individual comprise a matrix with dimensions S (occasions) and K (detectors). The matrix entries \\omega_{isk} may be binary (0/1) or integer (0, 1, 2, …). Various probability models exist for \\omega_{isk}. The appropriate probability model follows in most cases directly from the type of detector device; we therefore classify probability models according to device type. Table 3.2 matches this classification to that of Royle et al. (2014). This section covers passive detection at a point; area searches are considered later.\n\n\n\nTable 3.2: Detector types based on Efford & Boulanger (2019, Table 1) with cross references to Royle et al. (2014)\n\n\n\n\n\n\n\n\n\n\nDetector type\nRoyle et al.\nData\n\n\n\n\nBinary proximity\nBernoulli1\nbinary animal \\times occasion \\times detector\n\n\nCount proximity\n\n\n\n\n   Poisson\nPoisson\ninteger animal \\times occasion \\times detector\n\n\n   Binomial\nBinomial\ninteger animal \\times occasion \\times detector\n\n\nMulti-catch trap\nMultinomial\nbinary animal \\times occasion\n\n\nSingle-catch trap\n—\nbinary animal \\times occasion, exclusive\n\n\nArea search\n\ninteger animal \\times occasion \\times detector\n\n\nTransect search\n\ninteger animal \\times occasion \\times detector\n\n\nExclusive area search2\n\nbinary animal \\times occasion\n\n\nExclusive transect search2\n\nbinary animal \\times occasion\n\n\n\n\n\n\n\nAlso ‘Binomial’ in Royle & Gardner (2011)\n‘Exclusive’ here means that an individual can be detected at no more than one detector (polygon or transect) per occasion.\n\nFor each type of detector we require \\mathrm{Pr}(\\omega_{isk} | \\mathbf x) and the overall probability of detection p_\\cdot(\\mathbf x). For some detector types it is more natural to express the probability in terms of the occasion- and trap-specific hazard h_{sk} = \\lambda[d_k(\\mathbf x); \\theta^\\prime]  = -\\log(1-g[d_k(\\mathbf x); \\theta]) than the probability p_{sk} (\\mathbf x) = g[d_k(\\mathbf x); \\theta] = 1 - \\exp\\{-\\lambda[d_k(\\mathbf x); \\theta^\\prime]\\}3.\nWe summarise the probability models in Table 3.3, with comments on each point detector type below.\n\n\n\nTable 3.3: Summary of point detector types (conditioning on \\theta omitted to save space)\n\n\n\n\n\n\n\n\n\n\nDetector type\n\\mathrm{Pr}(\\omega_{isk} | \\mathbf x)\np_\\cdot(\\mathbf x)\n\n\n\n\nBinary proximity\np_{sk}(\\mathbf x) ^{\\omega_{isk}} [1-p_{sk}(\\mathbf x)]^{(1-\\omega_{isk})}\n1 - \\prod_s\\prod_k 1 - p_{sk} (\\mathbf x)\n\n\nCount proximity\n\n\n\n\n   Poisson\n\\{h_{sk} (\\mathbf x)^{\\omega_{isk}} \\exp [-h_{sk}(\\mathbf x)]\\} / \\omega_{isk}!\n1 - \\exp [- \\sum_s \\sum_k h_{sk}(\\mathbf x)]\n\n\n   Binomial1\n\\binom{B_s}{\\omega_{isk}} p_{sk}(\\mathbf x)^{\\omega_{isk}} [1-p_{sk}(\\mathbf x)]^{(B_s-\\omega_{isk})}\n1 - \\prod_s\\prod_k [1 - p_{sk} (\\mathbf x)]^{B_s}\n\n\nMulti-catch trap2\n\\{1 - \\exp [-H_s(\\mathbf x)]\\}\\frac{h_{sk}(\\mathbf x)}{H_s(\\mathbf x)}\n1 - \\exp[ -\\sum_s H_s(\\mathbf x)]\n\n\n\n\n\n\n\nB_s is the size of the binomial distribution, the number of opportunities for detection, assumed constant across detectors\nH_s = \\sum_k h_{sk}(\\mathbf x) is the hazard summed over traps\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.4.1 Binary proximity detector\n\nA proximity detector records the presence of an individual at or near a point without restricting its movement. The data are binary when any detections after the first are ignored (this avoids worries about the non-independence of repeated visits to a detector).\nAssuming independence among detectors, the distance-detection model applies directly as the probability of detection in a particular detector, and the overall probability of detection is the complement of the product of probabilities of non-detection in all detectors.\n\n\n3.4.2 Poisson count proximity detector\n\nHazard is the natural scale for the Poisson parameter.\n\n\n3.4.3 Binomial count proximity detector\n\nBinomial counts arise when there is a known, finite number of opportunities for detection within each occasion that we denote B_s. This is the result when binary proximity data over many occasions are collapsed to a single occasion: the initial number of occasions is known (B_s = S) and places an upper limit on the count. Collapsing is often efficient. It precludes modelling parameter variation or learned responses across occasions.\nEach count is binomial with size B_s and probability equal to the per-occasion detection probability.\n\n\n3.4.4 Multi-catch trap\n \nA trap is a device that detains an animal until it is released, allowing only one detection of that animal per occasion. The single-detector, single-AC probability from a distance-dependent detection function (preceding section) must be modified to allow for prior capture in a different trap: traps effectively “compete” for animals. If the trap remains open for captures of further animals then the solution is a straightforward competing risk model (Borchers & Efford, 2008).\nThe competing risk model uses the occasion- and trap-specific hazard h_{sk}.\n\n\n3.4.5 Single-catch trap\n \nA single-catch trap can catch only one animal at a time. This entails competition both among traps for animals and among animals for traps. No simple likelihood is available. Simulation-based methods (Efford, 2004, 2023) must be used for unbiased estimation of \\theta and trend in density unless the time of detection has been recorded (Distiller & Borchers, 2015).",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Likelihood-based SECR</span>"
    ]
  },
  {
    "objectID": "03-theory.html#sec-fixedN",
    "href": "03-theory.html#sec-fixedN",
    "title": "3  Likelihood-based SECR",
    "section": "3.5 Fixed N",
    "text": "3.5 Fixed N\n\nThe formulation of the state model as an inhomogeneous Poisson process (Chapter 1) does not refer to population size N. The state model may also be cast as a ‘conditional’ or ‘binomial’ Poisson process’ (Illian et al., 2008). For an arbitrary area A the number of AC is then considered fixed rather than Poisson.\nThe distribution of n is then binomial with size N(A) and probability p_c(\\phi, \\theta) = \\int_A p_\\cdot(\\mathbf x; \\theta) f(\\mathbf x; \\phi) d\\mathbf x, where f(\\mathbf x; \\phi) = D(\\mathbf x; \\phi) / \\int_A D(\\mathbf x; \\phi) d\\mathbf x.\nThe form conditional on N(A) leads to narrower confidence intervals for density owing to the exclusion of variation in N(A) among realisations of the Poisson process for AC. This makes sense when A contains an isolated population with a natural boundary, but most applications do not meet this criterion.",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Likelihood-based SECR</span>"
    ]
  },
  {
    "objectID": "03-theory.html#sec-confidenceintervals",
    "href": "03-theory.html#sec-confidenceintervals",
    "title": "3  Likelihood-based SECR",
    "section": "3.6 Confidence intervals",
    "text": "3.6 Confidence intervals\n\nMaximizing the log likelihood leads to a straightforward estimate of the asymptotic covariance matrix \\mathbf V of the beta parameters. If \\hat \\theta is the vector of estimates and \\mathbf H(\\hat \\theta) is the Hessian matrix evaluated at \\hat \\theta then an estimate of the covariance matrix is \\hat {\\mathbf V} = \\mathbf H(\\hat \\theta)^{-1}.\n\n\n\n\n\n\nHessian\n\n\n\nThe Hessian matrix is the square matrix of second-order partial derivatives of the log likelihood. For more on asymptotic variances of MLE see Seber (1982), Borchers et al. (2002), Cooch & White (2023) 1.3.2, and many statistics texts.\n\n\nThe sampling error of MLE is asymptotically normal, and symmetric (Wald) intervals for SECR parameters appear to work well on the link scale i.e. \\hat \\theta_j \\pm z_{\\alpha/2} \\hat \\sigma_j is a 100(1-\\alpha)\\% interval for \\hat \\theta_j where -z_{\\alpha/2} is the \\alpha/2 quantile of the standard normal deviate (z_{0.025} = 1.96) and \\hat \\sigma_j^2 is the estimated variance from \\hat{\\mathbf V}.\nOn back transformation to the natural (‘real’) scale these intervals become asymmetrical and generally have good coverage properties.\nThe method of profile likelihood is also available (e.g., Evans et al. (1996); secr::confint.secr), but it is seldom used as no problem has been shown with intervals based on asymptotic variances. Similarly, the additional computation required by parametric bootstrap methods is not usually warranted.",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Likelihood-based SECR</span>"
    ]
  },
  {
    "objectID": "03-theory.html#sec-varyingeffort",
    "href": "03-theory.html#sec-varyingeffort",
    "title": "3  Likelihood-based SECR",
    "section": "3.7 Varying effort",
    "text": "3.7 Varying effort\n\nWhen sampling effort varies between detectors or over time in a capture–recapture study we expect a commensurate change in the number of detections. Allowing for known variation in effort when modelling detections has these benefits:\n\ndetection parameters are related to a consistent unit of effort (e.g., one trap set for one day)\nthe fit of the detection model is improved\ntrends in the estimates may be modelled without confounding.\n\nBorchers & Efford (2008) allowed the duration of exposure to vary between sampling occasions in their competing-hazard model for multi-catch traps. Efford et al. (2013) generalised the method to allow joint variation in effort over detectors and over time (occasions), and considered other detector types.\nWe use T_{sk} for the effort on occasion s at detector k. At its simplest, T_{sk} can be a binary indicator taking the values 0 (detector not used) or 1 (detector used) (when T_{sk} = 0, no detections are possible and \\omega_{isk} = 0). For small, continuously varying, T_{sk} we expect the number of detections to increase linearly with T_{sk}; saturation may occur at higher effort, depending on the detector type. Examples of possible effort variables are the number of days that each automatic camera was operated in a tiger study, or the number of rub trees sampled for DNA in each grid cell of a grizzly bear study.\nFollowing convention in non-spatial capture–recapture (Cooch & White, 2023) we could model g_0 or \\lambda_0 on the link scale (logit or log) as a linear function of T_{sk} (a time covariate if constant across detectors, a detector covariate if constant across occasions, or a time-varying detector-level covariate). However, this is suboptimal because varying effort has a linear effect only on \\lambda_0 for Poisson count detectors, and the estimation of additional parameters is an unnecessary burden. T_{sk} is like an offset in a generalised linear model: it can be included in the SECR model without estimating an additional coefficient.\nThe SECR models for various detectors (Table 3.3) are expressed in terms of either the probability p_{sk} or the hazard h_{sk}. Each of these scales differently with T_{sk} as shown in Table 3.4. Only in the Poisson case is the expected number of detections linear on effort.\n\n\n\nTable 3.4: Including effort in SECR models for various detector types. p^\\prime_{sk}(\\mathbf x) and h^\\prime_{sk}(\\mathbf x) replace the matching quantities in Table 3.3.\n\n\n\n\n\n\n\n\n\nDetector type\nAdjusted probability or hazard\n\n\n\n\nMulti-catch trap\nh^\\prime_{sk}(\\mathbf x) = h_{sk}(\\mathbf x) T_{sk}; H^\\prime_s(\\mathbf x) = \\sum_k h^\\prime_{sk}(\\mathbf x)\n\n\nBinary proximity\np^\\prime_{sk}(\\mathbf x) = 1 - (1 - p_{sk}(\\mathbf x))^{T_{sk}}\n\n\nPoisson count proximity\nh^\\prime_{sk}(\\mathbf x) = h_{sk}(\\mathbf x) T_{sk}\n\n\nBinomial count proximity\nsee below\n\n\n\n\n\n\nFor binomial count detectors we use a formulation not based directly on instantaneous hazard, as explained by Efford et al. (2013). For these detectors T_{sk} (assumed integer) is taken as the size of the binomial (maximum possible detections) and p_{sk}(\\mathbf x) is unchanged.\n\n\n\n\nBorchers, D. L., Buckland, S. T., & Zucchini, W. (2002). Estimating animal abundance. In Statistics for Biology and Health. Springer London. https://doi.org/10.1007/978-1-4471-3708-5\n\n\nBorchers, D. L., & Efford, M. G. (2008). Spatially explicit maximum likelihood methods for capture-recapture studies. Biometrics, 64, 377–385.\n\n\nCooch, E., & White, G. (Eds.). (2023). Program MARK: A gentle introduction. (23rd ed.). http://www.phidot.org/software/mark/docs/book/\n\n\nDistiller, G., & Borchers, D. L. (2015). A spatially explicit capture–recapture estimator for single‐catch traps. Ecology and Evolution, 5(21), 5075–5087. https://doi.org/10.1002/ece3.1748\n\n\nEfford, M. G. (2004). Density estimation in live-trapping studies. Oikos, 106, 598–610.\n\n\nEfford, M. G. (2011). Estimation of population density by spatially explicit capture-recapture analysis of data from area searches. Ecology, 92, 2202–2207.\n\n\nEfford, M. G. (2023). ipsecr: An R package for awkward spatial capture–recapture data. Methods in Ecology and Evolution, 14(5), 1182–1189. https://doi.org/10.1111/2041-210x.14088\n\n\nEfford, M. G., Borchers, D. L., & Byrom, A. E. (2009). Density estimation by spatially explicit capture-recapture: Likelihood-based methods. In D. L. Thomson, E. G. Cooch, & M. J. Conroy (Eds.), Modeling demographic processes in marked populations (pp. 255–269). Springer.\n\n\nEfford, M. G., Borchers, D. L., & Mowat, G. (2013). Varying effort in capture-recapture studies. Methods in Ecology and Evolution, 4, 629–636.\n\n\nEfford, M. G., & Boulanger, J. (2019). Fast evaluation of study designs for spatially explicit capture-recapture. Methods in Ecology and Evolution, 10, 1529–1535. https://doi.org/10.1111/2041-210X.13239\n\n\nEvans, M. A., Kim, H.-M., & O’Brien, T. E. (1996). An application of profile-likelihood based confidence interval to capture: Recapture estimators. Journal of Agricultural, Biological, and Environmental Statistics, 1(1), 131. https://doi.org/10.2307/1400565\n\n\nIllian, J., Penttinen, A., Stoyan, H., & Stoyan, D. (2008). Statistical analysis and modelling of spatial point patterns. Wiley.\n\n\nKendeigh, S. C. (1944). Measurement of bird populations. Ecological Monographs, 14, 67–106. https://doi.org/10.2307/1961632\n\n\nRoyle, J. A., Chandler, R. B., Sollmann, R., & Gardner, B. (2014). Spatial capture-recapture. Academic Press.\n\n\nRoyle, J. A., & Gardner, B. (2011). Hierarchical spatial capture-recapture models for estimating density from trapping arrays. In A. F. O’Connell, J. D. Nichols, & K. U. Karanth (Eds.), Camera traps in animal ecology: Methods and analyses (pp. 163–190). Springer.\n\n\nSeber, G. A. F. (1982). The estimation of animal abundance and related parameters. (2nd ed.). Griffin.",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Likelihood-based SECR</span>"
    ]
  },
  {
    "objectID": "03-theory.html#footnotes",
    "href": "03-theory.html#footnotes",
    "title": "3  Likelihood-based SECR",
    "section": "",
    "text": "We use D(\\mathbf x) in preference to \\lambda(\\mathbf x) because \\lambda has multiple meanings.↩︎\nIntegration is commonly performed by summing over many small cells for a finite region near the detectors, as both \\mathrm{Pr}(\\omega_i) and f(\\mathbf x) decline to zero at greater distances. We state the model in terms of the real plane and defer discussion of the region of integration to Chapter 12.↩︎\nThe parameter vectors \\theta and \\theta^\\prime differ for detection functions expressed in terms of probability (g()) and hazard (\\lambda()). ↩︎",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Likelihood-based SECR</span>"
    ]
  },
  {
    "objectID": "04-theory-area-search.html",
    "href": "04-theory-area-search.html",
    "title": "4  Area search",
    "section": "",
    "text": "4.1 Detector types for area search\nArea-search analogues exist for each of the point detector types.\nWe do not consider the area-search analogue of a binary proximity point detector because it seems improbable that binary data would be collected from each of several areas on one occasion.",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Area search</span>"
    ]
  },
  {
    "objectID": "04-theory-area-search.html#detector-types-for-area-search",
    "href": "04-theory-area-search.html#detector-types-for-area-search",
    "title": "4  Area search",
    "section": "",
    "text": "The ‘Poisson-count polygon’ type is suited to individually identifiable cues (e.g., faeces sampled for DNA).\n‘Exclusive polygons’ are an analogue of multi-catch traps - they provide at most one detection of an individual per occasion, most likely as a result of a direct search for the animal itself. The horned lizard dataset of Royle & Young (2008) is a good example .\nThe ‘binomial-count polygon’ type may result from collapsing exclusive polygon data to a single occasion.\n\n\n\n4.1.1 Detection model for area search\nThe distance-dependent detection model for point detectors is replaced for area searches by an overlap model. The hazard of detection of an individual within an irregular searched area is modelled as a function of the quantitative overlap between its home range1 (assumed circular) and the area searched (Fig. 4.1).\n\n\n\n\n\n\n\nFigure 4.1: Hazard of detection for an animal centered at the blue dot on an irregular searched polygon (red outline). The cumulative hazard is modelled by the quantitative overlap (grey shading) between a radially symmetrical probability density (circular contours) and the searched area.\n\n\n\n\nWe use \\lambda_0 for the expected number of detections (detected cues) of an animal whose home range lies completely within the area \\kappa, and h(\\mathbf u|\\mathbf x) for the probability density of activity at point \\mathbf u for an animal centred at \\mathbf x (i.e. \\int_{R^2} h(\\mathbf u|\\mathbf x) \\, d \\mathbf u = 1). Then the expected number of detected cues from an individual on occasion s in polygon k is\n\nh_{sk}(\\mathbf x; \\theta) = \\lambda_0 \\int_{\\kappa_k} h(\\mathbf u|\\mathbf x; \\theta^-) \\, du,\n\\tag{4.1}\nwhere \\theta = (\\lambda_0, \\theta^-) is the vector of detection parameters and \\kappa_k refers to the k-th polygon. The probability of at least one detected cue is p_{sk}(\\mathbf x) = 1 - \\exp[-h_{sk}(\\mathbf x)] as before. The detector-level probabilities conditional on AC location \\mathbf x follow directly from Table 3.3 (repeated in Table 4.1).\n\n\nTable 4.1: Detector-level probabilities for area-search detector types\n\n\n\n\n\n\n\n\nDetector type\n\\mathrm{Pr}(\\omega_{isk} | \\mathbf x)\np_\\cdot(\\mathbf x)\n\n\n\nCount polygon\n\n\n\n\n    Poisson\n\\{h_{sk} (\\mathbf x)^{\\omega_{isk}} \\exp [-h_{sk}(\\mathbf x)]\\} / \\omega_{isk}!\n1 - \\exp [- \\sum_s \\sum_k h_{sk}(\\mathbf x)]\n\n\n    Binomial\n\\binom{B_s}{\\omega_{isk}} p_{sk}(\\mathbf x)^{\\omega_{isk}} [1-p_{sk}(\\mathbf x)]^{(B_s-\\omega_{isk})}\n1 - \\prod_s\\prod_k [1 - p_{sk} (\\mathbf x)]^{B_s}\n\n\nExclusive polygon1\n\n\\{1 - \\exp [-H_s(\\mathbf x)]\\}\\frac{h_{sk}(\\mathbf x)}{H_s(\\mathbf x)}\n1 - \\exp[ -\\sum_s H_s(\\mathbf x)]\n\n\n\n\n\n\n\n\nH_s = \\sum_k h_{sk}(\\mathbf x) is the hazard summed over areas. If a single polygon is searched then h_{sk}(\\mathbf x) = H_s(\\mathbf x), simplifying the expression for \\mathrm{Pr}(\\omega_{isk} | \\mathbf x).\n\n4.1.2 Location within searched polygon\nThe only data we have considered to this point are the occasion- and detector-specific binary or integer values \\omega_{isk} that record detections at the level of polygons. Each spatial detection history \\omega_i also includes within-polygon locations. These provide important information on detection scale \\sigma and spatial variation in density \\phi, and we need to include them in the likelihood.\n\n\n\n\n\n\n\nFigure 4.2: Cues of one animal (yellow) within a searched polygon (red outline).\n\n\n\n\nFor each individual i there are \\omega_{isk} locations of detected cues on occasion s at detector k. We use \\mathbf Y_{isk} for the collection and \\mathbf y_{iskj} (j = 1,...,\\omega_{isk}) for each separate location. Then \n  \\mathrm{Pr}(\\mathbf Y_{isk} | \\mathbf x, \\theta^-) \\propto \\prod_{j=1}^{\\omega_{isk}} \\frac{h(\\mathbf y_{iskj} | \\mathbf x; \\theta^-)}{\\int_{\\kappa_k} h(\\mathbf u|\\mathbf x; \\theta^-) \\, du}.\n\\tag{4.2}\n\n4.1.3 Likelihood\n\nThe likelihood component associated with \\omega_i, conditional on \\mathbf x, is a product of the probability of observing \\omega_{isk} cues, and the probability of the within-polygon locations: \n\\mathrm{Pr}(\\omega_i | \\omega_i &gt; 0, \\mathbf x; \\phi, \\theta) \\propto \\frac{1}{p_\\cdot(\\mathbf x; \\theta)} \\prod_s \\prod_k \\mathrm{Pr}(\\omega_{isk} | \\mathbf x; \\theta) \\, \\mathrm{Pr}(\\mathbf Y_{isk} | \\mathbf x, \\theta^-).\n Inserted in Eq. 3.3, along with f(\\mathbf x | \\omega_i&gt;0; \\phi, \\theta) from Eq. 3.5, this provides the likelihood component \\mathrm{Pr} (\\Omega | n, \\phi, \\theta) of Eq. 3.1.",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Area search</span>"
    ]
  },
  {
    "objectID": "04-theory-area-search.html#transect-search",
    "href": "04-theory-area-search.html#transect-search",
    "title": "4  Area search",
    "section": "\n4.2 Transect search",
    "text": "4.2 Transect search\n\nTransect detectors are the linear equivalent of polygons. Cues may be observed only along the searched zero-width transect. Transect detectors, like polygon detectors, may be independent or exclusive. See Appendix D for more.\n\n\n\nFigure 4.1: Hazard of detection for an animal centered at the blue dot on an irregular searched polygon (red outline). The cumulative hazard is modelled by the quantitative overlap (grey shading) between a radially symmetrical probability density (circular contours) and the searched area.\nFigure 4.2: Cues of one animal (yellow) within a searched polygon (red outline).\n\n\n\nEfford, M. G. (2011). Estimation of population density by spatially explicit capture-recapture analysis of data from area searches. Ecology, 92, 2202–2207.\n\n\nKendeigh, S. C. (1944). Measurement of bird populations. Ecological Monographs, 14, 67–106. https://doi.org/10.2307/1961632\n\n\nRoyle, J. A., Chandler, R. B., Sollmann, R., & Gardner, B. (2014). Spatial capture-recapture. Academic Press.\n\n\nRoyle, J. A., & Young, K. V. (2008). A hierarchical model for spatial capture-recapture data. Ecology, 89, 2281–2289.",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Area search</span>"
    ]
  },
  {
    "objectID": "04-theory-area-search.html#footnotes",
    "href": "04-theory-area-search.html#footnotes",
    "title": "4  Area search",
    "section": "",
    "text": "‘Home range’ is used here loosely - a more nuanced explanation would distinguish between the stationary distribution of activity (the home-range utilisation distribution) and the spatial distribution of cues (opportunities for detection) generated by an individual.↩︎",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Area search</span>"
    ]
  },
  {
    "objectID": "05-theory-special-topics.html",
    "href": "05-theory-special-topics.html",
    "title": "5  Special topics",
    "section": "",
    "text": "5.1 Multi-session likelihood\nThe data may comprise multiple independent datasets to be analysed together. We call these ‘sessions’, following the terminology of secr. They may be synchronous spatial samples from non-overlapping populations or samples of the same population widely separated in time. If there are J such datasets we can denote them \\Omega_j, j = 1,...,J. The likelihood to be maximized is then \\prod_j L(\\phi, \\theta | \\Omega_j). While the parameter vector (\\phi, \\theta) is common to all sessions, the mechanism of linear submodels allows ‘real’ parameter values to be specific to a session, common to multiple sessions, or modelled as a function of session-level covariates.\nFailure of the independence assumption in a multi-session analysis results in underestimation of the sampling variance.",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Special topics</span>"
    ]
  },
  {
    "objectID": "05-theory-special-topics.html#sec-groups",
    "href": "05-theory-special-topics.html#sec-groups",
    "title": "5  Special topics",
    "section": "5.2 Groups",
    "text": "5.2 Groups\n\nUsers of MARK (Cooch & White, 2023) will be familiar with the stratification of a population into ‘groups’, each with potentially different values for parameters such as survival probability. Grouping requires that each animal is assigned to a group on first detection, and that the assignment is permanent. Then density and any detection parameter may be modelled as a function of the grouping factor. Group models are available in secr only when maximizing the full likelihood. When maximizing the conditional likelihood the model may specify an individual factor covariate directly, with the same effect as grouping.\nThe multinomial coefficient in secr is stratified by group.",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Special topics</span>"
    ]
  },
  {
    "objectID": "05-theory-special-topics.html#sec-esa",
    "href": "05-theory-special-topics.html#sec-esa",
    "title": "5  Special topics",
    "section": "5.3 Effective sampling area",
    "text": "5.3 Effective sampling area\n\nThe effective sampling area is defined (Borchers & Efford, 2008) as \na(\\theta) \\equiv \\int_{R^2} p_\\cdot(\\mathbf x; \\theta) \\, d\\mathbf x.\n\\tag{5.1}\nThis is a scalar effective area for which \\hat D = n / a(\\hat \\theta) is an unbiased estimate of density. It does not correspond to a geographic region or delimited polygon on the ground. It bears no relation to the traditional ‘effective trapping area’ A_W (e.g., Otis et al., 1978) for which \\hat D = \\hat N / A_W, given a non-spatial population estimate \\hat N and boundary strip width W. Variation in a(\\theta) depends not only on obviously spatial quantities such as the extent of the detector array and the spatial scale of detection \\sigma, but also on non-spatial quantities such as sampling effort (e.g., the number of days of trapping) and the intercept of the distance-dependent detection function (g_0, \\lambda_0).\nGardner et al. (2009) and Royle et al. (2009) defined an ‘effective trapping area’ or ‘effective sample area’ A_e somewhat differently, omitting the intercept of the detection function. To our knowledge their definition has not found further use. A version closer to ours appears in Royle et al. (2014, Section 5.12).\nEfford & Mowat (2014) defined a ‘single-detector sampling area’ a_0(\\theta) = 2\\pi \\lambda_0 \\sigma^2 that is equal to Eq. 5.1 for an isolated detector with detection functions HHN or HEX. For K isolated detectors a(\\theta) = Ka_0(\\theta), but overlap of the ‘catchment areas’ of adjacent detectors leads to a(\\theta) &lt; Ka_0(\\theta).",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Special topics</span>"
    ]
  },
  {
    "objectID": "05-theory-special-topics.html#sec-conditional",
    "href": "05-theory-special-topics.html#sec-conditional",
    "title": "5  Special topics",
    "section": "5.4 Conditional likelihood",
    "text": "5.4 Conditional likelihood\n \nThe detection parameters (\\theta) may be estimated by maximizing the likelihood conditional on n (Eq. 3.3). When density is constant this reduces to\n\nL_n (\\theta| \\Omega) \\propto \\prod_{i=1}^n \\frac{\\int_{R^2} \\mathrm{Pr}(\\omega_i | \\mathbf x; \\theta) \\; d\\mathbf x}{a(\\theta)}.\n Conditioning on n allows individual covariates \\mathbf{z}_i to be included in the detection model, so that the parameter vector takes a potentially unique value \\theta_i = f(\\mathbf{z}_i) for each individual. The effective sampling area then varies among individuals as a function of their covariates. A corresponding Horvitz-Thompson-like derived estimate of density is \\hat D_{HT} = n / \\sum_{i=1}^n a(\\hat \\theta_i) (Borchers & Efford, 2008).\nWhen the conditional likelihood is maximized, the inverse Hessian provides variances for \\theta. The variance of the derived estimate of density depends also on the distribution of n. Following Huggins (1989), \n\\mathrm{var}(\\hat D_{HT}) = s^2 + \\hat {\\mathbf G}^T_\\theta \\hat {\\mathbf I} \\hat {\\mathbf G}_\\theta\n\\tag{5.2} where s^2 is the variance of \\hat D when \\theta is known, \\hat {\\mathbf I} is the estimated information matrix (inverse Hessian), and \\hat {\\mathbf G} is a vector containing the gradients of \\hat D with respect to the elements of \\theta, evaluated at the maximum likelihood estimates. Numerical evaluation of the second term is straightforward.\nWhen the distribution of AC is inhomogeneous Poisson and detections are independent we expect n to have a Poisson distribution. If n is Poisson then s^2 = \\sum_{i=1}^n a(\\hat \\theta_i)^{-2}. This simplifies to n/a(\\hat \\theta)^2 in the absence of individual covariates.\nWhen N(A) is fixed, n is binomial and s^2 = \\sum_{i=1}^n [1 - a(\\hat \\theta_i)/|A|] / a(\\hat \\theta_i)^2. \nThe variance from Eq. 5.2 is on the natural scale, and the Wald confidence interval computed on this scale is symmetrical. Intervals that are symmetric on the log scale and asymmetric on the natural scale have better coverage properties. These are obtained as (\\hat D_{HT}/C, \\hat D_{HT}C) where C = \\exp \\{ z_{\\alpha/2} \\sqrt{\\log[1 + \\frac{\\mathrm{var}(\\hat D_{HT})}{\\hat D_{HT}^2}]} \\} (Burnham et al., 1987; Chao, 1989).",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Special topics</span>"
    ]
  },
  {
    "objectID": "05-theory-special-topics.html#sec-relativedensity1",
    "href": "05-theory-special-topics.html#sec-relativedensity1",
    "title": "5  Special topics",
    "section": "5.5 Relative density",
    "text": "5.5 Relative density\n\nConditioning on n without requiring uniform density as in the previous section leads to another result that is useful. We can estimate relative density, the distribution of AC in relation to habitat covariates, rather than absolute density. Conditioning allows individual covariates of detection \\mathbf z_i as before, and fitting is faster than for absolute density (Efford, 2025).\nWe define relative density by D^\\prime(\\mathbf x | \\phi^-) \\equiv k^{-1} D(\\mathbf x | \\phi) where \\phi^- is a reduced vector of coefficients for the density sub-model and k is a constant of proportionality. Then we can discard the first factor in Eq. 3.1 and maximize a likelihood based on the second factor alone \nL_r(\\phi^-, \\theta | \\Omega, n) \\propto  \\prod_{i=1}^n \\frac{\\int \\mathrm{Pr}(\\omega_i | \\theta, \\mathbf x) \\, D^\\prime(\\mathbf x | \\phi^-) \\, p_\\cdot(\\mathbf x | \\theta, \\mathbf z_i) \\; d\\mathbf x}{\\int  D^\\prime(\\mathbf x | \\phi^-) \\, p_\\cdot(\\mathbf x | \\theta, \\mathbf z_i) \\; d\\mathbf x}.\n\\tag{5.3} For a log link the new parameter vector \\phi^- corresponds to the original \\phi with one coefficient, the intercept, fixed at zero. For an identity link the intercept is fixed at 1 and other coefficients are scaled. A derived estimate of the constant of proportionality is \n\\hat k = \\sum_{i=1}^n 1 / \\int D^\\prime(\\mathbf x|\\hat \\phi^-) p_\\cdot(\\mathbf x| \\hat \\theta, \\mathbf z_i)\\, d \\mathbf x.\n A derived estimate of the absolute density at point \\mathbf x is \n\\hat D(\\mathbf x | n, \\hat \\phi^-, \\hat \\theta) = \\sum_{i=1}^n \\frac{D^\\prime(\\mathbf x| \\hat \\phi^-)}{\\int D^\\prime(\\mathbf x| \\hat \\phi^-) p_\\cdot(\\mathbf x| \\hat \\theta, \\mathbf z_i)\\, d \\mathbf x} .\n\nThis approach makes the same sampling assumptions as the full model, including that individuals enter the sample by a spatial detection process whose parameter vector \\theta we estimate from the data. For poisson-distributed AC, parameter estimates from maximizing Eq. 5.3 are identical to those of the full likelihood except for the missing density intercept.",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Special topics</span>"
    ]
  },
  {
    "objectID": "05-theory-special-topics.html#adjustment-for-spatially-selective-prior-marking",
    "href": "05-theory-special-topics.html#adjustment-for-spatially-selective-prior-marking",
    "title": "5  Special topics",
    "section": "5.6 Adjustment for spatially selective prior marking",
    "text": "5.6 Adjustment for spatially selective prior marking\n\nIn some scenarios, the only individuals at risk of detection are those that were marked in an earlier phase of the study. This is the case for the automated detection of animals with implanted acoustic telemetry tags or passive integrated transponders (PIT) (e.g., Whoriskey et al., 2019). The data resemble mark–resight data (Appendix E) except that they do not include counts of unmarked animals. Estimates of \\phi^- using Eq. 5.3 then describe only the distribution of the individuals marked previously. Even if we can assume that AC are stationary between the two phases, \\phi^- is biased as a model for population distribution as it incorporates the spatial selectivity of marking.\nLack of information on the marking phase also afflicts mark-resight analyses (e.g., Efford & Hunter, 2018). The possible ‘solutions’ are all ad hoc. We can assume that the probability of becoming marked was independent of location, or we can scale p_\\cdot(\\mathbf x; \\phi^-) by an externally computed variable q(\\mathbf x) that is proportional to the spatial probability of marking. Essentially, q(\\mathbf x) is an offset in the model for relative density.\nAn example is shown later. A model with flat (constant) probability of prior marking is inevitably a poor fit because in reality tagged individuals are concentrated near the detectors.",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Special topics</span>"
    ]
  },
  {
    "objectID": "05-theory-special-topics.html#population-size-n",
    "href": "05-theory-special-topics.html#population-size-n",
    "title": "5  Special topics",
    "section": "5.7 Population size N",
    "text": "5.7 Population size N\n\nPopulation size is the number of individual AC in a particular region; we denote this N(A) for region A. For a flat density surface \\mathrm{E}[N(A)] = D.|A| where |A| is the area of A.\nD and N(A) are interchangeable for specified A. In most applications of SECR there is no naturally defined region A, and therefore \\hat N(A) depends on the arbitrary choice of A. This weakness is not shared by \\hat D.\n\n\n\n\n\n\nAbundance\n\n\n\n\n\nPopulation size is sometimes termed ‘abundance’. We avoid this usage because ‘abundance’ has also been used as an umbrella term for density and population size, and its overtones are vague and biblical rather than scientific.\n\n\n\nThe population size of any region B may be predicted post hoc from a fitted density model using \n\\hat N(B) = \\int_B \\hat D(\\mathbf x) \\, d\\mathbf x.\n The prediction variance of \\hat N(B) follows from Poisson assumptions regarding D(\\mathbf x) (Efford & Fewster, 2013).",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Special topics</span>"
    ]
  },
  {
    "objectID": "05-theory-special-topics.html#sec-finitemixtures",
    "href": "05-theory-special-topics.html#sec-finitemixtures",
    "title": "5  Special topics",
    "section": "5.8 Finite mixture models",
    "text": "5.8 Finite mixture models\n\nFinite mixture models for individual heterogeneity of capture probability were formalised for non-spatial capture–recapture by Pledger (2000) and remain widely used (e.g., Cooch & White, 2023). These are essentially random-effect models in which the distribution of capture probability comprises two or more latent classes, each with a capture probability and probability of membership.\nBorchers & Efford (2008, p. 381) gave the likelihood for a Poisson SECR model with U latent classes in proportions \\psi = (\\psi_1, …, \\psi_U). In all examples we have tried U is 2 or 3. For each class u there is an associated vector of detection parameters \\theta_u (collectively \\theta). Omitting the constant multinomial term, \n\\mathrm{Pr}(\\Omega | n, \\phi,\\theta, \\psi) \\propto \\prod_{i=1}^n \\sum_{u=1}^U\\int\\frac{\\mathrm{Pr}\\{\\omega_i | \\mathbf x; \\theta_u\\}}{p_\\cdot(\\mathbf x; \\theta_u)}\nf(\\mathbf x, u | \\omega_i&gt;0) \\,d\\mathbf x\n where \nf(\\mathbf x, u | \\omega_i &gt; 0) = \\frac{D(\\mathbf x; \\phi) p_\\cdot(\\mathbf x; \\theta_u) \\psi_u}{\\sum_{u=1}^U \\int D(\\mathbf x; \\phi) p_\\cdot(\\mathbf x; \\theta_u) \\psi_u \\; d\\mathbf x}.\n\nThe expected number of detected animals n, replacing Eq. 3.2, is a weighted sum over latent classes: \n\\Lambda(\\phi, \\theta,\\psi) = \\sum_{u=1}^U \\psi_u \\int D(\\mathbf x; \\phi) p_\\cdot(\\mathbf x;\\theta_u) \\; d\\mathbf x.\n\\tag{5.4} Integration is over points in potential habitat, as usual.",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Special topics</span>"
    ]
  },
  {
    "objectID": "05-theory-special-topics.html#sec-hybridmixtures",
    "href": "05-theory-special-topics.html#sec-hybridmixtures",
    "title": "5  Special topics",
    "section": "5.9 Hybrid finite mixture models",
    "text": "5.9 Hybrid finite mixture models\n\nWe can modify the finite mixture likelihood for data in which the class membership of some or all individuals is known. Indicate the class membership of the i-th individual by a variable u_i that may take values 0, 1, …, U, where u_i = 0 indicates an individual of unknown class, and the class frequencies are n_0, n_1, …, n_U (not to be confused with n_1,…,n_C in Eq. 3.3). We assume here that detection histories are sorted by class membership, starting with the unknowns.\nThe expression for \\lambda in Eq. 5.4 is unchanged, but we must split \\mathrm{Pr}(\\Omega | n, \\phi, \\theta, \\psi) and include a multinomial term for the observed distribution over classes:\n\n\\begin{split}\n\\mathrm{Pr}(\\Omega | n, \\phi,\\theta, \\psi) \\propto \\; &\\prod_{i=1}^{n_0}\\sum_{u=1}^U\\int\\frac{\\mathrm{Pr}\\{\\omega_i | \\mathbf x; \\theta_u\\}}{p_\\cdot(\\mathbf x ; \\theta_u)}\nf(\\mathbf x ,u|\\omega_i&gt;0) \\; d\\mathbf x  \\\\\n&\\times\n\\prod_{i={n_0+1}}^n  \\int \\frac{\\mathrm{Pr}\\{\\omega_i | \\mathbf x; \\theta_{u_i}\\}}{p_\\cdot(\\mathbf x ; \\theta_{u_i})}\nf'(\\mathbf x  | \\omega_i&gt;0; u_i) \\; d\\mathbf x  \\\\\n&\\times {n - n_0 \\choose n_1, ...,n_U}\n\\prod_{u=1}^U \\left[  \\frac{\\lambda_u}{\\lambda} \\right] ^{n_u},\n\\end{split}\n\\tag{5.5}\nwhere \\lambda_u = \\psi_u \\int D(\\mathbf x ) p_\\cdot(\\mathbf x ; \\theta_u) \\; d\\mathbf x, and the multinomial coefficient {n - n_0 \\choose n_1, ...,n_U} is a constant that can be omitted. Rather than representing the joint probability density of \\mathbf x and u_i as in f(\\cdot) previously, f'(\\cdot) is the probability density of \\mathbf x for given u_i: \n  f'(\\mathbf x  | \\omega_i &gt; 0; u_i) = \\frac{D(\\mathbf x )p_\\cdot(\\mathbf x ; \\theta_{u_i})}{\\int D(\\mathbf x )p_\\cdot(\\mathbf x ; \\theta_{u_i}) d\\mathbf x }.\n\nThe likelihood conditions on the number of known-class animals detected (n-n_0), rather than modelling class identification as a random process. It assumes that the probability that class will be recorded does not depend on class, and that such recording when it happens is without error.\nFor homogeneous density the likelihood simplifies to \n\\begin{split}\n\\mathrm{Pr}(\\Omega | n, \\phi,\\theta, \\psi) \\propto &\\prod_{i=1}^{n_0}\\sum_{u=1}^U\\int\\frac{\\mathrm{Pr}\\{\\omega_i | \\mathbf x; \\theta_u\\} \\psi_u} {\\sum_u a(\\theta_u) \\psi_u } \\; d\\mathbf x  \n\\\\\n&\\times\n\\prod_{i={n_0+1}}^n  \\int \\frac{\\mathrm{Pr}\\{\\omega_i | \\mathbf x; \\theta_{u_i}\\}}{a(\\theta_{u_i})} \\;\nd\\mathbf x  \n\\prod_{u=1}^U {  \\left[ \\frac{a(\\theta_u)\\psi_u} {\\sum_u a(\\theta_u) \\psi_u } \\right] ^{n_u}},\n\\end{split}\n\nwhere a(\\theta_u) = \\int p_\\cdot(\\mathbf x ; \\theta_{u}) \\; d\\mathbf x.",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Special topics</span>"
    ]
  },
  {
    "objectID": "05-theory-special-topics.html#sec-parameterizations",
    "href": "05-theory-special-topics.html#sec-parameterizations",
    "title": "5  Special topics",
    "section": "5.10 Alternative parameterizations",
    "text": "5.10 Alternative parameterizations\n\nThe ‘real’ parameters in SECR are typically assumed to be independent (orthogonal). However, some parameter pairs co-vary in predictable ways owing to constraints on animal behaviour. Here it is more straightforward to work with the hazard detection functions. The intercept of the detection function \\lambda_0 declines with increasing \\sigma, all else being equal (Efford & Mowat, 2014). This is inevitable if detection is strictly proportional to time spent near a point, given a bivariate home range utilisation model (pdf for activity). Also, home-range size and the SECR parameter \\sigma decline with population density (Efford et al., 2016). Allowing for covariation may improve biological insight and lead to more parsimonious models.\nCovariation may be ‘hard-wired’ into SECR models by reparameterization. In each case a new ‘surrogate’ parameter is proportional to a combination of the co-varying parameters. One of the co-varying parameters is seen as driving variation, while the other is inferred from the surrogate and the driver. Deviations from the expected covariation are implied when the surrogate is found to vary (i.e. a model with varying surrogate is superior to a model with constant surrogate).\nFor concreteness, consider a difference in home range size over the seasons, causing variation in \\sigma. A reasonable null hypothesis is that there will be reciprocal seasonal variation in \\lambda_0 such that a_0 = 2 \\pi \\lambda_0 \\sigma^2 is constant. Here variation in \\sigma is the driver, a_0 is the surrogate, and \\lambda_0 is derived.\n\n\n\n\n\nTable 5.1: Parameterizations implementing three covariation models. The third option combines the first two. In secr, a_0 is called ‘a0’ and k is called ‘sigmak’.\n\n\n\n\n\n\n\n\n\n\n\n\nParameters\nDriver\nSurrogate\nDerived\nEffect\n\n\n\n\n(D, a_0, \\sigma)\n\\sigma\na_0\n\\lambda_0 = a_0 / (2 \\pi \\sigma^2)\nReciprocal \\sigma^2, \\lambda_0\n\n\n(D, \\lambda_0, k)\nD\nk\n\\sigma = k/\\sqrt D\nDensity-dependent \\sigma\n\n\n(D, a_0, k)\nD\nk, a_0\n\\sigma = k/\\sqrt D\nboth\n\n\n\n\n\n\\lambda_0 = a_0 / (2 \\pi \\sigma^2)\n\n\n\n\n\n\n\nSee Appendix H for further detail on alternative parameterizations and their implementation in secr.",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Special topics</span>"
    ]
  },
  {
    "objectID": "05-theory-special-topics.html#model-based-location-of-ac",
    "href": "05-theory-special-topics.html#model-based-location-of-ac",
    "title": "5  Special topics",
    "section": "5.11 Model-based location of AC",
    "text": "5.11 Model-based location of AC\n\nAssume we have fitted a spatial model by integrating over the unknown locations of AC for a given SECR dataset \\Omega = {\\omega_1, \\omega_2, ..., \\omega_n}. We may retrospectively infer the probability density of the AC corresponding to each detection history, using the model and the estimated parameters: \nf(\\mathbf x | \\omega_i; \\hat \\phi, \\hat \\theta) = \\frac{ \\mathrm{Pr} (\\omega_i | \\mathbf x; \\hat \\theta) D(\\mathbf x ; \\hat \\phi)}\n{\\int_{R^2} \\mathrm{Pr}(\\omega_i | \\mathbf x; \\hat \\theta) D(\\mathbf x ; \\hat \\phi) \\, d \\mathbf x}.\n\\tag{5.6} This is equivalent to the posterior distribution of each latent AC in Bayesian applications. See fxi and related functions in secr. For known \\theta and known \\phi (unless D(\\mathbf x; \\phi) uniform) the modal location of the AC for animal i may be estimated by maximizing \\mathrm{Pr} (\\omega_i | \\mathbf x; \\theta)D(\\mathbf x; \\phi) with respect to \\mathbf x. The distribution often has more than one mode for animals at the edge of a detector array or searched area. It should not be confused with the home range utilisation pdf.\nSection 11.6 discusses the use and interpretation of f(\\mathbf x | \\omega_i; \\hat \\phi, \\hat \\theta) (see also Durbach et al., 2024).\n\n\n\n\n\n\nBorchers, D. L., & Efford, M. G. (2008). Spatially explicit maximum likelihood methods for capture-recapture studies. Biometrics, 64, 377–385.\n\n\nBurnham, K. P., Anderson, D. R., White, G. C., Brownie, C., & Pollock, K. H. (1987). Design and analysis methods for fish survival experiments based on release-recapture. American Fisheries Society.\n\n\nChao, A. (1989). Estimating population size for sparse data in capture-recapture experiments. Biometrics, 45(2), 427. https://doi.org/10.2307/2531487\n\n\nCooch, E., & White, G. (Eds.). (2023). Program MARK: A gentle introduction. (23rd ed.). http://www.phidot.org/software/mark/docs/book/\n\n\nDurbach, I., Chopara, R., Borchers, D. L., Phillip, R., Sharma, K., & Stevenson, B. C. (2024). That’s not the mona lisa! How to interpret spatial capture-recapture density surface estimates. Biometrics, 80. https://doi.org/10.1093/biomtc/ujad020\n\n\nEfford, M. G. (2025). Spatially explicit capture–recapture models for relative density. BioRxiv. https://doi.org/10.1101/2025.01.22.634401\n\n\nEfford, M. G., Dawson, D. K., Jhala, Y. V., & Qureshi, Q. (2016). Density-dependent home-range size revealed by spatially explicit capture-recapture. Ecography, 39, 676–688.\n\n\nEfford, M. G., & Fewster, R. M. (2013). Estimating population size by spatially explicit capture-recapture. Oikos, 122, 918–928.\n\n\nEfford, M. G., & Hunter, C. M. (2018). Spatial capture-mark-resight estimation of animal population density. Biometrics, 74, 411–420. https://doi.org/10.1111/biom.12766\n\n\nEfford, M. G., & Mowat, G. (2014). Compensatory heterogeneity in spatially explicit capture-recapture data. Ecology, 95, 1341–1348.\n\n\nGardner, B., Royle, J. A., & Wegan, M. T. (2009). Hierarchical models for estimating density from DNA mark-recapture studies. Ecology, 90, 1106–1115.\n\n\nHuggins, R. M. (1989). On the statistical analysis of capture experiments. Biometrika, 76, 133–140.\n\n\nKendeigh, S. C. (1944). Measurement of bird populations. Ecological Monographs, 14, 67–106. https://doi.org/10.2307/1961632\n\n\nOtis, D. L., Burnham, K. P., White, G. C., & Anderson, D. R. (1978). Statistical inference from capture data on closed animal populations. Wildlife Monographs, 62.\n\n\nPledger, S. (2000). Unified maximum likelihood estimates for closed capture-recapture models using mixtures. Biometrics, 56, 434–442.\n\n\nRoyle, J. A., Chandler, R. B., Sollmann, R., & Gardner, B. (2014). Spatial capture-recapture. Academic Press.\n\n\nRoyle, J. A., Nichols, J. D., Karanth, K. U., & Gopalaswamy, A. M. (2009). A hierarchical model for estimating density in camera‐trap studies. Journal of Applied Ecology, 46(1), 118–127. https://doi.org/10.1111/j.1365-2664.2008.01578.x\n\n\nWhoriskey, K., Martins, E. G., Auger‐Méthé, M., Gutowsky, L. F. G., Lennox, R. J., Cooke, S. J., Power, M., & Mills Flemming, J. (2019). Current and emerging statistical techniques for aquatic telemetry data: A guide to analysing spatially discrete animal detections. Methods in Ecology and Evolution, 10(7), 935–948. https://doi.org/10.1111/2041-210x.13188",
    "crumbs": [
      "Theory",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Special topics</span>"
    ]
  },
  {
    "objectID": "06-assumptions.html",
    "href": "06-assumptions.html",
    "title": "6  Assumptions and robustness",
    "section": "",
    "text": "6.1 Robustness\nFor assumptions that cannot be met by better design or customized modelling, we rely on the robustness of SECR estimators. A robust estimator gives estimates that are close to the truth even when assumptions have been breached.  We care most about estimates of population density, which may be robust even when estimates of other parameters are not.\nOur main criterion will be the relative bias of an estimator, abbreviated RB and defined for an estimator \\hat \\theta of any arbitrary parameter \\theta as \n\\mathrm{RB}(\\hat \\theta) = \\frac{\\mathrm{E} (\\hat \\theta - \\theta)}{\\theta}.\n Robustness is estimated in practice by obtaining \\hat \\theta for a large sample of simulated datasets. Simulation allows the statistician to control precisely any deviation from the assumptions. There have now been several simulation studies that we review below. We supplement these with new simulations that we outline below and describe in full in the GitHub repo MurrayEfford/secr-simulations.\nBreaches of assumptions may also impair estimates of sampling variance, leading to poor coverage of confidence intervals even when an estimator is unbiased. ‘Poor coverage’ here means that the true value lies outside the computed confidence interval more often (or sometimes less often) than expected by chance given the nominal level (e.g. 95%). Coverage of simulated intervals is therefore a further criterion.\nSimulations cannot span the full range of scenarios, so the results are only indicative. Ideally we would supplement simulations with field validations of the method, but as we see in Chapter 7 these are difficult to execute and interpret.\nAnother approach is to test whether a particular dataset is consistent with each assumption. This was popular in the past (Otis et al., 1978), but has lost ground along with the declining credibility of null hypothesis testing, the growth of modelling frameworks, and reliance on the inherent robustness of the spatial methods.\nOur simulations use a base scenario of sampling with a square grid of 64 binary proximity detectors operated for 10 occasions; detector spacing is 2\\sigma for a hazard half-normal detection function with \\lambda_0 = 0.1. A population with density 0.5\\sigma^{-2} is distributed uniformly at random in a region extending 4\\sigma beyond the detectors. We report relative bias of density estimates and detection parameters under a null model – one with uniform density and no additional effects on detection – except where stated. The range |RB| &lt; 10% is shaded on the graphs, and bars indicate 95% confidence limits, although these are often obscured by the plotted points. Off-scale values are flagged with an asterisk (*).",
    "crumbs": [
      "Performance",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Assumptions and robustness</span>"
    ]
  },
  {
    "objectID": "06-assumptions.html#sec-popnclosed",
    "href": "06-assumptions.html#sec-popnclosed",
    "title": "6  Assumptions and robustness",
    "section": "Assumption 1: Population closed",
    "text": "Assumption 1: Population closed\n\nBirths, deaths and dispersal result in population turnover. Over an extended period of turnover, more animals may be observed than were present at any instant, and density estimates will be biased upwards. Studies using automatic cameras often accumulate data slowly over many days, and study duration has triggered much angst (e.g., Harihar et al., 2017). How much does a little turnover matter? Conversely, How long can the sampling period be? Results for non-spatial models (Kendall, 1999) cannot be transferred. We distinguish turnover due to movement of AC from in situ births and deaths, and consider it separately under Assumption 3d.\nDupont et al. (2019) investigated the effect of increasing study duration on the precision and bias of population size estimates. Their results are complicated by an artifact that caused their Bayesian estimator to be positively biased for short durations (Dupont et al., 2019, p. 669).\n\nAdditional simulations are shown in Fig. 6.1. Turnover in a constant population resulted in positive bias equal to about 70% of the mortality over the duration of the sampling. Thus 50% annual mortality (~16% over 3 months) resulted in about +11% relative bias in a 3-month study, and coverage of nominal 95% confidence intervals dropped to about 82%. The new results are broadly consistent with those from the more complex scenarios of Dupont et al. (2019) for ‘slow’ and ‘intermediate’ life histories.\n\n\n\n\n\n\n\nFigure 6.1: Simulated effect of study duration on relative bias of density estimates for three levels of annual survival \\phi in a population with constant density. \\phi = 1 corresponds to a closed population.\n\n\n\n\nAssumption 1b: Population closed to immigration and emigration\nThis overlaps with Assumption 3 and is covered later.",
    "crumbs": [
      "Performance",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Assumptions and robustness</span>"
    ]
  },
  {
    "objectID": "06-assumptions.html#sec-noIDerror",
    "href": "06-assumptions.html#sec-noIDerror",
    "title": "6  Assumptions and robustness",
    "section": "Assumption 2: Identification without error",
    "text": "Assumption 2: Identification without error\n\nWe can generally assume accurate identification on recapture of animals trapped and marked by conventional methods such as numbered leg bands or ear tags. However, identification may be unreliable with modern methods for passive sampling using natural marks (DNA from hair or faecal samples, and images from motion-sensitive cameras). This is a major limitation.\nIn genetic sampling there are two possible reasons for mis-identification: (i) there is too little variation at the chosen loci to distinguish all individuals in the sampled population, and (ii) genotyping is subject to error. Mills et al. (2000), Lukacs & Burnham (2005), Waits & Paetkau (2005), and Lampa et al. (2013) reviewed the early literature, and citations of those reviews are a good entry point to the voluminous recent literature. Sethi et al. (2014) provide technical advice. Augustine et al. (2020) provide a useful summary and a model framework that encompasses the various errors. Kodi et al. (2024) is a recent SECR study.\nNatural marks such as coat patterns are prone to identification problems that parallel those from genotyping: individuals may not be distinguishable or some may be mis-recorded, leading to spurious ‘ghost’ individuals. The robustness of estimates will depend on the likely magnitude of each effect. We address the bias for varying frequencies of each effect below, with particular reference to genotyping.\nA single camera may photograph only the left or right flank of a passing animal. Owing to the asymmetry of patterns, identity cannot be inferred conclusively from a single photograph. This identification problem can be addressed in the field by using paired cameras, so that both flanks are recorded (Karanth & Nichols, 1998). There may still be a minority of single-sided photographs, and probabilistic models have been suggested to incorporate these (Augustine et al., 2018).\nAssumption 2a. Natural marks sufficiently diverse\nThe ability to distinguish individuals is measured by the probability of identity (PI). This is the probability that two individuals drawn at random from the population will appear the same, i.e. have the same genotype at the loci examined. Identity of genotypes was considered by Mills et al. (2000) to cause a “shadow effect”, as the existence of some individuals is concealed. The problem for SECR is even greater than in non-spatial capture–recapture. A group of two or more indistinguishable individuals becomes a ‘super individual’ whose detections spread over a larger area than each occupies individually.\nSimulations in Fig. 6.2 and Fig. 6.3 illustrate the potential impact of shadow effects on SECR estimates. In a scenario with about 110 detected individuals, PI = 10^{-3} resulted in relative bias of -20\\% in density estimates. This is largely due to the inflated spatial footprint of each super individual, which causes positive bias in estimates of the spatial scale of detection \\sigma.\n\n\n\n\n\n\n\nFigure 6.2: Relative bias of parameter estimates from null model when some individuals cannot be distinguished (‘shadow effects’).\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.3: Relative bias of density estimates as function of PI for varying density. Density is expressed in units of animals per \\sigma^2. At low density few individuals are detected, reducing the chance of mis-identification for given PI.\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.4: Threshold of PI at which absolute relative bias of density estimates exceeded the threshold shown, as a function of the expected number of individuals detected.\n\n\n\n\nAssumption 2b. Natural marks not corrupted\nDNA samples degrade over time exposed to heat, moisture and UV light (e.g., Woodruff et al., 2014). This results in both a lower rate of successful DNA amplification, and increasing frequency of genotyping errors. A key genotyping error is the phenomenon of allelic dropout, when one allele at a heterozygous locus fails to amplify, resulting in an apparent homozygote. False alleles may also appear in the laboratory and give the appearance of a distinct genotype and individual. Either error is likely to result in a spurious ‘ghost’ individual that is never recaptured.\nWe measure the ghost effect by the probability a detection results in a ‘ghost’ individual. Unlike shadow effects, ghost individuals have almost no effect on \\hat \\sigma. However, they do cause negative bias in \\hat \\lambda_0 and a reciprocal (positive) bias in density estimates \\hat D (Fig. 6.5). More extensive simulations were published by Kodi et al. (2024).\n\n\n\n\n\n\n\nFigure 6.5: Relative bias of parameter estimates from null model when genotyping errors generate ‘ghost’ individuals.\n\n\n\n\n\n\n\n\n\n\nGiven sufficient diversity at the chosen loci, culling of inadequate samples and intensive checking of doubtful genotypes is generally sufficient to ensure adequate data (Paetkau, 2003). Lukacs & Burnham (2005) raised doubts about possible biases due to sample culling, but these have not been confirmed. Elimination of poor samples for which identity is uncertain tends to reduce sample size and precision. Precision can be improved by judiciously including samples with fewer loci in models that allow for uncertain identity (Augustine et al., 2020).\nUnknown identity can be handled in SECR-like probability models. These generally lack power unless supplemented by detections of known individuals or telemetry (references in Appendix G). Bias due to ghost individuals may be reduced in some circumstances by modelling only detection histories with more than one detection (Kodi et al., 2024).",
    "crumbs": [
      "Performance",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Assumptions and robustness</span>"
    ]
  },
  {
    "objectID": "06-assumptions.html#assumption-3-detection-declines-radially-from-randomly-located-fixed-ac",
    "href": "06-assumptions.html#assumption-3-detection-declines-radially-from-randomly-located-fixed-ac",
    "title": "6  Assumptions and robustness",
    "section": "Assumption 3: Detection declines radially from randomly located, fixed AC",
    "text": "Assumption 3: Detection declines radially from randomly located, fixed AC\nThe spatial component of spatially explicit capture–recapture rests on quite specific spatial models for the population (a 2-D distribution of activity centres AC) and for detection (a declining function of distance from the AC). These follow from a biological model in which detection hazard is proportional to each animal’s utilisation distribution i.e., its home range conceived as a stationary 2-dimensional probability density function (van Winkle, 1975).\nAssumption 3a: AC locations independent\n\nThe SECR model of Borchers & Efford (2008) treats activity centres as distributed independently according to an inhomogeneous Poisson point process. This allows local density to vary according to habitat or other persistent effects. It does not allow for the spacing behaviour of the animals themselves, which can lead to either contagion or repulsion of AC. Several authors have ventured into this area (Bischof et al., 2020; Efford & Fletcher, 2024; López-Bao et al., 2018; McLaughlin & Bar, 2020; Reich & Gardner, 2014; Russell et al., 2012). The general conclusion is that point estimates of density from SECR are robust to clustering of AC due to social behaviour, but the implied overdispersion leads to confidence limits that are too narrow. Non-independence of detection is a distinct issue (Assumption 4).\nAssumption 3b: Home ranges circular\n\nConcerns were expressed by Ivan et al. (2013) about the effect of non-circularity of home ranges. Simulations by Efford (2019) generally defused those concerns, with an important caveat: estimates of the spatial scale of detection \\sigma and density are unreliable when elongated home ranges are sampled with a linear array of detectors. Simulations of randomly oriented elliptical ranges with an aspect ratio of 3:1 resulted in bias on the order of +13% for a linear array (Efford, 2019: Fig. 2). Bias is extreme (often &gt;50%, Efford, 2019: Table 1) when home ranges are both elongated and have a common alignment to the array (Fig. 6.6 c). Modelling the anisotropy can be beneficial when the alignment is predictable from the landscape (Efford, 2019; Murphy et al., 2016; Murphy & Luja, 2025) and the array is not exactly linear, but the method is not universally applicable.\n\n\n\n\n\n\n\nFigure 6.6: SECR models typically assume that home ranges are circular (a). Noncircularity of home ranges causes limited bias in density estimates when ranges are oriented at random with respect to a linear detector array (b). Systematic alignment of ranges to the array (c) causes large bias in \\hat \\sigma and \\hat D.\n\n\n\n\nAssumption 3c: Locations independent within home range\n\nIt is entirely likely that animals use their home ranges in a nested fashion (i.e., activity during any sub-interval localised in part of the range) or that some AC move during sampling. This may be viewed as a subset of Assumption 5 because if AC are not fixed during sampling then successive locations of an individual will be autocorrelated.\nRoyle et al. (2015) modelled movement patterns they called “transience” (a Gaussian random walk) and “dispersal” (discrete shift of AC). They concluded\n\n… while estimators of density are extremely robust, even to pathological levels of movement (e.g., complete transience), the estimator of the spatial scale parameter of the encounter probability model is confounded with the dispersal/transience scale parameter.\n\nWe extend their simulations by considering a model in which the overall home range is stationary but locations within the home range are autocorrelated. The bivariate Ornstein-Uhlenbeck (OU) distribution is a convenient model with these properties (Dunn & Gipson, 1977; Hooten et al., 2017; D. S. Johnson et al., 2008). The home range as a whole is bivariate normal; we simulate the uncorrelated, circular case with equal variance on both axes. R code and details of the simulations are provided on GitHub.\nFig. 6.7 illustrates OU movement tracks for individuals with increasing autocorrelation parameter \\tau. Detection is assumed here to happen when an individual is within some small threshold distance \\epsilon of a detector at the end of a time step. Other detection models are possible.\n\n\n\n\n\n\n\nFigure 6.7: Examples of correlated movement paths over 100 time steps (t = 100). Parameter ‘tau’ (\\tau) controls autocorrelation. Red dot indicates activity centre; grey circle is 95% probability contour of distribution as t \\to \\infty or \\tau \\to 0.\n\n\n\n\nOne consequence of autocorrelated movement is that the overall spatial scale increases with the duration and decreases with autocorrelation. With respect to telemetry, Otis & White (1999) concluded that autocorrelation per se did not bias inference so long as summary statistics were not generalized beyond the temporal sampling frame:\n\nSampling designs that predefine a time frame of interest, and that generate representative samples of an animal’s movement during this time frame, should not be affected by length of the sampling interval and autocorrelation.\n\nThe analogy with SECR is close: we are concerned with unbiased estimation of the detection parameters \\lambda_0, \\sigma that describe the detection process over the time frame of sampling. Extrapolation to other time frames would require knowledge of the correlation structure of the data, expressed in a model such as the bivariate Ornstein-Uhlenbeck distribution (e.g., Dunn & Gipson, 1977; Hooten et al., 2017), but that is not relevant to inference for the chosen time frame.\n\n\n\n\n\n\n\nFigure 6.8: Relative bias RB of null-model density estimates of density D and spatial scale \\sigma from Ornstein-Uhlenbeck model with autocorrelation parameter \\tau.\n\n\n\n\nFig. 6.8 confirms the robustness of SECR null-model density estimates to serial correlation of location over a broad range of values (0 \\le \\tau \\le 50). Bias is apparent in estimates of the global \\sigma, as expected from the reduced extent of movements in a given time frame. Note here that autocorrelation induces negative bias in \\hat \\sigma. There is no direct analogue of \\lambda_0 in the OU generating model, so we cannot determine the bias in \\hat \\lambda_0.\nAssumption 3d. Home ranges stationary\n\nRobustness of SECR to movement of home ranges (dispersal) was demonstrated by Royle et al. (2015). Harihar et al. (2017) expressed a further concern that movement over a long sampling duration would breach Assumption 1 (closure).\nWe simulated detections over 100 ‘days’ during which each AC underwent a random walk governed by a Gaussian kernel with scale \\sigma_m. Details are given [elsewhere].\n\n\n\n\n\n\n\nFigure 6.9: Relative bias of density estimates from studies with varying between-occasion movement. Both \\hat \\lambda_0 and \\hat \\sigma are biased by movement, but the net effect on \\hat D is negligible.",
    "crumbs": [
      "Performance",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Assumptions and robustness</span>"
    ]
  },
  {
    "objectID": "06-assumptions.html#assumption-4-probability-of-detection-constant",
    "href": "06-assumptions.html#assumption-4-probability-of-detection-constant",
    "title": "6  Assumptions and robustness",
    "section": "Assumption 4: Probability of detection constant",
    "text": "Assumption 4: Probability of detection constant\n\nOtis et al. (1978) classified non-spatial capture–recapture models according to three possible sources of variation in detection probability: time (t – sampling occasion), behaviour (b – learned response to capture) and heterogeneity (h – persistent individual differences). Models may accommodate any one of these sources (t, b, h) or their combinations (tb, th, bh, tbh). Space adds other sources of variation – most simply, detection probability may also vary between detectors (d) – and more complex potential interactions.\nFor SECR we have at least two parameters that control detection (g_0 or \\lambda_0, and \\sigma).\nDetection parameters may also be considered a function of AC location, but for (relative) simplicity we stick to t, b, h, and d.\n4a. Temporal variation\nSollmann (2024) reached the conclusion from simulations that temporal variation in the baseline detection probability g_0 may safely be ignored when fitting SECR models. Density estimates from null and temporal models are usually the same or nearly so.\nFurther simulations confirm the lack of bias in \\hat D, but show that temporal variation in \\sigma causes significant bias in estimates of both \\lambda_0 and \\sigma under the null model (Fig. 6.10).\n\n\n\n\n\n\n\nFigure 6.10: Relative bias of parameter estimates from null model given temporal variation in \\lambda_0 or \\sigma.\n\n\n\n\n4b. Behavioural responses\nWe introduced behavioural responses in Chapter 10. Behavioural responses may also be treated as a breach of the assumption of serial independence, but we keep them here for consistency with the non-spatial literature.\nThe discovery of a detector and experience of capture may result in either a positive (trap-happy) or negative (trap-shy) change in detection probability. The response may persist for the duration of sampling or be transient (Markovian) and apply only at the next sampling occasion. In SECR there is the further complication that the response may be general, applying across all detectors, or localised to the initial detector.\nEach of these responses is readily modelled if the sequence of detections is known. This is generally not the case with proximity detectors, for which an animal may visit multiple detectors on one sampling occasion. We therefore restrict consideration of learned responses to trapping data (single-catch and multi-catch traps). This is not a great loss, as proximity detectors are typically non-invasive and less likely than traps to affect behaviour.\nIf a behavioural response is not modelled then it may cause a heavy bias in density estimates, positive for trap shyness and negative for trap happiness (Fig. 6.11). In simulations we describe the magnitude of the behavioural response by a ‘recapture factor’ by which the initial (naive) hazard is multiplied after first capture. We simulated recapture factors between 0.25 and 2.0, where 1.0 represents no behavioural response. The bias is much reduced if the effect is detector-specific, but then \\hat \\sigma is also biased. For the scenario used in these simulations, the relative bias of density estimates fell between -10% and +10% for recapture factors between 0.5 and 2. In Chapter 10 we estimated a much larger response by deermice (about 10-fold, but with poor precision), although there was only a small difference in estimated density between the null and bk models. Behavioural responses are potentially important for SECR and deserve closer investigation.\n\n\n\n\n\n\n\nFigure 6.11: Relative bias of parameter estimates from null model given behavioural variation in lambda0. Relative bias of density was off-scale (+1.41) for recapture factor 0.25 with a general behavioural response.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAnimals may encounter a detector without being detected. For example, a camera flash may be triggered when an animal is out of frame, or a trap door may fall without a clean capture. If such events cause avoidance then the behavioural response leaves no trace in the histories of detected animals and cannot be fully modelled. We would nevertheless expect some evidence for the aversive effect within detected histories.\n\n\n4c. Individual heterogeneity\nIndividual heterogeneity has long been the bane of capture–recapture. Some of the variation is due to proximity to detectors, and does not bias SECR estimates because it is included in the model. A modest level of additional variation has little effect on null-model estimates: Efford & Mowat (2014) found that the relative bias of \\hat D did not exceed -0.05 when \\mathrm{CV}(a_0) &lt; 0.3 for a_0 = 2\\pi\\lambda_0\\sigma^2 (based on hazard halfnormal or exponential detection function). We confirmed this in simulations that varied each of the components \\lambda_0 and \\sigma^2 separately (Fig. 6.12). Biologically we expect an inverse relationship between \\lambda_0 and \\sigma^2, so in general \\mathrm{CV}(a_0) will be less than either \\mathrm{CV}(\\lambda_0) or \\mathrm{CV}(\\sigma^2) on its own (Efford & Mowat, 2014).\nDespite these appeals to robustness, Fig. 6.12 indicates a risk of significant bias in estimates of both density and detection parameters when individual heterogeneity is large.\n\n\n\n\n\n\n\nFigure 6.12: Relative bias of parameter estimates from null model given individual variation in \\lambda_0 and \\sigma^2.\n\n\n\n\n4d. Homogeneity across detectors\nSpatial (detector-specific) variation in detection parameters is conceptually linked to individual heterogeneity. The location of each individual determines its detection probability via a particular set of AC-to-detector distances, as modelled automatically in SECR. Variation among detectors in efficiency (\\lambda_{0_k}) adds between-animal heterogeneity because the detectors near a particular AC differ in efficiency from the average (Fig. 6.13), and the variation is not modelled automatically.\n\n\n\n\n\n\n\nFigure 6.13: Variation in detector efficiency induces individual heterogeneity in overall detection probability p_\\cdot(\\vec x). AC 1–3 have similar exposure to detectors but experience respectively high, low and medium detection probability because of the neighbourhoods in which they are located.\n\n\n\n\nThe individual heterogeneity induced by between-detector variation is increased by spatial autocorrelation on the scale of home ranges, because this makes it more likely that all detectors in the neighborhood of an AC will be above average or below average. Variable \\lambda_{0_k} may also affect null-model estimates indirectly owing to bias in \\hat \\sigma and perhaps \\hat \\lambda_0, but these interactions are poorly understood and we do not consider them further.\nA Gaussian random field (GRF) is a convenient source of random spatial variation. Spatial structure of a GRF is controlled by its covariance function, which is typically exponential with a single scale parameter. Several authors have used a GRF to generate detector-specific variation in detection probability on the link scale (logit or cloglog). Reported bias in null-model density estimates from these simulations was typically in the range 0 to 20% Dey et al. (2023).\nDetector-level variation has several possible origins –\n\nHabitat-determined variation in detector efficiency,\nSelective use of habitats (third-order selection of D. H. Johnson (1980)), or\nUnrecorded variation in sampling intensity (Dey et al., 2023).\n\nSelective use implies a modification to each animal’s utilisation distribution. Time spent in preferred habitats allows less time in other habitats, and the time spent near any detector depends on the mix of habitats in the neighbourhood. Thus (1) and (2) imply different models of detector-level variation that we characterise as ‘unnormalised’ and ‘normalised’ (Efford, 2014).\n\nWe illustrate the effect of (1) and (2) with the scenarios simulated by Royle et al. (2013) and Efford (2014). The spatial scale parameter of the exponential covariance function was fixed at 2.5\\sigma, the spacing of a 7 x 7 grid of binary proximity detectors.\nThe original model specified the hazard of detection as \n\\lambda_k(\\mathbf x) = \\exp(\\alpha_0 -\\alpha_1 d^2 + \\alpha_2 X_k),\n\\tag{6.1} where d is the distance between an AC at \\mathbf x and detector k and X_k is a spatially autocorrelated variable with marginal distribution N(0,1), evaluated at detector k. The first two parameters are transformed versions of the parameters of the secr hazard-halfnormal detection function (\\alpha_0 = \\log(\\lambda_0), \\alpha_1 = 1/(2\\sigma^2); Chapter 10). The magnitude of detector-level variation was controlled by parameter \\alpha_2.\nBias in density estimates was small (\\mathrm{RB}(\\hat D) &lt; 5\\%)) for \\alpha_2 &lt;0.5, but could become worrying for large \\alpha_2 (Fig. 6.14 a). But what value of \\alpha_2 is realistic? Royle et al. (2013) estimated \\alpha_2 = 0.2 for black bear data from DNA hair snares, or \\alpha_2 = -0.3 for a combined hair-snare and telemetry model. \n\n\n\n\n\n\n\nFigure 6.14: Relative bias of parameter estimates from null model when \\lambda_0 varies among detectors according to (a) unnormalised model of Royle et al. (2013), and (b) normalised model of Efford (2014). The magnitude of variation depends on the coefficient \\alpha_2 (Eq. 6.1).\n\n\n\n\nNormalisation, as implied by third-order habitat selection, attenuates the effect on the bias of density estimates (Fig. 6.14 b). We note also that detector-level covariate effects measured at each detector are readily included in the model; estimates are then nearly unbiased (e.g., Efford, 2014).\nLarge bias can arise when the sample is assumed to come from a detector array larger than that actually sampled (Dey et al., 2023; Moqanaki et al., 2021). This would seem to come under the heading ‘Data requirements’ (Chapter 8.2) rather than model robustness.\nFurther complexity is possible. Stevenson et al. (2021) suggested modelling a latent Gaussian random field for each individual and showed how the parameters of the GRF could be estimated, assuming these to be shared across individuals. It is unclear how the latent GRF should be interpreted biologically. Activity is not normalised, so the combination of detection function and latent GRF does not describe a utilisation distribution.",
    "crumbs": [
      "Performance",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Assumptions and robustness</span>"
    ]
  },
  {
    "objectID": "06-assumptions.html#sec-independence",
    "href": "06-assumptions.html#sec-independence",
    "title": "6  Assumptions and robustness",
    "section": "Assumption 5: Independence",
    "text": "Assumption 5: Independence\n\n5a. Individuals are detected independently\nAnimals that move in groups are unlikely to be detected independently. Bischof et al. (2020) labelled the phenomenon “cohesion”. The primary result is overdispersion and underestimation of the sampling variance. Confidence intervals will be too narrow, resulting in less-than-nominal coverage. Earlier optimism (Bischof et al., 2020) that a measure of overdispersion would correct for the underestimation appears to be unwarranted (Efford & Fletcher, 2024).\nMcLaughlin & Bar (2020) modelled random associations between adjacent neighbours; the method has yet to find general application.\nAnimals are not detected independently in single-catch traps. Maximizing the multi-catch likelihood provides unbiased estimates of density and spatial scale in most cases (see Distiller & Borchers (2015) for an exception), but estimates of the intercept of the detection function (g_0, \\lambda_0) may be strongly biased. The coverage of confidence intervals based on the multi-catch likelihood has yet to be assessed. An algorithm combining simulation and inverse prediction allows unbiased estimation of all parameters (Efford, 2004, 2023), but lacks the flexibility of MLE. \n5b. Detections of an individual are independent\nTrapping by definition causes non-independence of detection events: an animal that is captured cannot be caught anywhere else until released. The non-independence of detections in multi-catch traps is readily modelled as competing risk, as we saw in Chapter 3.\nA more problematic form of dependence arises when a single visit to a passive detector results in multiple observations. This can happen when DNA is amplified from multiple hair samples left at a hair snag, or when a camera takes multiple photographs of an animal on a single visit. Analysing such data as if they were independent events results in overdispersion and accompanying underestimation of sampling variance. A simple solution is to collapse counts to binary observations.\nIn the basic SECR model of Chapter 3 the next detection of an individual may happen anywhere in its home range, regardless of where it was last detected. More biologically realistic patterns of movement lead to sequential dependence of locations.",
    "crumbs": [
      "Performance",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Assumptions and robustness</span>"
    ]
  },
  {
    "objectID": "06-assumptions.html#sec-robustnesssummary",
    "href": "06-assumptions.html#sec-robustnesssummary",
    "title": "6  Assumptions and robustness",
    "section": "\n6.2 Summary",
    "text": "6.2 Summary\nMost breaches of assumption can be managed to have only minor effects on density estimates, as we summarise in Table 6.1. Simulations only tell part of the story: to evaluate robustness we also need to know the real-life magnitude of any breach. The magnitude of some breaches may be assessed from external evidence (e.g. telemetry), or from a more general model that incorporates the effect (e.g., finite mixtures for individual heterogeneity; spatial random effect models Dey et al. (2023)). More work is needed on particular sampling systems and species.\nSpecial care is needed when these effects may apply:\n\nmis-identification leads to the appearance of super individuals (the shadow effect)\na learned (behavioural) response is global rather than specific to detector locations\nlarge individual heterogeneity\nalignment of home ranges with detector array\n\nWe have not considered in detail the potential for correlated variation in overall detection (a) and density. This was advanced as a major issue by McLellan et al. (2023).\nSeveral breaches lead to underestimation of sampling variance and impaired coverage of confidence intervals. If the magnitude of the breach can be estimated then better confidence intervals may be achieved by simulating from the full model.\nParticular detection parameters may be estimated poorly even when density estimates are robust. A common example arises when a multi-catch model is used for single-catch data: the estimated intercept of the detection function is strongly biased while density estimates are generally unbiased.\n\n\nTable 6.1: Robustness of SECR models to breaches of assumptions. Split colours indicate that the effect is generally weak or ignorable, but may be damaging in some circumstances if not modelled.\n\n\n\n\n\n\n\n\n\nBreach of assumption\n\nEffect\nMitigating models\n\n\n\n1. Closure\n\n\n\n\n\n   1a. Births and deaths\n\n\nRB(\\hat D) proportional turnover\nopen population\n\n\n   1b. Dispersal\n\n\n\nopen population\n\n\n2. Identification\n\n\n\n\n\n   2a. Inadequate marks\n\n\n-ve RB(\\hat D) when high probability of identity\nAugustine et al. (2020)\n\n\n   2b. Marks corrupted\n\n\n\n\n\n\n3. Home range HR\n\n\n\n\n\n   3a. AC not independent\n\n\n\n\n\n\n   3b. HR non-circular\n\n\nsevere RB(\\hat D) when HR align with detectors\nanisotropic model\n\n\n   3c. Location autocorrelated\n\n\n\n\n\n\n   3d. HR non-stationary\n\n\n\n\n\n\n4. Constant parameters\n\n\n\n\n\n   4a. Temporal variation\n\n\n\ncovariate, time-specific\n\n\n   4b. Behavioural responses\n\n\n\nlearned response\n\n\n   4c. Individual heterogeneity\n\n\nRB(\\hat D) nonlinear on CV(a)\ncovariate, finite mixture\n\n\n   4d. Detector heterogeneity\n\n\n\ncovariate, GRF\n\n\n5. Independence\n\n\n\n\n\n   5a. Individuals interact\n\n\n\nEfford (2023)\n\n\n   5b. Serial dependence\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLegend\n\nignorable\n\nminor\n\nmajor\n\nsevere\n\n\n\n\n\nFigure 6.1: Simulated effect of study duration on relative bias of density estimates for three levels of annual survival \\phi in a population with constant density. \\phi = 1 corresponds to a closed population.\nFigure 6.2: Relative bias of parameter estimates from null model when some individuals cannot be distinguished (‘shadow effects’).\nFigure 6.3: Relative bias of density estimates as function of PI for varying density. Density is expressed in units of animals per \\sigma^2. At low density few individuals are detected, reducing the chance of mis-identification for given PI.\nFigure 6.4: Threshold of PI at which absolute relative bias of density estimates exceeded the threshold shown, as a function of the expected number of individuals detected.\nFigure 6.5: Relative bias of parameter estimates from null model when genotyping errors generate ‘ghost’ individuals.\nFigure 6.6: SECR models typically assume that home ranges are circular (a). Noncircularity of home ranges causes limited bias in density estimates when ranges are oriented at random with respect to a linear detector array (b). Systematic alignment of ranges to the array (c) causes large bias in \\hat \\sigma and \\hat D.\nFigure 6.7: Examples of correlated movement paths over 100 time steps (t = 100). Parameter ‘tau’ (\\tau) controls autocorrelation. Red dot indicates activity centre; grey circle is 95% probability contour of distribution as t \\to \\infty or \\tau \\to 0.\nFigure 6.8: Relative bias RB of null-model density estimates of density D and spatial scale \\sigma from Ornstein-Uhlenbeck model with autocorrelation parameter \\tau.\nFigure 6.9: Relative bias of density estimates from studies with varying between-occasion movement. Both \\hat \\lambda_0 and \\hat \\sigma are biased by movement, but the net effect on \\hat D is negligible.\nFigure 6.10: Relative bias of parameter estimates from null model given temporal variation in \\lambda_0 or \\sigma.\nFigure 6.11: Relative bias of parameter estimates from null model given behavioural variation in lambda0. Relative bias of density was off-scale (+1.41) for recapture factor 0.25 with a general behavioural response.\nFigure 6.12: Relative bias of parameter estimates from null model given individual variation in \\lambda_0 and \\sigma^2.\nFigure 6.13: Variation in detector efficiency induces individual heterogeneity in overall detection probability p_\\cdot(\\vec x). AC 1–3 have similar exposure to detectors but experience respectively high, low and medium detection probability because of the neighbourhoods in which they are located.\nFigure 6.14: Relative bias of parameter estimates from null model when \\lambda_0 varies among detectors according to (a) unnormalised model of Royle et al. (2013), and (b) normalised model of Efford (2014). The magnitude of variation depends on the coefficient \\alpha_2 (Eq. 6.1).\nTable 6.1: Robustness of SECR models to breaches of assumptions. Split colours indicate that the effect is generally weak or ignorable, but may be damaging in some circumstances if not modelled.\nTable 6.1: Robustness of SECR models to breaches of assumptions. Split colours indicate that the effect is generally weak or ignorable, but may be damaging in some circumstances if not modelled.\nTable 6.1: Robustness of SECR models to breaches of assumptions. Split colours indicate that the effect is generally weak or ignorable, but may be damaging in some circumstances if not modelled.\nTable 6.1: Robustness of SECR models to breaches of assumptions. Split colours indicate that the effect is generally weak or ignorable, but may be damaging in some circumstances if not modelled.\nTable 6.1: Robustness of SECR models to breaches of assumptions. Split colours indicate that the effect is generally weak or ignorable, but may be damaging in some circumstances if not modelled.\nTable 6.1: Robustness of SECR models to breaches of assumptions. Split colours indicate that the effect is generally weak or ignorable, but may be damaging in some circumstances if not modelled.\nTable 6.1: Robustness of SECR models to breaches of assumptions. Split colours indicate that the effect is generally weak or ignorable, but may be damaging in some circumstances if not modelled.\nTable 6.1: Robustness of SECR models to breaches of assumptions. Split colours indicate that the effect is generally weak or ignorable, but may be damaging in some circumstances if not modelled.\nTable 6.1: Robustness of SECR models to breaches of assumptions. Split colours indicate that the effect is generally weak or ignorable, but may be damaging in some circumstances if not modelled.\nTable 6.1: Robustness of SECR models to breaches of assumptions. Split colours indicate that the effect is generally weak or ignorable, but may be damaging in some circumstances if not modelled.\nTable 6.1: Robustness of SECR models to breaches of assumptions. Split colours indicate that the effect is generally weak or ignorable, but may be damaging in some circumstances if not modelled.\nTable 6.1: Robustness of SECR models to breaches of assumptions. Split colours indicate that the effect is generally weak or ignorable, but may be damaging in some circumstances if not modelled.\nTable 6.1: Robustness of SECR models to breaches of assumptions. Split colours indicate that the effect is generally weak or ignorable, but may be damaging in some circumstances if not modelled.\nTable 6.1: Robustness of SECR models to breaches of assumptions. Split colours indicate that the effect is generally weak or ignorable, but may be damaging in some circumstances if not modelled.\n\n\n\nAugustine, B. C., Royle, J. A., Kelly, M. J., Satter, C. B., Alonso, R. S., Boydston, E. E., & Crooks, K. R. (2018). Spatial capture–recapture with partial identity: An application to camera traps. The Annals of Applied Statistics, 12(1). https://doi.org/10.1214/17-aoas1091\n\n\nAugustine, B. C., Royle, J. A., Linden, D. W., & Fuller, A. K. (2020). Spatial proximity moderates genotype uncertainty in genetic tagging studies. Proceedings of the National Academy of Sciences, 117(30), 17903–17912. https://doi.org/10.1073/pnas.2000247117\n\n\nBischof, R., Dupont, P., Milleret, C., Chipperfield, J., & Royle, J. A. (2020). Consequences of ignoring group association in spatial capture–recapture analysis. Wildlife Biology, 2020(1), 1–10. https://doi.org/10.2981/wlb.00649\n\n\nBorchers, D. L., & Efford, M. G. (2008). Spatially explicit maximum likelihood methods for capture-recapture studies. Biometrics, 64, 377–385.\n\n\nDey, S., Moqanaki, E., Milleret, C., Dupont, P., Tourani, M., & Bischof, R. (2023). Modelling spatially autocorrelated detection probabilities in spatial capture-recapture using random effects. Ecological Modelling, 479, 110324. https://doi.org/10.1016/j.ecolmodel.2023.110324\n\n\nDistiller, G., & Borchers, D. L. (2015). A spatially explicit capture–recapture estimator for single‐catch traps. Ecology and Evolution, 5(21), 5075–5087. https://doi.org/10.1002/ece3.1748\n\n\nDunn, J. E., & Gipson, P. S. (1977). Analysis of radio telemetry data in studies of home range. Biometrics, 33(1), 85. https://doi.org/10.2307/2529305\n\n\nDupont, P., Milleret, C., Gimenez, O., & Bischof, R. (2019). Population closure and the bias‐precision trade‐off in spatial capture–recapture. Methods in Ecology and Evolution, 10(5), 661–672. https://doi.org/10.1111/2041-210x.13158\n\n\nEfford, M. G. (2004). Density estimation in live-trapping studies. Oikos, 106, 598–610.\n\n\nEfford, M. G. (2014). Bias from heterogeneous usage of space in spatially explicit capture-recapture analyses. Methods in Ecology and Evolution, 5, 599–602.\n\n\nEfford, M. G. (2019). Non‐circular home ranges and the estimation of population density. Ecology, 100(2). https://doi.org/10.1002/ecy.2580\n\n\nEfford, M. G. (2023). ipsecr: An R package for awkward spatial capture–recapture data. Methods in Ecology and Evolution, 14(5), 1182–1189. https://doi.org/10.1111/2041-210x.14088\n\n\nEfford, M. G., & Fletcher, D. (2024). Effect of spatial overdispersion on confidence intervals for population density estimated by spatial capture-recapture. bioRxiv. https://doi.org/10.1101/2024.03.12.584742\n\n\nEfford, M. G., & Mowat, G. (2014). Compensatory heterogeneity in spatially explicit capture-recapture data. Ecology, 95, 1341–1348.\n\n\nHarihar, A., Chanchani, P., Pariwakam, M., Noon, B. R., & Goodrich, J. (2017). Defensible inference: Questioning global trends in tiger populations. Conservation Letters, 10(5), 502–505. https://doi.org/10.1111/conl.12406\n\n\nHooten, M. B., Johnson, D. S., McClintock, B. T., & Morales, J. M. (2017). Animal movement: Statistical models for telemetry data. CRC Press. https://doi.org/10.1201/9781315117744\n\n\nIvan, J. S., White, G. C., & Shenk, T. M. (2013). Using auxiliary telemetry information to estimate animal density from capture–recapture data. Ecology, 94(4), 809–816. https://doi.org/10.1890/12-0101.1\n\n\nJohnson, D. H. (1980). The comparison of usage and availability measurements for evaluating resource preference. Ecology, 61, 65–71.\n\n\nJohnson, D. S., London, J. M., Lea, M.-A., & Durban, J. W. (2008). Continuous-time correlated random walk model for animal telemetry data. Ecology, 89(5), 1208–1215. https://doi.org/10.1890/07-1032.1\n\n\nKaranth, K. U., & Nichols, J. D. (1998). Estimation of tiger densities in india using photographic captures and recaptures. Ecology, 79, 2852–2862.\n\n\nKendall, W. L. (1999). Robustness of closed capture–recapture methods to violations of the closure assumption. Ecology, 80(8), 2517–2525. https://doi.org/10.1890/0012-9658(1999)080[2517:roccrm]2.0.co;2\n\n\nKendeigh, S. C. (1944). Measurement of bird populations. Ecological Monographs, 14, 67–106. https://doi.org/10.2307/1961632\n\n\nKodi, A. R., Howard, J., Borchers, D. L., Worthington, H., Alexander, J. S., Lkhagvajav, P., Bayandonoi, G., Ochirjav, M., Erdenebaatar, S., Byambasuren, C., Battulga, N., Johansson, Ö., & Sharma, K. (2024). Ghostbusting - reducing bias due to identification errors in spatial capture‐recapture histories. Methods in Ecology and Evolution. https://doi.org/10.1111/2041-210x.14326\n\n\nLampa, S., Henle, K., Klenke, R., Hoehn, M., & Gruber, B. (2013). How to overcome genotyping errors in non-invasive genetic mark-recapture population size estimation-a review of available methods illustrated by a case study: Genotyping errors in CMR-a review. The Journal of Wildlife Management, 77(8), 1490–1511. https://doi.org/10.1002/jwmg.604\n\n\nLópez-Bao, J. V., Godinho, R., Pacheco, C., Lema, F. J., García, E., Llaneza, L., Palacios, V., & Jiménez, J. (2018). Toward reliable population estimates of wolves by combining spatial capture-recapture models and non-invasive DNA monitoring. Scientific Reports, 8(1). https://doi.org/10.1038/s41598-018-20675-9\n\n\nLukacs, P. M., & Burnham, K. P. (2005). Estimating population size from DNA-based closed capture-recapture data incorporating genotyping error. Journal of Wildlife Management, 69, 396–403.\n\n\nMcLaughlin, P., & Bar, H. (2020). A spatial capture–recapture model with attractions between individuals. Environmetrics, 32(1). https://doi.org/10.1002/env.2653\n\n\nMcLellan, B. A., Howe, E., Marrotte, R. R., & Northrup, J. M. (2023). Accounting for heterogeneous density and detectability in spatially explicit capture–recapture studies of carnivores. Ecosphere, 14(10). https://doi.org/10.1002/ecs2.4669\n\n\nMills, L. S., Ciotta, J. C., Lair, K. P., Schwartz, M. K., & Tallmon, D. A. (2000). Estimating animal abundance using noninvasive genetic sampling: Promise and pitfalls. Ecological Applications, 10, 283–294.\n\n\nMoqanaki, E. M., Milleret, C., Tourani, M., Dupont, P., & Bischof, R. (2021). Consequences of ignoring variable and spatially autocorrelated detection probability in spatial capture-recapture. Landscape Ecology, 36(10), 2879–2895. https://doi.org/10.1007/s10980-021-01283-x\n\n\nMurphy, S. M., Cox, J. J., Augustine, B. C., Hast, J. T., Guthrie, J. M., Wright, J., McDermott, J., Maehr, S. C., & Plaxico, J. H. (2016). Characterizing recolonization by a reintroduced bear population using genetic spatial capture-recapture: Recolonization by reintroduced bear population. The Journal of Wildlife Management, 80(8), 1390–1407. https://doi.org/10.1002/jwmg.21144\n\n\nMurphy, S. M., & Luja, V. H. (2025). Jaguar density estimation in mexico: The conservation importance of considering home range orientation in spatial capture–recapture. Conservation Science and Practice. https://doi.org/10.1111/csp2.13301\n\n\nOtis, D. L., Burnham, K. P., White, G. C., & Anderson, D. R. (1978). Statistical inference from capture data on closed animal populations. Wildlife Monographs, 62.\n\n\nOtis, D. L., & White, G. C. (1999). Autocorrelation of location estimates and the analysis of radiotracking data. The Journal of Wildlife Management, 63(3), 1039. https://doi.org/10.2307/3802819\n\n\nPaetkau, D. (2003). An empirical exploration of data quality in DNA‐based population inventories. Molecular Ecology, 12(6), 1375–1387. https://doi.org/10.1046/j.1365-294x.2003.01820.x\n\n\nReich, B. J., & Gardner, B. (2014). A spatial capture‐recapture model for territorial species. Environmetrics, 25(8), 630–637. https://doi.org/10.1002/env.2317\n\n\nRoyle, J. A., Chandler, R. B., Sun, C. C., & Fuller, A. K. (2013). Integrating resource selection information with spatial capture–recapture. Methods in Ecology and Evolution, 4(6), 520–530. https://doi.org/10.1111/2041-210x.12039\n\n\nRoyle, J. A., Fuller, A. K., & Sutherland, C. (2015). Spatial capture–recapture models allowing markovian transience or dispersal. Population Ecology, 58(1), 53–62. https://doi.org/10.1007/s10144-015-0524-z\n\n\nRussell, R. E., Royle, J. A., Desimone, R., Schwartz, M. K., Edwards, V. L., Pilgrim, K. P., & Mckelvey, K. S. (2012). Estimating abundance of mountain lions from unstructured spatial sampling. The Journal of Wildlife Management, 76(8), 1551–1561. https://doi.org/10.1002/jwmg.412\n\n\nSethi, S. A., Cook, G. M., Lemons, P., & Wenburg, J. (2014). Guidelines for MSAT and SNP panels that lead to high-quality data for genetic mark–recapture studies. Canadian Journal of Zoology, 92(6), 515–526. https://doi.org/10.1139/cjz-2013-0302\n\n\nSollmann, R. (2024). Mt or not mt: Temporal variation in detection probability in spatial capture-recapture and occupancy models. Peer Community Journal, 4. https://doi.org/10.24072/pcjournal.357\n\n\nStevenson, B. C., Fewster, R. M., & Sharma, K. (2021). Spatial correlation structures for detections of individuals in spatial capture–recapture models. Biometrics, 78(3), 963–973. https://doi.org/10.1111/biom.13502\n\n\nvan Winkle, W. (1975). Comparison of several probabilistic home-range models. Journal of Wildlife Management, 39, 118–123.\n\n\nWaits, L. P., & Paetkau, D. (2005). Noninvasive genetic sampling tools forwildlife biologists: A review of applications and recommendations foraccurate data collection. Journal of Wildlife Management, 69, 1419–1433.\n\n\nWoodruff, S. P., Johnson, T. R., & Waits, L. P. (2014). Evaluating the interaction of faecal pellet deposition rates and &lt;scp&gt;DNA&lt;/scp&gt; degradation rates to optimize sampling design for &lt;scp&gt;DNA&lt;/scp&gt;‐based mark–recapture analysis of sonoran pronghorn. Molecular Ecology Resources, 15(4), 843–854. https://doi.org/10.1111/1755-0998.12362",
    "crumbs": [
      "Performance",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Assumptions and robustness</span>"
    ]
  },
  {
    "objectID": "07-validation.html",
    "href": "07-validation.html",
    "title": "7  Empirical validation",
    "section": "",
    "text": "7.1 Brushtail possums in New Zealand\nEfford et al. (2005) live-trapped brushtail possums (Trichosurus vulpecula) on a coastal peninsula and compared the results to an attempted total removal by leg-hold trapping and acute poisoning. SECR estimates of density from five hollow grids were consistent with the removal estimate.\nAlthough broadly reassuring, the brushtail possum study had weaknesses. The live-trapping data were compromised by tag loss (documented in secr), the landward boundary of the removal area was somewhat arbitrary, and sampling may have been inadequate to represent density variation across the peninsula (see also Rodents in New Mexico).",
    "crumbs": [
      "Performance",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Empirical validation</span>"
    ]
  },
  {
    "objectID": "07-validation.html#sec-redsquirrel",
    "href": "07-validation.html#sec-redsquirrel",
    "title": "7  Empirical validation",
    "section": "\n7.2 Red squirrels in the Yukon",
    "text": "7.2 Red squirrels in the Yukon\n\nVan Katwyk (2014) analysed data from a multi-year behavioural study of red squirrels (Tamiasciurus hudsonicus) in the Yukon. Squirrels were marked and followed across six study areas, each about 36-ha in area. Intensive trapping at defended food middens maintained nearly 100% marking coverage. Capture–recapture data came from 50 cage traps operated periodically in the centre of each study area.\nCalculation of ‘true’ density focussed on the area within the perimeter of the trapping grid. Behavioural observations of all squirrels whose territories potentially overlapped the grid (i.e. within one territory width) were scored for time spent inside the grid. The sum of these proportions is the number of ‘animal equivalents’ (Boutin, 1984) that gives the population density when divided by the grid area.\nSECR density estimates averaged only 5% less than the ‘true’ density calculated by this method, and the estimates showed a strong correlation between sessions with the varying ‘true’ density. It is hard to fault this validation. The ‘true’ density was limited to the interior of each grid, whereas each SECR estimate includes data from a somewhat wider area and sample sizes were modest.",
    "crumbs": [
      "Performance",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Empirical validation</span>"
    ]
  },
  {
    "objectID": "07-validation.html#sec-newmexico",
    "href": "07-validation.html#sec-newmexico",
    "title": "7  Empirical validation",
    "section": "\n7.3 Rodents in New Mexico",
    "text": "7.3 Rodents in New Mexico\n\nOne major evaluation has questioned the reliability of SECR. Gerber & Parmenter (2015) re-analysed live-trapping data for rodents of several taxa confined to pens in New Mexico (Parmenter et al., 2003). Traps were arranged in either a central square grid or a star-shaped pattern, a ‘trapping web’. The main study was followed by intensive trapping across the full extent of each pen to enumerate each taxon.\nSECR provided instances of both over- and under-estimation, and its performance was “sometimes underwhelming”. The authors focussed on heterogeneity of detection and asymmetry of home ranges as possible explanations, although direct evidence of these effects was lacking. Asymmetry is not a significant source of bias in itself (Efford (2019)). Heterogeneity must be large to cause significant bias, and the bias is negative (e.g., Efford & Mowat (2014)) see also 6.\nWe suggest an alternative explanation. The main trap layouts did not sample the full extent of each pen, and the probability of detecting an individual in the periphery was quite low for species with small home ranges such as Perognathus flavus (Fig. 7.1). Perfect correspondence with pen-wide exhaustive trapping is not to be expected if the density in the under-sampled peripheral zone differs from the central zone. This applies whether the difference is random or systematic. The peripheral zone (p_\\cdot(\\mathbf x) &lt; 0.25) was a larger fraction of the pen for the grid (\\approx 54\\%) than for the web (\\approx 28\\%), which could explain why the grid estimates of P. flavus density were particularly poor (Gerber & Parmenter, 2015, Figures 2a, 3a).\nThe final exhaustive trapping might be used to test the hypothesis of spatial heterogeneity, but these data are not available. Some evidence of within-pen density variation may be gleaned from density models fitted to the capture–recapture data: in 50% of the populations a density model that included distance-to-wall as a covariate provided better fit than a null model in 50% of cases (likelihood ratio test P &lt; 0.05; unpubl. results).\n\n\n\n\n\n\n\nFigure 7.1: Peroganthus flavus contours of overall 5-night detection probability p_\\cdot(\\mathbf{x}) (0.1, 0.25, 0.5, 0.75) within a ca. 4.2-ha pen (grey shading). a. Grid, b. Trapping web. Detection parameters from first P. flavus population in each case.",
    "crumbs": [
      "Performance",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Empirical validation</span>"
    ]
  },
  {
    "objectID": "07-validation.html#sec-chimpanzees",
    "href": "07-validation.html#sec-chimpanzees",
    "title": "7  Empirical validation",
    "section": "\n7.4 Chimpanzees in Ivory Coast",
    "text": "7.4 Chimpanzees in Ivory Coast\nDesprés‐Einspenner et al. (2017) applied SECR to automatic camera records of chimpanzees Pan troglodytes in a single group territory. Group size (27 excluding unweaned young) was known from intensive observations. The authors reported a good match between the estimated and known populations. However, \\sigma could not be estimated because individuals moved throughout the territory and cameras were not placed outside it (see also comments on array size under Study design). This required them to treat the extent of the population as known (confined within the independently determined territory boundary). The resulting analysis is in essence non-spatial, and does not validate SECR.",
    "crumbs": [
      "Performance",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Empirical validation</span>"
    ]
  },
  {
    "objectID": "07-validation.html#sec-salamanders",
    "href": "07-validation.html#sec-salamanders",
    "title": "7  Empirical validation",
    "section": "\n7.5 Red-backed salamanders in Massachusetts",
    "text": "7.5 Red-backed salamanders in Massachusetts\nFleming et al. (2021) examined the effect of varying the configuration of cover boards used to survey red-backed salamanders (Plethodon cinereus). Cover boards are artificial refuges placed on the ground. Data on salamanders found under cover boards are analysed as if cover boards were multi-catch traps. The authors’ concern was that SECR estimates of salamander density might depend on the layout and interfere with comparisons and aggregation of data across studies. Estimates of detection parameters (half-normal intercept g_0 and spatial scale \\sigma) were sensitive to the extent and spacing of cover board arrays, whereas density estimates were mostly robust. Salamanders appeared more mobile (larger \\hat \\sigma) when cover boards were further apart. Fleming et al. (2021) suggested salamanders might move directly to the next cover board, presumably within some distance limit.",
    "crumbs": [
      "Performance",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Empirical validation</span>"
    ]
  },
  {
    "objectID": "07-validation.html#summary",
    "href": "07-validation.html#summary",
    "title": "7  Empirical validation",
    "section": "\n7.6 Summary",
    "text": "7.6 Summary\nNone of these field studies fully validates SECR methods, but there is at least no strong counter evidence. We encourage field researchers to test SECR further. Where discrepancies are found it is important to investigate the cause. The assumption of independence among detectors underpins the compounding of single-detector distance-detection functions to predict array-level detection probability. Evidence supporting this assumption might relieve the need for elaborate models such as Stevenson et al. (2021).\n\n\n\nFigure 7.1: Peroganthus flavus contours of overall 5-night detection probability p_\\cdot(\\mathbf{x}) (0.1, 0.25, 0.5, 0.75) within a ca. 4.2-ha pen (grey shading). a. Grid, b. Trapping web. Detection parameters from first P. flavus population in each case.\n\n\n\nBoutin, S. (1984). Home range size and methods of estimating snowshoe hare densities. Acta Zoologica Fennica, 171, 275–278.\n\n\nDesprés‐Einspenner, M., Howe, E. J., Drapeau, P., & Kühl, H. S. (2017). An empirical evaluation of camera trapping and spatially explicit capture‐recapture models for estimating chimpanzee density. American Journal of Primatology, 79(7). https://doi.org/10.1002/ajp.22647\n\n\nEfford, M. G. (2019). Non‐circular home ranges and the estimation of population density. Ecology, 100(2). https://doi.org/10.1002/ecy.2580\n\n\nEfford, M. G., & Mowat, G. (2014). Compensatory heterogeneity in spatially explicit capture-recapture data. Ecology, 95, 1341–1348.\n\n\nEfford, M. G., Warburton, B., Coleman, M. C., & Barker, R. J. (2005). A field test of two methods for density estimation. Wildlife Society Bulletin, 33, 731–738.\n\n\nFleming, J., Grant, E. H. C., Sterrett, S. C., & Sutherland, C. (2021). Experimental evaluation of spatial capture–recapture study design. Ecological Applications, 31(7). https://doi.org/10.1002/eap.2419\n\n\nGerber, B. D., & Parmenter, R. R. (2015). Spatial capture–recapture model performance with known small‐mammal densities. Ecological Applications, 25(3), 695–705. https://doi.org/10.1890/14-0960.1\n\n\nKendeigh, S. C. (1944). Measurement of bird populations. Ecological Monographs, 14, 67–106. https://doi.org/10.2307/1961632\n\n\nParmenter, R., Yates, T., Anderson, D., Burnham, K., Dunnum, J., Franklin, A., Friggens, M., Lubow, B., Miller, M., Olson, G., Parmenter, C., Pollard, J., Rexstad, E., Shenk, T., Stanley, T., & White, G. (2003). Small-mammal density estimation: A field comparison of grid-based vs web-based density estimators. Ecological Monographs, 73(1), 1–26.\n\n\nStevenson, B. C., Fewster, R. M., & Sharma, K. (2021). Spatial correlation structures for detections of individuals in spatial capture–recapture models. Biometrics, 78(3), 963–973. https://doi.org/10.1111/biom.13502\n\n\nTwining, J. P., McFarlane, C., O’Meara, D., O’Reilly, C., Reyne, M., Montgomery, W. I., Helyar, S., Tosh, D. G., & Augustine, B. C. (2022). A comparison of density estimation methods for monitoring marked and unmarked animal populations. Ecosphere, 13(10). https://doi.org/10.1002/ecs2.4165\n\n\nVan Katwyk, K. E. (2014). Empirical validation of closed population abundance estimates and spatially explicit density estimates using a censused population of north american red squirrels [Master’s thesis, University of Alberta]. https://redsquirrel.biology.ualberta.ca/wp-content/uploads/sites/32/2014/07/Van-Katwyk_Kristin_Spring-2014.pdf",
    "crumbs": [
      "Performance",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Empirical validation</span>"
    ]
  },
  {
    "objectID": "08-studydesign.html",
    "href": "08-studydesign.html",
    "title": "8  Study design",
    "section": "",
    "text": "8.1 Studies focussing on design\nGeneral issues of study design for SECR were considered by Sollmann et al. (2012), Tobler & Powell (2013), Sun et al. (2014), Clark (2019), and Efford & Boulanger (2019). Algorithmic optimisation of detector locations was described by Dupont et al. (2021) and Durbach et al. (2021). Design issues appeared incidentally in Efford et al. (2005), Efford & Fewster (2013), Noss et al. (2012), and Palmero et al. (2023), among other papers.",
    "crumbs": [
      "Performance",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Study design</span>"
    ]
  },
  {
    "objectID": "08-studydesign.html#sec-datarequirements",
    "href": "08-studydesign.html#sec-datarequirements",
    "title": "8  Study design",
    "section": "\n8.2 Data requirements for SECR",
    "text": "8.2 Data requirements for SECR\n\nWe list here some minimum data requirements that might be labelled “assumptions” but are more fundamental. They should be addressed by appropriate study design.\nRequirement 1. Sampling representative of the area of interest\nThis requirement is important and easy to overlook. The problem takes care of itself when detectors are placed evenly throughout the area of interest, exposing all individuals to a similar probability of detection. This is usually too costly when the area of interest is large. Sub-sampling is then called for, and we consider the options later.\nRequirement 2. Many individuals detected more than once \nThis requirement is common to all capture–recapture methods. Estimation of detection parameters conditions on the first detection, and detections after the first are needed to estimate a non-zero detection rate. The question ‘How many is enough?’ is addressed later.\nRequirement 3. Spread adequate to estimate spatial scale of detection\nBy ‘spread’ we mean the spatial extent of the detections of an individual. The spread of detections must be adequate in two respects:\nRequirement 3a. Some individuals are detected at more than one detector, and\nRequirement 3b. Detections of an individual should be localised to part of the detector array.\nAdequate spread is achieved by matching the size and spacing of a detector array to the scale of movement, as shown schematically in Fig. 8.1.\n\n\n\n\n\n\n\nFigure 8.1: Three scenarios for array size in relation to home range size. For intuition we use a circle to indicate a hard-edged uniform home range. When the array is too small, recaptures are equally likely anywhere in the array. When the spacing is too large, recaptures must be at the same detector. In either case, the recaptures carry almost no information on the spatial scale of detection \\sigma.\n\n\n\n\nSome qualification is needed here. We require only that some individuals are detected at more than one detector, and that the detector array is large enough to differentiate points in the middle and edge of some home ranges. Failure to meet the ‘spread’ requirement may be addressed, at least in principle, by combining SECR and telemetry, but improved study design is a better solution.\nA poorly designed detector array may give data that cannot be analysed by SECR, or provide highly biased estimates of low precision. Such designs were described by Efford & Boulanger (2019) as ‘pathological’. We seek a non-pathological and representative design that gives the most precise estimates possible for a given cost.",
    "crumbs": [
      "Performance",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Study design</span>"
    ]
  },
  {
    "objectID": "08-studydesign.html#precision-and-power",
    "href": "08-studydesign.html#precision-and-power",
    "title": "8  Study design",
    "section": "\n8.3 Precision and power",
    "text": "8.3 Precision and power\n\nPrecise estimates have narrow confidence intervals and high power to detect change. Precision and bias jointly determine the accuracy of estimates, measured by the root-mean-square error (the difference between estimates and the true value). Bias is typically a minor component, and for now we assume it is negligible.\nPrecision is conveniently measured by its inverse, the relative standard error (RSE) where \\mathrm{RSE}(\\hat \\theta) = \\widehat{\\mathrm{SE}}(\\hat \\theta) / {\\hat \\theta} for estimate \\hat \\theta of parameter \\theta. In statistics, the standard error is the square root of the sampling variance, estimated for MLE as described in Section 3.6. Wildlife papers often use ‘CV’ for the RSE of estimates, but this confuses relative standard error and relative standard deviation.\nThe question ‘What precision do I need?’ requires clarity on the purpose of the study. If the purpose is to compare density estimates from different times or places then the power to detect change of a given magnitude is a direct function of the initial \\mathrm{RSE}(\\hat D) (Fig. 8.2). Although RSE = 0.2 is often touted as adequate “for management purposes”, estimates with RSE = 0.2 have low (\\le 50%) power to detect a change in density of even \\pm 50% at the usual \\alpha = 0.05. If reliable estimates are vital for population management, as claimed routinely in methodological papers, then surely greater precision is required. RSE = 0.1 is a better general target.\n\n\n\n\n\n\n\nFigure 8.2: Power of a 2-sided test (\\alpha = 0.05) to detect change in density between two surveys as a function of effect size ((D2/D1 − 1) × 100 on the x-axis) for two levels of \\mathrm{RSE}(\\hat D) in the initial survey. For initial RSE = 0.1, only changes greater than –36% and +44% are detected with power greater that 80% (dashed line). Reproduced from Efford & Boulanger (2019) Fig. 1.\n\n\n\n\nWe lack a clear guide to required RSE in other studies, for which the effect size may be ill-defined. We have observed fitted models with \\mathrm{RSE}(\\hat D) \\gg 0.2 to behave erratically with respect to AIC model selection, presumably because of sampling variance in the AIC values themselves.",
    "crumbs": [
      "Performance",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Study design</span>"
    ]
  },
  {
    "objectID": "08-studydesign.html#pilot-parameter-values",
    "href": "08-studydesign.html#pilot-parameter-values",
    "title": "8  Study design",
    "section": "\n8.4 Pilot parameter values",
    "text": "8.4 Pilot parameter values\n\nTo evaluate a potential study design we must know something about the target population. Here we describe the population and the behaviour of individuals by a simple SECR model and its parameters: uniform density D and the parameters \\lambda_0 and \\sigma of a hazard half-normal detection function. Predictions regarding the suitability and performance of any design then depend on the values of D, \\lambda_0 and \\sigma. This seems like a catch-22 – impossible until we have estimates – but for design purposes we can call on approximate values from multiple sources\n\npublished estimates from studies of similar species,\na low-precision pilot study, or\nindirect inference.\n\nReviews of SECR studies are a useful source of pilot estimates (e.g., Palmero et al., 2023). The hardest parameter to pin down is \\lambda_0, as this is very study-specific. The good news is that it has only a secondary effect on the relative merits of different array designs.\nIndirect inference is a murky option, but better than nothing, and there are some constraints. The quantity k = \\sigma \\sqrt D (loosely described by Efford et al. (2016) as an index of home range overlap) usually falls in the range 0.3–1.3 for solitary species (M. Efford unpubl.). secr expresses D in animals / ha and \\sigma in metres, so a factor of 100 is needed, and \\sigma = \\frac{100}{\\sqrt{2D}} is a good start (k \\approx 0.707).\nThe intercept and spatial scale parameters of the hazard detection functions (\\lambda_0, \\sigma) may be substituted for design purposes by the parameters of the corresponding probability detection function (g_0, \\sigma)1.\n\n\n\n\n\n\n\nFigure 8.3: Relationship between density and the spatial scale parameter \\sigma for different values of the overlap index k.\n\n\n\n\nWe can use the close analogy between the detection function and a home-range utilisation distribution to extract a pilot value of \\sigma from home range data. Most directly, a circular bivariate normal (BVN) model can be fitted to telemetry data; we then use the dispersion parameter directly as a pilot value of \\sigma for hazard half-normal detection. If the home range data have been summarised as the area a within a notional 95% activity contour then the spatial scale parameter of the hazard half-normal is close to \\sigma = \\sqrt \\frac{a}{6 \\pi} (Jennrich & Turner, 1969 Eq. 13)2.",
    "crumbs": [
      "Performance",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Study design</span>"
    ]
  },
  {
    "objectID": "08-studydesign.html#sec-Enr",
    "href": "08-studydesign.html#sec-Enr",
    "title": "8  Study design",
    "section": "\n8.5 Expected sample size",
    "text": "8.5 Expected sample size\n\nGiven some pilot parameter values, we might proceed directly to simulating data from different designs and computing the resulting SECR estimates. This is effective, but slow. A useful preliminary step is to check the expected sample size of potential designs.\nThe sample size for SECR is more than a single number. We suggest calculating the expected values of these count statistics:\n\n\nn   the number of distinct individuals detected at least once,\n\nr   the total number of re-detections (any detection after an individual is first detected), and\n\nm   the total number of movements (re-detections at a detector different to the preceding one).\n\nThe counts depend on the detector configuration, the extent of habitat (buffer width), and the type of detector (trap, proximity detector etc.) in addition to the parameter values. Formulae are given in Appendix K. Calculation is fast and the counts give insight on whether the data generated by a design are likely to be adequate. E(r) is a direct measure of Requirement 2 above. E(m) addresses Requirement 3a (‘Spacing too large’ in Fig. 8.1).\nEfford & Boulanger (2019) found that E(n) and E(r) alone were often sufficient to predict the precision of SECR density estimates, and maximum precision was achieved when E(n) \\approx E(r).\n\n\n\n\n\n\nSpatial recaptures\n\n\n\n\n\nThe term ‘spatial recaptures’ corresponds loosely with our m. Spatial recaptures are a sine qua non of SECR, but their role in determining precision can be overstated. They are a consequence of Requirements 2 and 3, not an independent effect. Furthermore, movements are ill-defined when detections are aggregated by time, as then we cannot distinguish the capture histories ABABA and AAABB at detectors A and B (both are 3A, 2B).",
    "crumbs": [
      "Performance",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Study design</span>"
    ]
  },
  {
    "objectID": "08-studydesign.html#sec-arraysize",
    "href": "08-studydesign.html#sec-arraysize",
    "title": "8  Study design",
    "section": "\n8.6 Array size in relation to home range",
    "text": "8.6 Array size in relation to home range\n\nWe lack a count statistic matching the second part of Requirement 3 (‘Array too small’ in Fig. 8.1). We therefore define an ad hoc measure of sampling scale that we call the ‘extent ratio’: this is the diameter of the array (the distance between the most extreme detectors) divided by the nominal diameter of a 95% home range. The 95% home-range radius is r_{0.95} \\approx 2.45 \\sigma for a BVN utilisation distribution (from the previous formula).\n\n\n\n\n\n\n\nFigure 8.4: Effect of array size on relative bias (RB) and relative root-mean-square error (rRMSE) of density estimates. Simulations of a square array with size (diagonal length) divided by the diameter of a 95% BVN home range. Density was estimated with both the detection function used to generate the data (HHN) and a mis-specified detection function (HEX).\n\n\n\n\nSimulations described on GitHub show that SECR performs poorly when the extent ratio is less than 1: the relative bias and root-mean-square error of \\hat D increase abruptly (Fig. 8.4). We earlier touted the robustness of \\hat D to misspecification of the detection function, but this desirable property evaporates when the array is small, as shown by estimates from a hazard exponential function applied to hazard half-normal data in Fig. 8.4.\nEfford (2011) simulated area-search data for a range of area sizes. Plotting those results with the extent ratio as the x-axis shows a similar pattern to Fig. 8.4 (see GitHub).\n\nThe extent ratio does not provide a precise criterion because it combines two somewhat arbitrary measures (array diameter, 95% home range diameter). What the simulations reveal about array size can be summed up in a simple rule: the grid or searched area should be at least the size of the home range, and larger arrays provide greater robustness and accuracy.",
    "crumbs": [
      "Performance",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Study design</span>"
    ]
  },
  {
    "objectID": "08-studydesign.html#the-n-r-tradeoff",
    "href": "08-studydesign.html#the-n-r-tradeoff",
    "title": "8  Study design",
    "section": "\n8.7 The n-r tradeoff",
    "text": "8.7 The n-r tradeoff\n\nThe performance of the SECR density estimator depends on both the number of individuals n and the number of re-detections r. However, we cannot maximise their expected values simultaneously. E(n) is greatest when detectors are far apart and each detector samples a different set of individuals (AC). For binary proximity detectors E(r) is greatest when detectors are clumped together and declines monotonically with spacing; for multi-catch traps, E(r) increases and then declines (Fig. 8.5).\n\n\n\n\n\n\n\nFigure 8.5: Sample size as a function of detector spacing for two detector types. Square grid of 64 detectors operated for 10 occasions with D = 0.5\\sigma^{-2} and \\lambda_0 = 0.1.\n\n\n\n\nTo maximise the sample size we seek a satisfactory compromise, an intermediate spacing of detectors that yields both high E(n) and high E(r). There is so far no evidence for weighting one more than the other; later simulations suggest aiming for E(n) \\approx E(r), as indicated by the vertical lines in Fig. 8.5. The number of movement recaptures m follows the pattern of total recaptures r and need not be considered separately.",
    "crumbs": [
      "Performance",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Study design</span>"
    ]
  },
  {
    "objectID": "08-studydesign.html#sec-spacing",
    "href": "08-studydesign.html#sec-spacing",
    "title": "8  Study design",
    "section": "\n8.8 Spacing and precision",
    "text": "8.8 Spacing and precision\n\nWe expect precision to improve with sample size. From the last section we understand that sample size depends somewhat subtly on detector spacing: wider spacing increases n because a larger area is sampled, but it ultimately reduces r. We next use simulation to demonstrate the effect on the precision. We measure precision by the relative standard error of density estimates \\mathrm{RSE} (\\hat D). In the following examples we assume a Poisson distribution for n rather than the binomial distribution that results when N(A) is fixed.\n\n\n\n\n\n\n\nFigure 8.6: Relationship between precision and detector spacing for scenarios of differing grid size and detection rate \\lambda_0 (5 sampling occasions, reproduced from Fig. 2, Efford & Boulanger, 2019).\n\n\n\n\nThe simulations in Fig. 8.6 are representative: there is an optimal spacing (often around 2\\sigma for a hazard half-normal detection function) and a broad range of spacings yielding similar precision. The optimum is close, but not identical, to the spacing that gives E(n) = E(r) in Fig. 8.5. The variability of the simulations increases away from the optimum spacing: if we are in the right ballpark then very few replicates are needed to capture it.\nAlthough 2\\sigma is often mentioned as an optimum, the optimal spacing is smaller when sampling intensity is low. This happens when there are few sampling occasions or \\lambda_0 is small, as illustrated in Fig. 8.6.\n\n\n\n\n\n\n\nFigure 8.7: Effect of sampling intensity (number of occasions and \\lambda_0) on optimal detector spacing from simulations. Base scenario as in Fig. 8.5.\n\n\n\n\nThe curves in Fig. 8.7 are given by R_{opt} = 2 \\sqrt{\\lambda_0 S} where R_{opt} is the optimal spacing in \\sigma units and S is the number of occasions. The reason for this apparently tight relationship has yet to be determined, and it is unreliable for intensive sampling (large S\\lambda_0), as indicated by the simulations for \\lambda_0 = 0.2 and S \\ge 10.",
    "crumbs": [
      "Performance",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Study design</span>"
    ]
  },
  {
    "objectID": "08-studydesign.html#sec-Amax",
    "href": "08-studydesign.html#sec-Amax",
    "title": "8  Study design",
    "section": "\n8.9 Detectors required for uniform array",
    "text": "8.9 Detectors required for uniform array\nIncreasing imprecision of \\hat D and failure of estimation in many cases (indicated by white crosses in Fig. 8.6) place a limit on the size of region A that can be sampled with a uniform grid of K detectors. At the optimum spacing (i.e. about 2\\sigma\\sqrt{\\lambda_0 S}) we require K &gt;  A / (4\\sigma^2\\lambda_0 S).\nA region of interest with area A \\gg 4K\\sigma^2 \\lambda_0 S cannot be covered adequately with a uniform grid of K detectors. Thus with \\lambda_0 = 0.1 and \\sigma = 50 m, sampling with 100 detectors for S = 5 occasions covers 50 ha at 71-m spacing3. Alternatively, with \\lambda_0 = 0.1 and \\sigma = 1000 m, sampling with 100 detectors for S = 5 occasions covers 200 km2.\nThe spacing may be stretched somewhat from the optimum if we are willing to accept suboptimal precision, but increasing spacing by more than 50% also runs the risk of bias (Fig. 8.6).\nThe ratio R_K = 4\\sigma^2K/A is a useful guide for planning, where K is the number of detectors that can be deployed. R_K \\ge 1.0 allows for a uniform grid, whereas for R_K \\ll 1.0 there are too few detectors for an efficient uniform grid. Here we assume \\sqrt{\\lambda_0 S}\\approx 1.0.",
    "crumbs": [
      "Performance",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Study design</span>"
    ]
  },
  {
    "objectID": "08-studydesign.html#number-of-detectors-in-fixed-region",
    "href": "08-studydesign.html#number-of-detectors-in-fixed-region",
    "title": "8  Study design",
    "section": "\n8.10 Number of detectors in fixed region",
    "text": "8.10 Number of detectors in fixed region\nWe have referred to 2\\sigma\\sqrt{\\lambda_0 S} as an ‘optimum’ spacing for the scenario that the number of detectors is fixed. Increasing the number of detectors in an area of interest can be expected to increase sample sizes, particularly \\mathrm E(r), and hence to improve on precision beyond the ‘optimum’. For a non-arbitrary area R_K &gt; 1.0 is entirely appropriate, and cost becomes the decider. The benefit from increasing the density of detectors may be small; for example, trebling the number of detectors in a 10-ha area from K_0 =  A / (4\\sigma^2\\lambda_0 S) = 62.5 resulted in only a 27% reduction in \\mathrm{RSE}(\\hat D) for the base parameter values of Fig. 8.5.",
    "crumbs": [
      "Performance",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Study design</span>"
    ]
  },
  {
    "objectID": "08-studydesign.html#clustered-designs",
    "href": "08-studydesign.html#clustered-designs",
    "title": "8  Study design",
    "section": "\n8.11 Clustered designs",
    "text": "8.11 Clustered designs\n\nWe have assumed a regular grid of detectors with constant spacing. Regular grids have an almost mystical status in small mammal trapping (Otis et al., 1978) that can be justified, for non-spatial capture–recapture, by the need to expose every individual to the same probability of capture. Checking traps laid in straight lines is also easier than navigating on foot to random points.\nSECR removes the strict requirement for equal exposure, and large-scale studies often use vehicular transport and navigation by GPS, removing the advantage of straight lines. We use the term ‘clustered design’ for any layout of detectors that departs from a simple grid. Clustering may be geometrical, as in a systematic layout of regular subgrids, or incidental, as when detectors are placed at a simple random sample of sites or according to some other non-geometrical rule such as algorithmic optimisation.\n\n8.11.1 Rationale\nThe primary use for a clustered design is to survey a large region of interest with a limited number of detectors, while ensuring some close spacings to allow recaptures (r, m). It is impossible to meet Requirement 3a in this scenario without clustering. The threshold of area for a region to be considered ‘large’ was addressed above.\nLocal density almost certainly varies across any large region of interest. Clustering of detectors implies patchy sampling, with the risk that selective placement of detectors in either high- or low-density areas leads to biased estimates. Spatial variation in detection from excessive clustering also adds to the variance of estimates. The risks are minimised by selecting a widely distributed and spatially representative sample. This can be achieved with a randomly located systematic array of small subgrids (e.g., Clark, 2019), and some other methods to be discussed.\nA secondary application of clustered designs is to encompass heterogeneous \\sigma (e.g., sex differences) by providing a range of spacings. Dupont et al. (2021, p. 8) supposed a general benefit of irregularity “to gain better resolution of movement distances for estimating \\sigma”, but this has yet to be demonstrated.\nClustering of detectors can reduce the total distance that must be traveled to visit all detectors. If travel is a major cost then clustering may allow more detectors to be used. This applies regardless of the size of region. \n\n8.11.2 Systematic clustered designs\nSubgrids\nRandom systematic designs usually provide a representative sample with lower sampling variance than a simple random sample (Cochran, 1977; Thompson, 2012). Any systematic design should have a random origin. For SECR each point on the systematic grid is the location of a subgrid. Subgrids may be any shape. We focus on square and hollow grids (Fig. 8.8), but circles, hexagons, and straight lines are also possible.\nIf subgrids are far apart then few individuals will be caught on more than one subgrid, and they can be designed and analysed as independent units (e.g., Clark, 2019). For design this means that the spacing of detectors on each subgrid should be optimised according to the considerations already raised in Section 8.5 to Section 8.8. For analysis it means that a homogeneous density model may be fitted to aggregated (‘mashed’) data from j subgrids as if they were collected from a single subgrid with the shared geometry and j times the density.\n\n\n\n\n\n\n\nFigure 8.8: Examples of systematic clustered layouts with (a) 98 detectors at 12-m spacing, and (b) 99 detectors at 10-m spacing, in a 10-ha region of interest (grey).\n\n\n\n\nTruncation of subgrids at the edge of the region of interest reduces the size and value of marginal clusters. Two other geometrical designs avoid this problem: a lacework design (available in Efford (2025)) and a spatial coverage design, following Walvoort et al. (2010).\nLacework\n\nIn a lacework design, detectors are placed along two sets of equally spaced lines that cross at right angles to form a lattice (Fig. 8.9). By making the spacing of the lattice lines (lattice spacing a) an integer multiple of the spacing of detectors along lines (detector spacing b) we avoid odd spacing at the intersections. The expected number of points on a lacework design is given by E(K) = A/a2 (2a/b - 1).\n\n\n\n\n\n\n\nFigure 8.9: Lacework with 102 detectors at 17.4-m spacing in a 10-ha region of interest.\n\n\n\n\nAs with any systematic layout, it is desirable to randomize the origin of the lacework. The orientation is arbitrary. Lacework designs have the advantage of requiring only two design variables (a,b) when subgrid methods involve three (number or spacing of subgrids, plus spacing and extent of each subgrid).\nSpatial coverage\n\nSpatial coverage may be achieved by first dividing the region of interest into equal-sized strata and then placing a subgrid in each stratum. A k-means algorithm is used to form the strata as compact clusters of pixels (Walvoort et al., 2010). A detector cluster may be centred or placed at random within each stratum (Fig. 8.10). We include the spatial coverage approach here because it uses nested subgrids, although the result is not strictly systematic.\n\n\n\n\n\n\n\nFigure 8.10: Spatial coverage design with 5 randomly oriented subgrids of 20 detectors centred in equal-area strata.\n\n\n\n\n\n8.11.3 Non-systematic designs\nNon-systematic designs result from algorithms for the placement of detectors that do not impose a particular geometry. One candidate is a simple random sample of points in the region of interest (SRS). Various algorithms improve on SRS with respect to spatial balance and efficiency. We focus on the Generalized Random Tessellation Stratified algorithm implemented by Dumelle et al. (2023) in the R package spsurvey, but there are other options (Robertson et al., 2018).\n\n\n\n\nNon-systematic clustered layouts (a) simple random sample, and (b) Generalized Random Tessellation Stratified sample, 100 detectors in a 10-ha region of interest (grey).\n\n\n\nAlgorithmic optimisation\n \nChoosing a set of detector locations may be reduced to finding a subset of potential locations that maximises some criterion (benefit function). A genetic algorithm is a robust way to search the vast set of possible layouts. Wolters (2015) implemented a genetic algorithm in R that was subsequently applied to the optimisation of SECR designs by Dupont et al. (2021) and Durbach et al. (2021). The set of potential locations may simply be a fine grid of points, excluding points that are deemed inaccessible or fall in non-habitat. Results depend on the choice of criterion: Durbach et al. (2021) used the minimum of E(n) and E(r), following the suggestion of Efford & Boulanger (2019) that this often leads to near-maximal precision of \\hat D. Optimisation may take many iterations and results are not unique.\nA larger criticism is that the resulting layouts are not spatially representative. The example in Fig. 8.11 shows 100 detectors placed “optimally” when RK = 0.16. Detectors are clumped to maximise the precision criterion min(E(n), E(r)) without regard to spatial balance. We view this to be a major weakness and discourage general use of the method except for exploratory purposes.\nAlgorithmic optimisation has revealed one point that makes intuitive sense and is otherwise hidden. AC near the edge tend to have access to fewer detectors than central AC, and increased density of detectors near the edge is beneficial for increasing n, but may reduce r (Dupont et al., 2021).\n\n\n\n\n\n\n\nFigure 8.11: Algorithmic optimisation of layout. (a) 2498 potential detector locations in a 100-ha (1-km2) region, and (b) Layout comprising a subset of 100 points that maximises min(E(n), E(r)) after 1000 iterations with the base parameters in Fig. 8.5.\n\n\n\n\n\n\n8.11.4 Comparison of clustered layouts\nThere is no general way to navigate the multiplicity of clustered designs. As an example, we compare the preceding clustered designs for a particular scenario: 100 binary proximity detectors in a 1-km2 region of interest for a species with \\sigma = 0.02 km (RK = 0.16) keeping other parameters as in Fig. 8.5.\n\n\n\n\n\n\n\nFigure 8.12: Layouts with about 100 detectors in a region 1-km2. Colour codes the probability of detection for an individual centred in each pixel (dark green &lt;0.05, white &gt;0.95). Detectors are joined by a minimum-length route.\n\n\n\n\nAs before, we expected precision to be driven by sample size, n and r, with E(n) maximised by scattering detectors widely. Differences among the designs with respect to the usual measures of performance (RB, RSE, rRMSE) were mostly minor (Table 8.1). The exceptions were SRS (\\mathrm{RB}(\\hat D) +4%) and GRTS (\\mathrm{RB}(\\hat D) +5%); these designs detected many individuals, but generated few re-detections. The lacework design gave similar results to SRS in this instance, but results can be improved with closer spacing along fewer lines.\nThe algorithmically optimised design was one of several with \\mathrm{RSE}(\\hat D) \\approx 11\\%. Tight clustering of detectors resulted in the shortest travel distance of the designs considered. However, ‘GA optimised’ clearly lacked spatial balance and is therefore likely to provide biased estimates of average density if density varies systematically across the region.\nCosting is complex and study-specific. Travel is an important component (time or mileage) and so we computed the length of the shortest route that included all detectors. Other components are the cost of detectors and initial setup (both proportional to K), and laboratory processing of DNA samples (proportional to n+r. Optimal route length is an example of the ‘traveling salesman problem’ (TSP). Heuristic methods in R package TSP (Hahsler & Hornik, 2007) do not produce a single minimum and perform poorly with regular grids. We therefore used the Concorde software outside R to obtain optimal routes.\n\n\nTable 8.1: Performance of seven sampling designs (K \\approx 100 detectors) with respect to sample size (expected numbers of individuals E(n), re-detections E(r), and movements E(m)), relative bias RB, relative standard error RSE, root-mean-square error rRMSE, and route length L. RB, RSE and rRMSE are percentages based on 100 simulations for each design.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDesign\nK\nE(n)\nE(r)\nE(m)\nRB\nRSE\nrRMSE\n\nL km\n\n\n\nsquare subgrid\n98\n191\n101\n54\n0.6\n10.6\n9.7\n6.2\n\n\nhollow subgrid\n99\n196\n99\n52\n0.2\n11.2\n9.5\n5.5\n\n\nlacework\n103\n217\n93\n41\n1.2\n11.7\n11.9\n5.4\n\n\nspatial coverage\n100\n182\n124\n74\n1.0\n9.4\n10.0\n5.9\n\n\nSRS\n100\n215\n92\n39\n1.5\n14.2\n14.9\n7.2\n\n\nGRTS\n100\n239\n68\n12\n2.2\n17.1\n23.3\n8.1\n\n\nGA optimised\n100\n153\n153\n112\n1.0\n10.6\n10.3\n5.2",
    "crumbs": [
      "Performance",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Study design</span>"
    ]
  },
  {
    "objectID": "08-studydesign.html#how-many-clusters",
    "href": "08-studydesign.html#how-many-clusters",
    "title": "8  Study design",
    "section": "\n8.12 How many clusters?",
    "text": "8.12 How many clusters?\nFor the preceding comparison we fixed the number of detectors in advance. More realistically, we might set a target \\mathrm{RSE}(\\hat D) and ask how many clusters (i.e. subgrids) are required. Efford & Boulanger (2019) observed that if clusters are independent (widely spaced) then E(n) and E(r) both scale with the number of clusters c, and \\mathrm{RSE}(\\hat D) scales with 1/ \\sqrt c. Thus we can predict the precision for a single cluster and extrapolate to assemblages of clusters (Fig. 8.13). Having determined the required number of clusters we can decide on a systematic spacing or apply the spatial coverage method.\n\n\n\n\n\n\n\nFigure 8.13: Overall \\mathrm{RSE}(\\hat D) from cluster assemblages varying in size and single-cluster RSE.",
    "crumbs": [
      "Performance",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Study design</span>"
    ]
  },
  {
    "objectID": "08-studydesign.html#designing-for-spatial-variation-in-parameters",
    "href": "08-studydesign.html#designing-for-spatial-variation-in-parameters",
    "title": "8  Study design",
    "section": "\n8.13 Designing for spatial variation in parameters",
    "text": "8.13 Designing for spatial variation in parameters\n\nWe have so far treated the parameters of the SECR model as constant across space. How should designs be modified to allow for spatial variation?\nWe must first understand the interaction between spatial variation in density and sampling intensity. Variation in density alone is not a source of significant bias in SECR. However, when sampling is more intensive in areas of high density, or areas of low density, a model that assumes homogeneous density and sampling produces biased estimates.\nSome aspects of sampling intensity are under the control of the experimenter: these are the type, number and spacing of detectors, and the duration of sampling. We group these under the heading ‘sampling effort’. They will usually be known and included explicitly in the SECR model.\nOther variation in sampling intensity may be due to behavioural or habitat factors that drive spatial variation in detection parameters (e.g., \\lambda_0, \\sigma) unknown to the experimenter.\nWe illustrate six scenarios for the interaction between spatial variation in density and effort, and between density and detection in Fig. 8.14. The estimate of overall density is unbiased when effort and detection are homogeneous (Fig. 8.14 a,d). Uncorrelated variation in effort does not bias \\hat D (Fig. 8.14 b). Selectively reducing effort in low-density areas (Fig. 8.14 c) causes significant bias, although this may be eliminated by including an area effect in the density model.\nVariation in detection parameters causes bias even when not correlated with density (Fig. 8.14 e). This is the effect of inhomogeneous detection noted previously. The effect is large when density and detection are correlated (Fig. 8.14 f and McLellan et al. (2023)).\n\n\n\n\n\n\n\nFigure 8.14: Effect of spatially varying effort (grid size 6 x 6 vs 8 x 8) and detection (\\lambda_0 orange low vs red high) on estimate of overall density when density of AC (white dots) varies spatially between low (light green) and high (light blue). See GitHub for simulation results.\n\n\n\n\nSpatially representative sampling aims to avoid correlation between density and effort (Fig. 8.14 c). A strong spatial model for density overcomes the problem, but it cannot usually be guaranteed that suitable covariates will be found, and representative sampling insures against failure.\nStratified sampling\n\nDeliberate stratification of effort is a special case. Stratification is used in conventional sampling to increase the precision of estimates for a given effort (e.g., Cochran, 1977; Thompson, 2012). We note that in SECR only detected individuals and their re-detections contribute to the estimation of detection parameters and, in this respect, effort in low-density areas is mostly wasted. Greater precision may therefore be achieved in some studies by concentrating effort where high density is expected. This requires care. A stratified analysis combines stratum-specific density estimates as follows.\nSuppose a region of interest with area A may be divided into H subregions with area A_h where A = \\sum_h A_h. Subregions have densities D_1, D_2, ..., D_H. Independent SECR sampling followed by separate model fitting in each region provides estimates \\hat D_1, \\hat D_2, ..., \\hat D_H. The weight for each region is based on its area: W_h = A_h/A and \\sum_h W_h = 1.\nThen an estimate of overall density is \\hat D = \\sum_h W_h \\hat D_h and an estimate of the sampling variance is \\widehat {\\mathrm{var}} (\\hat D) = \\sum_{h=1}^H W_h^2 \\widehat {\\mathrm{var}} (D_h).\nThe benefits of stratification for SECR have yet to be fully analysed. They are likely to be greatest when strata of contrasting density are easily distinguished, and when uncertainty regarding detection parameters (and hence the effective sampling area) dominates \\mathrm{var}(n) = s^2 in the combined variance.\nWe conducted a simulation experiment with two density strata (40% and 160% of the overall average) and a constant total effort allocated differentially (Fig. 8.15). Details are on GitHub. For this example precision was best when about 70% of detectors were placed in the high-density stratum, but the improvement over equal allocation was tiny (\\Delta\\mathrm{RSE} \\approx 0.005) for 10 occasions and small (\\Delta\\mathrm{RSE} \\approx 0.013) for 4 occasions. Effort may also be stratified by varying the number of sampling occasions.\nIt may help to assume that detection parameters are uniform throughout (i.e. shared among strata); a formal test of detection homogeneity is likely have low power.\n\n\n\n\n\n\n\nFigure 8.15: Design for evaluation of stratified effort.\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.16: Precision of overall density estimated with stratified effort. Grey line indicates minimum RSE when 88 of 128 detectors were placed in the high-density stratum (4 occasions, 0.129; 10 occasions, 0.069).\n\n\n\n\nDensity model as alternative to representative sampling\nOur emphasis on representative sampling may seem over the top. SECR is model-based and variance estimates do not depend on the design. Sampling deliberately across environmental gradients may well provide the best information on particular covariate effects, and a fitted model with these covariates may be extrapolated across the region of interest to estimate average density. We advocate representative sampling because it protects against vulnerabilities of a purely model-based approach: the reliance on a particular model, the assumption that it applies in unsampled parts of the region, and the hazard from extrapolation to unobserved values of the covariate(s).\nHowever, modelling is our only tool for handling heterogeneous detection.",
    "crumbs": [
      "Performance",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Study design</span>"
    ]
  },
  {
    "objectID": "08-studydesign.html#summary",
    "href": "08-studydesign.html#summary",
    "title": "8  Study design",
    "section": "\n8.14 Summary",
    "text": "8.14 Summary\nThe SECR method is very flexible, within limits. Our approach to study design for SECR is not prescriptive. Users should bear in mind these principles:\n\nsampling should be representative of the region of interest\nwithout large samples a study may lack the statistical power to answer real-world questions (this is not unique to SECR)\ntwo scales are important - the size of the region of interest and the scale of movement/detection\nrobustness to model misspecification is greatest when the sampling scale is well-matched to the scale of detection\nprecise estimation of density requires both many individuals (n) and many recaptures (r) and these aspects of sample size often entail a design tradeoff\nlarge areas may be sampled with clustered detectors in various configurations\nlogistic feasibility and cost considerations may override power in deciding among designs\nspatially heterogeneous density is not problematic in itself, but becomes so when confounded with spatially heterogeneous detection\nstratified sampling (concentrating sampling effort where density is predicted to be greater) has the potential to increase precision, but in our exploratory simulation the gains were minimal.\n\nIt should be stressed that our simulation results (e.g., Fig. 8.4, Fig. 8.6, Fig. 8.7, Fig. 8.16, and Table 8.1) relate to particular scenarios, and the conclusions we draw may be reversed in other scenarios.\n\n\n\n\n\n\n\n\n\nFigure 8.1: Three scenarios for array size in relation to home range size. For intuition we use a circle to indicate a hard-edged uniform home range. When the array is too small, recaptures are equally likely anywhere in the array. When the spacing is too large, recaptures must be at the same detector. In either case, the recaptures carry almost no information on the spatial scale of detection \\sigma.\nFigure 8.2: Power of a 2-sided test (\\alpha = 0.05) to detect change in density between two surveys as a function of effect size ((D2/D1 − 1) × 100 on the x-axis) for two levels of \\mathrm{RSE}(\\hat D) in the initial survey. For initial RSE = 0.1, only changes greater than –36% and +44% are detected with power greater that 80% (dashed line). Reproduced from Efford & Boulanger (2019) Fig. 1.\nFigure 8.3: Relationship between density and the spatial scale parameter \\sigma for different values of the overlap index k.\nFigure 8.4: Effect of array size on relative bias (RB) and relative root-mean-square error (rRMSE) of density estimates. Simulations of a square array with size (diagonal length) divided by the diameter of a 95% BVN home range. Density was estimated with both the detection function used to generate the data (HHN) and a mis-specified detection function (HEX).\nFigure 8.5: Sample size as a function of detector spacing for two detector types. Square grid of 64 detectors operated for 10 occasions with D = 0.5\\sigma^{-2} and \\lambda_0 = 0.1.\nFigure 8.6: Relationship between precision and detector spacing for scenarios of differing grid size and detection rate \\lambda_0 (5 sampling occasions, reproduced from Fig. 2, Efford & Boulanger, 2019).\nFigure 8.7: Effect of sampling intensity (number of occasions and \\lambda_0) on optimal detector spacing from simulations. Base scenario as in Fig. 8.5.\nFigure 8.8: Examples of systematic clustered layouts with (a) 98 detectors at 12-m spacing, and (b) 99 detectors at 10-m spacing, in a 10-ha region of interest (grey).\nFigure 8.9: Lacework with 102 detectors at 17.4-m spacing in a 10-ha region of interest.\nFigure 8.10: Spatial coverage design with 5 randomly oriented subgrids of 20 detectors centred in equal-area strata.\nNon-systematic clustered layouts (a) simple random sample, and (b) Generalized Random Tessellation Stratified sample, 100 detectors in a 10-ha region of interest (grey).\nFigure 8.11: Algorithmic optimisation of layout. (a) 2498 potential detector locations in a 100-ha (1-km2) region, and (b) Layout comprising a subset of 100 points that maximises min(E(n), E(r)) after 1000 iterations with the base parameters in Fig. 8.5.\nFigure 8.12: Layouts with about 100 detectors in a region 1-km2. Colour codes the probability of detection for an individual centred in each pixel (dark green &lt;0.05, white &gt;0.95). Detectors are joined by a minimum-length route.\nFigure 8.13: Overall \\mathrm{RSE}(\\hat D) from cluster assemblages varying in size and single-cluster RSE.\nFigure 8.14: Effect of spatially varying effort (grid size 6 x 6 vs 8 x 8) and detection (\\lambda_0 orange low vs red high) on estimate of overall density when density of AC (white dots) varies spatially between low (light green) and high (light blue). See GitHub for simulation results.\nFigure 8.15: Design for evaluation of stratified effort.\nFigure 8.16: Precision of overall density estimated with stratified effort. Grey line indicates minimum RSE when 88 of 128 detectors were placed in the high-density stratum (4 occasions, 0.129; 10 occasions, 0.069).\n\n\n\nClark, J. D. (2019). Comparing clustered sampling designs for spatially explicit estimation of population density. Population Ecology, 61, 93–101.\n\n\nCochran, W. G. (1977). Sampling techniques. (3rd ed.). John Wiley; Sons.\n\n\nDumelle, M., Kincaid, T., Olsen, A. R., & Weber, M. (2023). Spsurvey: Spatial sampling design and analysis in r. Journal of Statistical Software, 105. https://doi.org/10.18637/jss.v105.i03\n\n\nDupont, G., Royle, J. A., Nawaz, M. A., & Sutherland, C. (2021). Optimal sampling design for spatial capture-recapture. Ecology, 102, e03262.\n\n\nDurbach, I., Borchers, D., Sutherland, C., & Sharma, K. (2021). Fast, flexible alternatives to regular grid designs for spatial capture-recapture. Methods in Ecology and Evolution, 12(298-310), 2041–2210. https://doi.org/10.1111/2041-210X.13517\n\n\nEfford, M. G. (2011). Estimation of population density by spatially explicit capture-recapture analysis of data from area searches. Ecology, 92, 2202–2207.\n\n\nEfford, M. G. (2025). secr: Spatially explicit capture-recapture models. https://CRAN.R-project.org/package=secr\n\n\nEfford, M. G., & Boulanger, J. (2019). Fast evaluation of study designs for spatially explicit capture-recapture. Methods in Ecology and Evolution, 10, 1529–1535. https://doi.org/10.1111/2041-210X.13239\n\n\nEfford, M. G., Dawson, D. K., Jhala, Y. V., & Qureshi, Q. (2016). Density-dependent home-range size revealed by spatially explicit capture-recapture. Ecography, 39, 676–688.\n\n\nEfford, M. G., & Fewster, R. M. (2013). Estimating population size by spatially explicit capture-recapture. Oikos, 122, 918–928.\n\n\nEfford, M. G., Warburton, B., Coleman, M. C., & Barker, R. J. (2005). A field test of two methods for density estimation. Wildlife Society Bulletin, 33, 731–738.\n\n\nHahsler, M., & Hornik, K. (2007). TSP- infrastructure for the traveling salesperson problem. Journal of Statistical Software, 23(2). https://doi.org/10.18637/jss.v023.i02\n\n\nJennrich, R. I., & Turner, F. B. (1969). Measurement of non-circular home range. Journal of Theoretical Biology, 22, 227–237. https://doi.org/10.1016/0022-5193(69)90002-2\n\n\nKendeigh, S. C. (1944). Measurement of bird populations. Ecological Monographs, 14, 67–106. https://doi.org/10.2307/1961632\n\n\nMcLellan, B. A., Howe, E., Marrotte, R. R., & Northrup, J. M. (2023). Accounting for heterogeneous density and detectability in spatially explicit capture–recapture studies of carnivores. Ecosphere, 14(10). https://doi.org/10.1002/ecs2.4669\n\n\nNoss, A. J., Gardner, B., Maffei, L., Cuéllar, E., Montaño, R., Romero‐Muñoz, A., Sollman, R., & O’Connell, A. F. (2012). Comparison of density estimation methods for mammal populations with camera traps in the &lt;scp&gt;k&lt;/scp&gt;aa‐&lt;scp&gt;i&lt;/scp&gt;ya del &lt;scp&gt;g&lt;/scp&gt;ran &lt;scp&gt;c&lt;/scp&gt;haco landscape. Animal Conservation, 15(5), 527–535. https://doi.org/10.1111/j.1469-1795.2012.00545.x\n\n\nOtis, D. L., Burnham, K. P., White, G. C., & Anderson, D. R. (1978). Statistical inference from capture data on closed animal populations. Wildlife Monographs, 62.\n\n\nPalmero, S., Premier, J., Kramer‐Schadt, S., Monterroso, P., & Heurich, M. (2023). Sampling variables and their thresholds for the precise estimation of wild felid population density with camera traps and spatial capture–recapture methods. Mammal Review, 53(4), 223–237. https://doi.org/10.1111/mam.12320\n\n\nRobertson, B., McDonald, T., Price, C., & Brown, J. (2018). Halton iterative partitioning: Spatially balanced sampling via partitioning. Environmental and Ecological Statistics, 25(3), 305–323. https://doi.org/10.1007/s10651-018-0406-6\n\n\nSollmann, R., Gardner, B., & Belant, J. L. (2012). How does spatial study design influence density estimates from spatial capture-recapture models? PLoS ONE, 7(4), e34575. https://doi.org/10.1371/journal.pone.0034575\n\n\nSun, C. C., Fuller, A. K., & Royle, J. A. (2014). Trap configuration and spacing influences parameter estimates in spatial capture-recapture models. PLoS ONE, 9(2), e88025. https://doi.org/10.1371/journal.pone.0088025\n\n\nThompson, S. K. (2012). Sampling. In Wiley Series in Probability and Statistics. Wiley. https://doi.org/10.1002/9781118162934\n\n\nTobler, M. W., & Powell, G. V. N. (2013). Estimating jaguar densities with camera traps: Problems with current designs and recommendations for future studies. Biological Conservation, 159, 109–118.\n\n\nWalvoort, D. J. J., Brus, D. J., & Gruijter, J. J. de. (2010). An R package for spatial coverage sampling and random sampling from compact geographical strata by k-means. Computers & Geosciences, 36, 1261–1267. https://doi.org/10.1016/j.cageo.2010.04.005\n\n\nWolters, M. A. (2015). A genetic algorithm for selection of fixed-size subsets with application to design problems. Journal of Statistical Software, Code Snippets, 68, 1–18. https://doi.org/10.18637/jss.v068.c01",
    "crumbs": [
      "Performance",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Study design</span>"
    ]
  },
  {
    "objectID": "08-studydesign.html#footnotes",
    "href": "08-studydesign.html#footnotes",
    "title": "8  Study design",
    "section": "",
    "text": "The internal function dfcast of secrdesign provides a more precise match e.g., secrdesign:::dfcast(detectfn = 'HN', detectpar=list(g0 = 0.2, sigma = 25))↩︎\nMore generally, \\sigma = \\sqrt \\frac{a}{\\pi \\log [(1-p)^{-2}]} where p is the probability contour e.g., p = 0.95. (Jennrich & Turner, 1969 Eq. 12).↩︎\nCalculations for binary proximity detectors and a hazard half-normal detection function.↩︎",
    "crumbs": [
      "Performance",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Study design</span>"
    ]
  },
  {
    "objectID": "09-secr-package.html",
    "href": "09-secr-package.html",
    "title": "9  R package secr",
    "section": "",
    "text": "9.1 History\nsecr supercedes the Windows program DENSITY, an earlier graphical interface to SECR methods (Efford et al., 2004; Efford, 2012). The package was first released in March 2010 and continues to be developed. It implements almost all the methods described by Borchers & Efford (2008), Efford, Borchers, et al. (2009), Efford (2011), Efford & Fewster (2013), Efford et al. (2013), Efford & Mowat (2014), Efford et al. (2016), Efford & Hunter (2018) and Efford (2025b). External C++ code (Eddelbuettel et al., 2023) is used for computationally intensive operations. Multi-threading on multiple CPUs with RcppParallel (Allaire et al., 2023) gives major speed gains. The package is available from CRAN; the development version is on GitHub.\nAn interactive graphic user interface to many features of secr is provided by the Shiny app ‘secrapp’ available on GitHub and, currently, on a server at the University of Otago.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>R package secr</span>"
    ]
  },
  {
    "objectID": "09-secr-package.html#object-classes",
    "href": "09-secr-package.html#object-classes",
    "title": "9  R package secr",
    "section": "\n9.2 Object classes",
    "text": "9.2 Object classes\n \nR operates on objects each of which has a class. secr defines a set of R classes and methods for spatial capture–recapture data and models fitted to those data. To perform an SECR analysis you construct each of these objects in turn. Fig. 9.1 indicates the relationships among the classes.\n\n\n\n\n\n\nClasses\n\n\n\nA ‘class’ in R specifies a particular type of data object and the functions (methods) by which it is manipulated (computed, printed, plotted etc). Technically, secr uses old-fashioned ‘S3’ classes. See the R documentation ?class for further explanation.\n\n\n\n\nTable 9.1: Essential classes in secr\n\n\n\n\n\n\n\nClass\nData\n\n\n\ntraps\nlocations of detectors; detector type (‘proximity’, ‘multi’, etc.)\n\n\ncapthist\nspatial detection histories, including a ‘traps’ object\n\n\nmask\nraster map of habitat near the detectors\n\n\nsecr\nfitted SECR model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.1: Workflow in secr\n\n\n\n\n\nEach object class (shaded box) comes with methods to display and manipulate the data it contains (e.g. print, summary, plot, rbind, subset).\nThe function read.capthist forms a ‘capthist’ object from input in two files, one the detector layout (saved as attribute ‘traps’) and the other the capture data.\nBy default, a habitat mask is generated automatically by secr.fit using a specified buffer around the detectors (traps) (dashed arrow). The function make.mask gives greater control over this step.\nAny of the objects input to secr.fit (traps, capthist, mask) may include a dataframe of covariates saved as an attribute. Covariate names may be used in model formulae; the covariates method is used to extract or replace covariates. Use addCovariates for trap or mask covariates from spatial data sources (e.g., shapefile or ‘sf’ object)\nFitted secr models may be manipulated with the methods shown on the right.\n\n\n\n\n\n\n\nTip\n\n\n\nAvoid using ‘[’ to extract subsets from capthist, traps, mask and other secr objects. Use the provided subset methods. It is generally safe to use ‘[[’ to extract one session from a multi-session capthist object.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>R package secr</span>"
    ]
  },
  {
    "objectID": "09-secr-package.html#functions",
    "href": "09-secr-package.html#functions",
    "title": "9  R package secr",
    "section": "\n9.3 Functions",
    "text": "9.3 Functions\n\nFor details of how to use each secr function see the help pages and vignettes.\n\n\nTable 9.2: Core functions of secr. S3 methods are marked with an asterisk\n\n\n\nFunction\nPurpose\n\n\n\naddCovariates\nadd spatial covariates to traps or mask\n\n\n\nAIC*\nmodel selection, model weights\n\n\ncovariates\nextract or replace covariates of traps, capthist or mask\n\n\n\nderived*\ncompute density from conditional likelihood models\n\n\nmake.mask\nconstruct habitat mask (= mesh)\n\n\n\nplot*\nplot capthist, traps or mask\n\n\n\npredict*\ncompute ‘real’ parameters for arbitrary levels of predictor variables\n\n\npredictDsurface\nevaluate density surface at each point of a mask\n\n\nread.capthist\ninput captures and trap layout from Density format, one call\n\n\n\nregion.N*\ncompute expected and realised population size in specified region\n\n\nsecr.fit\nmaximum likelihood fit; result is a fitted ‘secr’ object\n\n\n\nsummary*\nsummarise capthist, traps, mask, or fitted model\n\n\ntraps\nextract or replace traps object in capthist",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>R package secr</span>"
    ]
  },
  {
    "objectID": "09-secr-package.html#detector-types",
    "href": "09-secr-package.html#detector-types",
    "title": "9  R package secr",
    "section": "\n9.4 Detector types",
    "text": "9.4 Detector types\n\nEach ‘traps’ object has a detector type attribute that is a character value.\n\n\nTable 9.3: Basic detector types in secr. See the appendices for area-search and telemetry types.\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n“single”\nsingle-catch trap\ncatch one animal at a time\n\n\n“multi”\nmulti-catch trap\nmay catch more than one animal at a time\n\n\n“proximity”\nbinary proximity\nrecords presence at a point without restricting movement\n\n\n“count”1\n\nPoisson count proximity\n[binomN = 0] allows &gt;1 detection per animal per time\n\n\n\nBinomial count proximity\n[binomN &gt; 0] up to binomN detections per animal per time\n\n\n\n\n\n\n\nThe “count” detector type is generic for integer data; the actual type depends on the secr.fit argument ‘binomN’.\n\nThere is limited support in secr for the analysis of locational data from telemetry (‘telemetry’ detector type). Telemetry data are used to augment capture–recapture data (Appendix G).",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>R package secr</span>"
    ]
  },
  {
    "objectID": "09-secr-package.html#input",
    "href": "09-secr-package.html#input",
    "title": "9  R package secr",
    "section": "\n9.5 Input",
    "text": "9.5 Input\nData input is covered in the data input vignette secr-datainput.pdf. One option is to use text files in the formats used by DENSITY; these accommodate most types of data. Two files are required, one of detector (trap) coordinates and one of the detections (captures) themselves; the function read.capthist reads both files and constructs a capthist object. It is also possible to construct the capthist object in two stages, first making a traps object (with read.traps) and a captures dataframe, and then combining these with make.capthist. This more general route may be needed for unusual datasets.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>R package secr</span>"
    ]
  },
  {
    "objectID": "09-secr-package.html#output",
    "href": "09-secr-package.html#output",
    "title": "9  R package secr",
    "section": "\n9.6 Output",
    "text": "9.6 Output\nFunction secr.fit returns an object of class secr. This is an R list with many components. Assigning the output to a named object saves both the fit and the data for further manipulation. Typing the object name at the R prompt invokes print.secr which formats the key results. These include the dataframe of estimates from the predict method for secr objects. Functions are provided for further computations on secr objects (e.g., AIC model selection, model averaging, profile-likelihood confidence intervals, and likelihood-ratio tests). Several of these were listed in Table 9.2.\n\nOne system of units is used throughout . Distances are in metres and areas are in hectares (ha). The unit of density for 2-dimensional habitat is animals per hectare. 1 ha = 10000 m2 = 0.01 km2. To convert density to animals per km2, multiply by 100. Density in linear habitats (see package secrlinear) is expressed in animals per km.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>R package secr</span>"
    ]
  },
  {
    "objectID": "09-secr-package.html#documentation-and-support",
    "href": "09-secr-package.html#documentation-and-support",
    "title": "9  R package secr",
    "section": "\n9.7 Documentation and support",
    "text": "9.7 Documentation and support\n\nThe primary documentation for secr is in the help pages that accompany the package. Help for a function is obtained in the usual way by typing a question mark at the R prompt, followed by the function name. Note the ‘Index’ link at the bottom of each help page – you will probably need to scroll down to find it. The index may also be accessed with help(package = secr). Static and potentially outdated versions of the help pages are available here.\nThe consolidated help pages are in the manual. Searching this pdf is a powerful way to locate a function for a particular task.\nOther documentation has traditionally been in the form of pdf vignettes built with knitr and available at https://otago.ac.nz/density/SECRinR. That content has been included in this book.\nThe GitHub repository holds the development version, and bugs may be reported there by raising an Issue. New versions will be posted on CRAN and noted on https://www.otago.ac.nz/density/, but there may be a delay. For information on changes in each version, type at the R prompt:\n\nnews (package = \"secr\") \n\nHelp may be sought in online forums such as phidot and secrgroup.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>R package secr</span>"
    ]
  },
  {
    "objectID": "09-secr-package.html#using-secr.fit",
    "href": "09-secr-package.html#using-secr.fit",
    "title": "9  R package secr",
    "section": "\n9.8 Using secr.fit\n",
    "text": "9.8 Using secr.fit\n\n\nWe saw secr.fit in action in Chapter 2. Here we expand on particular arguments.\n\n9.8.1 buffer – buffer width\n\nSpecifying the buffer width is an alternative to specifying a habitat mask. The choice of buffer width is discussed at length in Chapter 12.\n\n9.8.2 start – starting values\n\nNumerical maximization of the likelihood requires a starting value for each coefficient in the model. secr.fit relieves the user of this chore by applying an algorithm that works in most cases. The core of the algorithm is exported in function autoini.\n\nCompute an approximate bivariate normal \\sigma from the 2-D dispersion of individual locations: \n\\sigma = \\sqrt{\\frac {\\sum\\limits _{i=1}^{n} \\sum\\limits _{j=1}^{n_i} [\n     (x_{i,j} - \\overline x_i)^2 + (y_{i,j} - \\overline y_i)^2]}\n     {2\\sum\\limits _{i=1}^{n} (n_i-1)}},\n where (x_{i,j}, y_{i,j}) is the location of the j-th detection of individual i. This is implemented in the function RPSV with CC = TRUE. The value is approximate because it ignores that detections are constrained by the locations of the detectors.\nFind by numerical search the value of g_0 that with \\sigma predicts the observed mean number of captures per individual (Efford, Dawson, et al., 2009, Appendix B).\nCompute the effective sampling area a(g_0, \\sigma).\nCompute D = n/a(g_0, \\sigma), where n is the number of individuals detected.\n\nAfter transformation this provides intercepts on the link scale for the core parameters D, g_0 and \\sigma. For hazard models \\lambda_0 is first set to -\\log(1 - g_0). The starting values of further coefficients are set to zero on the link scale.\nUsers may provide their own starting values. These may be a vector of coefficients on the link scale, a named list of values for ‘real’ parameters, or a previously fitted model that includes some or all of the required coefficients.\n\n9.8.3 model – detection and density sub-models\n\nThe core parameters are ‘real’ parameters in the terminology of MARK (Cooch & White, 2023). Three real parameters are commonly modelled in secr: ‘D’ (for density), and ‘g0’ and ‘sigma’ (for the detection function). Other ‘real’ parameters appear in particular contexts. ‘z’ is a shape parameter that is used only when the detection function has three parameters. Some detection functions primarily model the cumulative hazard of detection, rather than the probability of detection; these use the real parameter ‘lambda0’ in place of ‘g0’. A further ‘real’ parameter is the mixing proportion ‘pmix’, used in finite mixture models and hybrid mixture models.\nBy default, each ‘real’ parameter is assumed to be constant over time, space and individual. We specify more interesting, and often better-fitting, models with the ‘model’ argument of secr.fit. Here ‘model’ refers to variation in a parameter that may be explained by known factors and covariates, perhaps better designated a ‘sub-model’ of the overarching SECR model (Chapter 3).\nSub-models are defined symbolically using a subset of the R formula notation. A separate linear predictor is used for each core parameter. The model argument of secr.fit is a list of formulae, one for each ‘real’ parameter. Null formulae such as D ~ 1 may be omitted, and a single non-null formula may be presented on its own rather than in list() form.\nSub-models are constructed differently for detection and density parameters as explained in Chapter 10 and Chapter 11.\n\n9.8.4 CL – conditional vs full likelihood\n\n‘CL’ switches between maximizing the likelihood conditional on n (TRUE) or the full likelihood (FALSE). The conditional option is faster because it does not estimate absolute density. Uniform absolute density may be estimated from the conditional fit, or indeed any fit, with the derived method. For Poisson n (the default), the estimate is identical within numerical error to that from the full likelihood. The alternative (binomial n) is obtained by setting the details argument ‘distribution = “binomial”’. Relative density (density modelled as a function of covariates, without an intercept) may be modelled with CL = TRUE in recent versions (see sections on the theory and implementation of relative density).\n\n9.8.5 method – maximization algorithm\n\nModels are fitted in secr.fit by numerically maximizing the log-likelihood with functions from the stats package (R Core Team, 2024). The default method is ‘Newton-Raphson’ in the function stats::nlm. By default, all reported variances, covariances, standard errors and confidence limits are asymptotic and based on a numerical estimate of the information matrix, as described here.\nThe Newton-Raphson algorithm is fast, but it sometimes fails to compute the information matrix correctly, causing some standard errors to be set to NA; see the ‘method’ argument of secr.fit for alternatives. Use confint.secr for profile likelihood intervals and sim.secr for parametric bootstrap intervals (both are slow).\nNumerical maximization has some implications for the user. Computation may be slow, especially if there are many points in the mask, and estimates may be sensitive to the particular choice of mask (either explicitly in make.mask or implicitly via the ‘buffer’ argument).\n\n9.8.6 ncores – multi-threading\n \nOn processors with multiple cores it is possible to speed up computation by using cores in parallel. In this happens automatically in secr.fit and a few other functions using the multi-threading paradigm of RcppParallel (Allaire et al., 2023). The number of threads may be set directly with the ‘ncores’ argument or with the separate function setNumThreads. Either way, the number of threads is stored in the environment variable RCPP_PARALLEL_NUM_THREADS.\n\n9.8.7 details – miscellaneous arguments\n\nMany minor or infrequently used arguments are grouped as ‘details’. We mention only the most important ones here:\n\ndistribution\nfastproximity\nmaxdistance\nfixedbeta\nLLonly\n\nSee ?details for a full list and description.\n\n9.8.7.1 Distribution of n\n\n \nThis details argument switches between two possibilities for the distribution of n: ‘poisson’ (the default) or ‘binomial’. Binomial n conditions on fixed N(A) where A is the area of the habitat mask. This corresponds to point process with a fixed number of activity centres inside an arbitrary boundary. Estimates of density conditional on N(A) have lower variance, but this is usually an artifact of the conditioning and therefore misleading.\n\n9.8.7.2 Fast proximity\n\nBinary and count data collected over several occasions may be collapsed to a single occasion, under certain conditions. Collapsed data lead to the same estimates (e.g., Efford, Dawson, et al., 2009) with a considerable saving in execution time. Data from binary proximity detectors are modelled as binomial with size equal to the number of occasions. The requirement is that no information is lost that is relevant to the model. This really depends on the model: collapsed data are inadequate for time-dependent models, including those with behavioural responses (Chapter 10).\nBy default, data are automatically collapsed to speed up processing when the model allows. This is inconvenient if you wish to use AIC to compare a variety of models. The problem is solved by setting ‘details = list(fastproximity = FALSE)’ for all models. Fitting will be slower.\n\n9.8.7.3 Individual mask subset\n\nIntegration by default is performed by summing over all mask cells for each individual. Cells distant from the detection locations of an individual contribute almost nothing to the likelihood, so it is efficient to limit the summation to a certain radius of the centroid of detections. This is achieved by specifying the details argument ‘maxdistance’. A radius similar to the buffer width is appropriate. See Section B.8 for an example.\n\n9.8.7.4 Fixing coefficients\n\nThe ‘fixed’ argument of secr.fit has the effect of fixing one or more ‘real’ parameters. The ‘details’ component ‘fixedbeta’ provides control at a finer level: it may be used to fix certain coefficients while allowing others to vary. It is a vector of values, one for each coefficient in the order they appear in the model. Coefficients that are to be estimated (i.e. not fixed) are given the value NA. Check the order of coefficients by applying coef to a fitted model, or by starting to fit a model with trace = TRUE.\n\n9.8.7.5 Single likelihood evaluation\nSetting LLonly = TRUE returns a single evaluation of the likelihood at the parameter values specified in start.\n\n\n\nFigure 9.1: Workflow in secr\n\n\n\nAllaire, J., Francois, R., Ushey, K., Vandenbrouck, G., Geelnard, M., & Intel. (2023). RcppParallel: Parallel programming tools for ’rcpp’. https://CRAN.R-project.org/package=RcppParallel\n\n\nBorchers, D. L., & Efford, M. G. (2008). Spatially explicit maximum likelihood methods for capture-recapture studies. Biometrics, 64, 377–385.\n\n\nCooch, E., & White, G. (Eds.). (2023). Program MARK: A gentle introduction. (23rd ed.). http://www.phidot.org/software/mark/docs/book/\n\n\nEddelbuettel, D., Francois, R., Allaire, J., Ushey, K., Kou, Q., Russell, N., Ucar, I., Bates, D., & Chambers, J. (2023). Rcpp: Seamless r and c++ integration. https://CRAN.R-project.org/package=Rcpp\n\n\nEfford, M. G. (2011). Estimation of population density by spatially explicit capture-recapture analysis of data from area searches. Ecology, 92, 2202–2207.\n\n\nEfford, M. G. (2012). DENSITY 5.0: Software for spatially explicit capture-recapture. Department of Mathematics; Statistics, University of Otago. https://www.otago.ac.nz/density\n\n\nEfford, M. G. (2025a). secr: Spatially explicit capture-recapture models. https://CRAN.R-project.org/package=secr\n\n\nEfford, M. G. (2025b). Spatially explicit capture–recapture models for relative density. BioRxiv. https://doi.org/10.1101/2025.01.22.634401\n\n\nEfford, M. G., Borchers, D. L., & Byrom, A. E. (2009). Density estimation by spatially explicit capture-recapture: Likelihood-based methods. In D. L. Thomson, E. G. Cooch, & M. J. Conroy (Eds.), Modeling demographic processes in marked populations (pp. 255–269). Springer.\n\n\nEfford, M. G., Borchers, D. L., & Mowat, G. (2013). Varying effort in capture-recapture studies. Methods in Ecology and Evolution, 4, 629–636.\n\n\nEfford, M. G., Dawson, D. K., & Borchers, D. L. (2009). Population density estimated from locations of individuals on a passive detector array. Ecology, 90, 2676–2682.\n\n\nEfford, M. G., Dawson, D. K., Jhala, Y. V., & Qureshi, Q. (2016). Density-dependent home-range size revealed by spatially explicit capture-recapture. Ecography, 39, 676–688.\n\n\nEfford, M. G., Dawson, D. K., & Robbins, C. S. (2004). DENSITY: Software for analysing capture-recapture data from passive detector arrays. Animal Biodiversity and Conservation, 27, 217–228.\n\n\nEfford, M. G., & Fewster, R. M. (2013). Estimating population size by spatially explicit capture-recapture. Oikos, 122, 918–928.\n\n\nEfford, M. G., & Hunter, C. M. (2018). Spatial capture-mark-resight estimation of animal population density. Biometrics, 74, 411–420. https://doi.org/10.1111/biom.12766\n\n\nEfford, M. G., & Mowat, G. (2014). Compensatory heterogeneity in spatially explicit capture-recapture data. Ecology, 95, 1341–1348.\n\n\nKendeigh, S. C. (1944). Measurement of bird populations. Ecological Monographs, 14, 67–106. https://doi.org/10.2307/1961632\n\n\nR Core Team. (2024). R: A language and environment for statistical computing. R Foundation for Statistical Computing. https://www.R-project.org/",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>R package secr</span>"
    ]
  },
  {
    "objectID": "10-detection-model.html",
    "href": "10-detection-model.html",
    "title": "10  Detection model",
    "section": "",
    "text": "10.1 Distance-detection functions\nThe probability of detection g(d) at a detector distance d from an activity centre may take one of the simple forms in Table 10.1. Alternatively, the probability of detection may be derived from g(d) = 1 - \\exp[-\\lambda(d)] where \\lambda(d) is the hazard of detection, itself modelled with one of the simple parametric forms (Table 10.2)1. Several further options are provided by secr.fit (see ? detectfn), but only ‘HN’,‘HR’, and ‘EX’ or their ‘hazard’ equivalents are commonly used.\nThe merits of focussing on the hazard3 are a little arcane. We list them here:\nThe ‘hazard variable power’ function is a 3-parameter function modelled on that of Ergon & Gardner (2013). The third parameter allows for smooth variation of shape, including both HHN (z = 2) and HEX (z = 1) as special cases.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Detection model</span>"
    ]
  },
  {
    "objectID": "10-detection-model.html#sec-detectfn",
    "href": "10-detection-model.html#sec-detectfn",
    "title": "10  Detection model",
    "section": "",
    "text": "Table 10.1: Probability detection functions.\n\n\n\n\n\n\n\n\n\nCode\nName\nParameters\nFunction\n\n\n\nHN\nhalfnormal\ng_0, \\sigma\ng(d) = g_0 \\exp \\left(\\frac{-d^2} {2\\sigma^2} \\right)\n\n\nHR\nhazard rate2\n\ng_0, \\sigma, z\ng(d) = g_0 [1 - \\exp\\{{-(^d/_\\sigma)^{-z}} \\}]\n\n\nEX\nexponential\ng_0, \\sigma\ng(d) = g_0  \\exp \\{-(^d/_\\sigma) \\}\n\n\n\n\n\n\n\n\n\nTable 10.2: Hazard detection functions.\n\n\n\n\n\n\n\n\n\nCode\nName\nParameters\nFunction\n\n\n\nHHN\nhazard halfnormal\n\\lambda_0, \\sigma\n\\lambda(d) = \\lambda_0 \\exp \\left(\\frac{-d^2} {2\\sigma^2} \\right)\n\n\nHHR\nhazard hazard rate\n\\lambda_0, \\sigma, z\n\\lambda(d) = \\lambda_0 (1 - \\exp \\{ -(^d/_\\sigma)^{-z} \\})\n\n\nHEX\nhazard exponential\n\\lambda_0, \\sigma\n\\lambda(d)  = \\lambda_0 \\exp \\{ -(^d/_\\sigma) \\}\n\n\nHVP\nhazard variable power\n\\lambda_0, \\sigma, z\n\\lambda(d) = \\lambda_0 \\exp \\{ -(^d/_\\sigma)^{z} \\}\n\n\n\n\n\n\n\n\nQuantities on the hazard scale are additive and more tractable for some purposes (e.g. adjusting for effort, computing expected counts).\nFor some detector types (e.g., Poisson counts) the data are integers, for which \\lambda(d) has a direct interpretation as the expected count. However, \\lambda(d) can always be derived from g(d) (\\lambda(d) = -\\log[1 - g(d)]).\nIntuitively, there is a close proportionality between \\lambda(d) and the height of an individual’s utilization pdf.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nUsing the ‘complementary log-log’ link function cloglog for a hazard detection function such as \\lambda(d) = \\lambda_0 \\exp[-d^2/(2\\sigma^2)] is equivalent to modelling \\lambda with a log link, as p = 1 - \\exp(-\\lambda) and \\lambda = -\\log(1-p). For a halfnormal function, the quantity y on the cloglog scale is then a linear function of \\exp(-d^2) with intercept \\alpha_0 = \\log(\\lambda_0) and slope \\alpha_1 = -1/(2\\sigma^2) (e.g., Royle et al., 2013).\n\n\n\n10.1.1 Choice of detection function not critical\nThe variety of detection functions is daunting. You could try them all and select the “best” by AIC, but we do not recommend this. Fortunately, the choice of function is not critical. We illustrate this with the snowshoe hare dataset of Chapter 2. The function list.secr.fit) is used to fit a series of models. Warnings due to the use of a multi-catch likelihood for single-catch traps are suppressed both here and in other examples.\n\ndf &lt;- c('HHN','HHR','HEX','HVP')\nfits &lt;- list.secr.fit(detectfn = df, constant = list(capthist = hareCH6, \n              buffer = 250, trace = FALSE), names = df)\n\n\n\n\n\n\n\n\nFigure 10.1: Four detection functions fitted to snowshoe hare data.\n\n\n\n\nThe relative fit of the HHR, HVP and HEX models is essentially the same, whereas HHN is distinctly worse:\n\nAIC(fits)[, c(2,3,4,7,8)]\n##                  detectfn npar  logLik   dAIC  AICwt\n## HHR    hazard hazard rate    4 -599.68  0.000 0.5474\n## HVP hazard variable power    4 -600.43  1.487 0.2603\n## HEX    hazard exponential    3 -601.73  2.092 0.1923\n## HHN     hazard halfnormal    3 -608.07 14.783 0.0000\n\n\n\n\n\n\n\n\nFigure 10.2: Parameter estimates from four detection functions (95% CI).\n\n\n\n\nThe third parameter z was estimated as 3.08 for HHR, and 0.67 for HVP.\nFitting the HVP function with z fixed to different values is another way to examine the effect of shape (Fig. 10.3). Density estimates ranged only from 1.47 to 1.42.\n\n\n\n\n\n\n\nFigure 10.3: HVP function fitted to snowshoe hare data with z parameter fixed at four different values.\n\n\n\n\nHow can different functions produce nearly the same estimates? Remember that \\hat D = n/a(\\hat \\theta), and n is the same for all models. Constant \\hat D therefore implies constant effective sampling area a(\\hat \\theta). In other words, variation in \\hat \\lambda_0 and \\hat \\sigma ‘washes out’ when they are combined in a(\\hat \\theta). Under the four models a(\\hat \\theta) is estimated as 46.4, 48.4, 45.9 and 46.7 ha.\n\n10.1.2 Detection parameters are nuisance parameters (mostly)\nThe detection model and its parameters (g_0, \\lambda_0, \\sigma etc.) provide the link between our observations and the state of the animal population represented by the parameter D (density, distribution in space, trend etc.). Sub-models for D are considered in Chapter 11. But what interpretation should we attach to the detection parameters themselves?\nThe intercept g_0 is in a sense the probability of that an animal will be detected at the centre of its home range. The spatial scale of detection \\sigma relates to the size of the home range. These attributed meanings can aid intuitive understanding. However, we advise against a literal reading. The estimates have meaning only for a specified detection function and cannot meaningfully be compared across functions. Observe the\nThe halfnormal function is closest to a standard reference, but estimates of halfnormal \\sigma are sensitive to infrequent large movements. Care is also needed because some early writers omitted the factor 2 from the denominator, increasing estimates of \\sigma by \\sqrt 24.\nContinuing the snowshoe hare example: the estimates of \\lambda_0 and \\sigma from HVP are surprisingly uncertain when considered on their own, yet the HVP estimate of density has about the same precision as other detection functions (Fig. 10.2). How can this be? There is strong covariation in the sampling distributions of the two parameters that we plot using ellipse.secr in Fig. 10.4.\n\n\n\n\n\n\n\nFigure 10.4: Confidence ellipse for HVP detection parameters plotted on the link scale and back-transformed to natural scale. ‘+’ indicates MLE.\n\n\n\n\n\n10.1.3 SECR is not distance sampling\nThe idea of a distance-detection function originated in distance sampling (Buckland et al., 2001) and Borchers et al. (2015) provided a unified framework for spatially explicit capture–recapture and distance sampling. Nevertheless, the role of the detection function differs substantially.\nIn distance sampling, shape matters a lot. In particular, the estimate of density depends on the slope of the detection function near the origin, given the assumption that all animals at the origin are detected (e.g., Buckland et al., 2015).\nIn SECR, no special significance is attached to the intercept or the shape of the function. The detection function serves as a spatial filter for a modelled 2-dimensional point pattern of activity centres; the filter must ‘explain’ the frequency of recaptures and their spatial spread. These are the components of the effective sampling area.\nThe hazard-rate function HR is recommended for distance sampling because it has a distinct ‘shoulder’ near zero distance, and distance sampling is not concerned with the tail (distant observations are often censored). SECR relies on the tail flattening to zero within the region of integration defined by the habitat mask. Otherwise, the population at risk of detection is determined by the choice of mask, which is usually arbitrary and ad hoc. The hazard-rate function has an extremely long tail (it is not convergent), so there is always a risk of mask-dependence. As an aside - in the snowshoe hare example with detection function HHR we suppressed the warning “predicted relative bias exceeds 0.01 with buffer = 250” that is due to truncation of the long tail.\n\n10.1.4 Why bother?\nGiven the preceding comments you may wonder why we bother with different detection functions at all. In part this is historical: it was not obvious in the beginning that density estimates were so robust. Sometimes it’s just nice to have the flexibility to match the model to animal behaviour. Functions with longer tails (e.g., HEX) accommodate occasional extreme movements that can prevent a short-tailed function (HHN) from fitting at all.\nAlso, it is desirable to account for any significant lack of fit due to the detection function before modelling effects that may have a more critical effect on density estimates, such as individual heterogeneity and learned responses.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Detection model</span>"
    ]
  },
  {
    "objectID": "10-detection-model.html#sec-linear-submodels",
    "href": "10-detection-model.html#sec-linear-submodels",
    "title": "10  Detection model",
    "section": "\n10.2 Detection submodels",
    "text": "10.2 Detection submodels\n\nUntil now we have assumed that there is a single beta parameter for each real parameter. A much richer set of models is obtained by treating each real parameter as a function of covariates. For convenience, the function is linear on the appropriate link scale. The single ‘beta’ coefficient is then replaced by two or more coefficients (e.g., intercept \\beta_0 and slope \\beta_1 of the linear relationship y = \\beta_0 + \\beta_1x_1 where y is a parameter on the link scale and x_1 is a covariate). Suppose, for example, that y depends on sampling occasion s then y(s) = \\beta_0 + \\beta_1x_1(s) and the corresponding real parameter is y(s) back transformed from the link scale.\nThis may be generalised using the notation of linear models, \n\\mathbf y = \\mathbf X \\pmb {\\beta},\n\\tag{10.1}\nwhere \\mathbf X is the design matrix,  \\pmb{\\beta} is a vector of coefficients, and \\mathbf y is the resulting vector of values on the link scale, one for each row of \\mathbf X. The first column of the design matrix is a column of 1’s for the intercept \\beta_0. Factor (categorical) predictors will usually be represented by several columns of indicator values (0’s and 1’s coding factor levels). See Cooch & White (2023) Chapter 6 for an accessible introduction to linear models and design matrices.\nIn secr each detection parameter (g_0, \\lambda_0, \\sigma, z) is controlled by a linear sub-model on its link scale, i.e. each has its own design matrix. Rows of the design matrix correspond to combinations of session, individual, occasion, and detector, omitting any of these four that is constant (perhaps because there is only one level). Finite-mixture models add further rows to the design matrix that we leave aside for now. Columns after the first are either (i) indicators to represent effects that can be constructed automatically (Table 10.3), or (ii) user-supplied covariates associated with sessions, individuals, occasions or detectors.\n\n\nTable 10.3: Automatically generated predictor variables for detection models\n\n\n\nVariable\nDescription\nNotes\n\n\n\ng\ngroup\n\ngroups are defined by the individual covariate(s) named in the ‘groups’ argument\n\n\nt\ntime factor\none level for each occasion\n\n\nT\ntime trend\nlinear trend over occasions on link scale\n\n\nb\nlearned response\nstep change after first detection\n\n\nB\ntransient response\ndepends on detection at preceding occasion (Markovian response)\n\n\nbk\nanimal x site response\nsite-specific step change\n\n\nBk\nanimal x site response\nsite-specific transient response\n\n\nk\nsite learned response\nsite effectiveness changes once any animal caught\n\n\nK\nsite transient response\nsite effectiveness depends on preceding occasion\n\n\nsession\nsession factor\none level for each session\n\n\nSession\nsession trend\nlinear trend on link scale\n\n\nh2\n2-class mixture\n\nfinite mixture model with 2 latent classes\n\n\nts\nmarking vs sighting\ntwo levels (marking and sighting occasions)\n\n\n\n\n\n\nEach design matrix is constructed automatically when secr.fit is called, using the data and a model formula. Computation of the linear predictor (Eq. 10.1) and back-transformation to the real scale are also automatic: the user need never see the design matrix.\n\n\n\n\n\n\nSpatially varying detection\n\n\n\nThe hard-wired structure of the design matrices precludes some possible sub-models: there is no direct way to model spatial variation in a detection parameter. This was a choice made in the design of the software. It aimed to tame the complexity and resource demands that would result if lambda0, g0 and sigma were allowed to vary continuously in space. However, spatial effects may be modelled efficiently using detector-level covariates, i.e. as a function of detector location rather than AC location, and a further workaround for parameter \\sigma is shown in Appendix F.\n\n\nThe formula may be constant (\\sim 1, the default) or some combination of terms in standard R formula notation (see ?formula). For example, g0 \\sim b + T specifies a model with a learned response and a linear time trend in g0; the effects are additive on the link scale. Table 10.4 has some examples.\n\n\nTable 10.4: Some examples of the ‘model’ argument in secr.fit\n\n\n\n\n\n\n\nFormula\nEffect\n\n\n\ng0 \\sim 1\ng0 constant across animals, occasions and detectors\n\n\ng0 \\sim b\nlearned response affects g0\n\n\nlist(g0 \\sim b, sigma \\sim b)\nlearned response affects both g0 and sigma\n\n\ng0 \\sim h2\n2-class finite mixture for heterogeneity in g0\n\n\ng0 \\sim b + T\nlearned response in g0 combined with trend over occasions\n\n\nsigma \\sim g\ndetection scale sigma differs between groups\n\n\nsigma \\sim g*T\ngroup-specific trend in sigma\n\n\n\n\n\n\n\nThe common question of how to model sex differences can be answered in several ways. we devote Chapter 16 to the possibilities (groups, individual covariate, hybrid mixtures etc.).\n\n\n\n\n\n\nNote\n\n\n\nLinear sub-models for parameters are considered by Cooch & White (2023) as a constraint on a more general model. Their default is for each parameter to be fully-time-specific e.g., a Cormack-Jolly-Seber open population survival model would fit a unique detection probability p and survival rate \\phi at each time. Our default is for each parameter to be constant (i.e. maximally constrained), and for linear sub-models to introduce variation.\n\n\n\n10.2.1 Covariates\n\nAny name in a formula that is not listed as a variable in Table 10.3 is assumed to refer to a user-supplied covariate. secr.fit looks for user-supplied covariates in data frames embedded in the ‘capthist’ argument, or supplied in the ‘timecov’ and ‘sessioncov’ arguments, or named with the ‘timevaryingcov’ attribute of a traps object, using the first match (Table 10.5).\n\n\nTable 10.5: Types of user-provided covariate for parameters of detection models. The names of columns in the respective dataframes may be used in model formulae. Time-varying detector covariates are a special case considered below.\n\n\n\nCovariate type\nData source\n\n\n\nIndividual\ncovariates(capthist)\n\n\nTime\ntimecov argument\n\n\nDetector\ncovariates(traps(capthist))\n\n\nDetector x Time\ncovariates(traps(capthist))\n\n\nSession\nsessioncov argument\n\n\n\n\n\n\n\nA continuous covariate that takes many unique values poses problems for the implementation in secr. A multiplicity of values inflates the size of internal lookup tables, both slowing down each likelihood evaluation and potentially exceeding the available memory5. A binned covariate should do the job equally well, while saving time and space (see function binCovariate).\n\n10.2.2 Time-varying trap covariates\n\nA special mechanism is provided for detector-level covariates that take different values on each occasion. Then we expect the dataframe of detector covariates to include a column for each occasion.\nA ‘traps’ object may have an attribute ‘timevaryingcov’ that is a list in which each named component is a vector of indices identifying which covariate column to use on each occasion. The name may be used in model formulae. Use timevaryingcov() to extract or replace the attribute.\n\n10.2.3 Regression splines\n\nModelling a link-linear6 relationship between a covariate and a parameter may be too restrictive.\nRegression splines are a very flexible way to represent non-linear responses in generalized additive models, implemented in the R package mgcv (Wood, 2006). Borchers & Kidney (2014) showed how they may be used to model 2-dimensional trend in density. They used mgcv to construct regression spline basis functions from mask x- and y-coordinates, and possibly additional mask covariates, and then passed these as covariates to secr.fit. Smooth, semi-parametric responses are also useful for modelling variation in detection parameters such as g_0 and \\sigma over time, or in response to numeric individual- or detector-level covariates, when (1) a linear or other parametric response is arbitrary or implausible, and (2) sampling spans a range of times or levels of the covariate(s).\nSmooth terms may be used in secr model formulae for both density and detection parameters. The covariate is merely wrapped in a call to the smoother function s(). Smoothness is controlled by the argument ‘k’.\nFor a concrete example, consider a population sampled monthly for a year (i.e. 12 ‘sessions’). If home range size varies seasonally then the parameter sigma may vary in a more-or-less sinusoidal fashion. A linear trend is obviously inadequate, and a quadratic is not much better. However, a sine curve is hard to fit (we would need to estimate its phase, amplitude, mean and spatial scale) and assumes the increase and decrease phases are equally steep. An extreme solution is to treat month as a factor and estimate a separate parameter for each level (month). A smooth (semi-parametric) curve may capture the main features of seasonal variation with fewer parameters.\nThere are some drawbacks to using smooth terms. The resulting fitted objects are large, on account of the need to store setup information from mgcv. The implementation may change in later versions of mgcv and secr, and smooth models fitted now will not necessarily be compatible with later versions. Setting the intercept of a smooth to zero is not a canned option in mgcv, and is not offered in secr. It may be achieved by placing a knot at zero and hacking the matrix of basis functions to drop the corresponding column, plus some more jiggling.\n\n10.2.4 Why bother?\nDetailed modelling of detection parameters may be a waste of energy for the same reasons that the choice of detection function itself has limited interest. See, for example, the simulation results of Sollmann (2024) on occasion-specific models (\\sim t). However, behavioural responses and individual heterogeneity can have a major effect on density estimates, and these deserve attention.\n\n10.2.4.1 Behavioural responses\n\nAn individual behavioral response is a change in the probability or hazard of detection on the occasions that follow a detection. Trapping of small mammals provides evidence of species that routinely become trap happy (presumably because they enjoy the bait) or trap shy (presumably because the experience of capture and handling is unpleasant). Positive or negative responses are modelled as a step change in a detection parameter, usually the intercept of the detection function (g_0, \\lambda_0).\nThe response may be permanent (b) or transient (B) (i.e. applying only on the next occasion). In spatial models we also distinguish between a global response, across all detectors, and a local response, specific to the initial detector (suffix ‘k’). This leads to four response models: b, bk, B, and Bk.\nWe explore these options with Reid’s Wet Swizer Gulch deer mouse (Peromyscus maniculatus) dataset from Otis et al. (1978). Mice were trapped on a grid of 99 traps over 6 days. The Sherman traps were treated as multi-catch traps for this analysis. We fit the four behavioural response models and the null model to the morning data.\n\ncmod &lt;- paste0('g0~', c('1','b','B','bk','Bk'))\n# convert each character string to a formula and fit the models\nfits &lt;- list.secr.fit(model = sapply(cmod, formula), constant = \n    list(capthist = 'deermouse.WSG', trace = FALSE, buffer = 80), \n    names = cmod)\nAIC(fits, sort = FALSE)[c(3:5,7,8)]\n\n      npar  logLik    AIC    dAIC AICwt\ng0~1     3 -663.54 1333.1 129.938     0\ng0~b     4 -643.72 1295.4  92.298     0\ng0~B     4 -651.62 1311.2 108.099     0\ng0~bk    4 -597.57 1203.1   0.000     1\ng0~Bk    4 -621.02 1250.0  46.910     0\n\n\nAll response models are preferred to the null model, but the differences among them are marked: the evidence supports a persistent local response (bk). The density estimates for bk and Bk are close to the null model, whereas the b and B estimates are greater. In our experience this is a common result: a local response is preferred by AIC and has less impact on density estimates than a global response, and there is little penalty for omitting the response from the model.\n\ncollate(fits, realnames = 'D')[1,,,]\n\n      estimate SE.estimate    lcl    ucl\ng0~1    14.089      2.0364 10.629 18.676\ng0~b    18.531      3.3459 13.044 26.324\ng0~B    15.726      2.3351 11.774 21.005\ng0~bk   13.884      2.1108 10.324 18.672\ng0~Bk   13.883      2.0472 10.414 18.506\n\n\nThe estimated magnitude of the responses may be examined with predict(fits[2:5], all.levels = TRUE) but the output is long and we show only the global (b) and local (bk) enduring responses:\n\n\n$`g0~b`\n                     estimate   lcl   ucl\nsession = WSG, b = 0    0.055 0.031 0.096\nsession = WSG, b = 1    0.210 0.163 0.267\n\n$`g0~bk`\n                      estimate   lcl   ucl\nsession = WSG, bk = 0    0.061 0.044 0.085\nsession = WSG, bk = 1    0.596 0.454 0.723\n\n\nHere ‘b = 0’ and ‘bk = 0’ refer to g_0 for a naive animal and ‘b = 1’ and ‘bk = 1’ refer to the post-detection values (estimates are shown with 95% limits). It appears that deermice are highly likely to return to traps where they have been caught.\nDetector-level ‘behavioural’ response is also possible (predictors k, K in secr.fit). Detection of any individuals at a detector may in principle be facilitated or inhibited by a previous detection there of any other individual. We are not aware of published examples.\nThe trap-facilitation model fitted to the deer mouse data results in a larger and less precise estimate of density (16.78/ha, SE 2.59/ha), with AIC intermediate between the null model and bk (facilitation model \\DeltaAIC = 73.7 relative to bk). There is a risk of confusing such an effect with simple heterogeneity in the performance of detectors or clumping of activity centres or an individual local response (bk). More investigation is needed.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Detection model</span>"
    ]
  },
  {
    "objectID": "10-detection-model.html#varying-effort",
    "href": "10-detection-model.html#varying-effort",
    "title": "10  Detection model",
    "section": "\n10.3 Varying effort",
    "text": "10.3 Varying effort\n\nResearchers are often painfully aware of glitches in their data gathering - traps that were not set, sampling occasions missed or delayed due to weather etc. Even when the actual estimates are robust, as in an example below, it is desirable (therapeutic and scientific) to allow for known irregularities in the data. This is the role of the ‘usage’ matrix as described in Chapter 3.\nThe ‘usage’ attribute of a ‘traps’ object in secr is a K x S matrix recording the effort (T_{sk}) at each detector k = 1...K and occasion s = 1...S. Effort may be binary (0/1) or continuous. If the attribute is missing (NULL) it will be treated as all ones. Extraction and replacement functions are provided (usage and usage&lt;-, as demonstrated below). All detector types accept usage data in the same format. The usage matrix for polygon and transect detectors has one row for each polygon or transect, rather than one row per vertex.\nBinomial count detectors are a special case. When the secr.fit argument binomN = 1, or equivalently binomN = ‘usage’, usage is interpreted as the size of the binomial distribution (the maximum possible number of detections of an animal at a detector on one occasion).\n\n10.3.1 Input of usage data\nUsage data may be input as extra columns in a file of detector coordinates (see ?read.traps and secr-datainput.pdf).\nUsage data also may be added to an existing traps object, even after it has been included in a capthist object. For example, the traps object in the demonstration dataset ‘captdata’ starts with no usage attribute, but we can add one. Suppose that traps 14 and 15 were not set on occasions 1–3. We construct a binary usage matrix and assign it to the traps object like this:\n\nK &lt;- nrow(traps(captdata))\nS &lt;- ncol(captdata)\nmat &lt;- matrix(1, nrow = K, ncol = S)\nmat[14:15,1:3] &lt;- 0   # traps 14:15 not set on occasions 1:3\nusage(traps(captdata)) &lt;- mat\n\n\n10.3.2 Models\nThe usage attribute of a traps object is applied automatically by secr.fit. Following on from the preceding example, we can confirm our assignment and fit a new model.\n\nsummary(traps(captdata))    # confirm usage attribute\n\nObject class       traps \nDetector type      single \nDetector number    100 \nAverage spacing    30 m \nx-range            365 635 m \ny-range            365 635 m \n\nUsage range by occasion\n    1 2 3 4 5\nmin 0 0 0 1 1\nmax 1 1 1 1 1\n\nfit &lt;- secr.fit(captdata, buffer = 100, trace = FALSE, biasLimit = NA)\npredict(fit)\n\n       link estimate SE.estimate      lcl      ucl\nD       log  5.47346    0.645991  4.34659  6.89249\ng0    logit  0.27473    0.027164  0.22479  0.33102\nsigma   log 29.39668    1.308422 26.94206 32.07494\n\n\nThe result in this case is only subtly different from the model with uniform usage (compare predict(secrdemo.0)). Setting biasLimit = NA avoids a warning message from secr.fit regarding bias.D: this function is usually run by secr.fit after any model fit using the ‘buffer’ argument, but it does not handle varying effort.\nUsage is hardwired and will be applied whenever a model is fitted. There are two ways to suppress this. The first is to remove the usage attribute (usage(traps(captdata)) &lt;- NULL). The second is to bypass the attribute for a single fit by calling secr.fit with ‘details = list(ignoreusage = TRUE)’.\nFor a more informative example, we simulate data from an array of binary proximity detectors (such as automatic cameras) operated over 5 occasions, using the default density (5/ha) and detection parameters (g0 = 0.1, sigma = 25 m) in sim.capthist. We choose to expose all detectors twice as long on occasions 2 and 3 as on occasion 1, and three times as long on occasions 4 and 5:\n\nsimgrid &lt;- make.grid(nx = 10, ny = 10, detector = 'proximity')\nusage(simgrid) &lt;- matrix(c(1,2,2,3,3), byrow = TRUE, nrow = 100, \n    ncol = 5)\nsimCH &lt;- sim.capthist(simgrid, popn = list(D = 5, buffer = 100), \n    detectpar = list(g0 = 0.1, sigma = 25), noccasions = 5, \n    seed = 123)\nsummary(simCH)\n\nObject class       capthist \nDetector type      proximity (5) \nDetector number    100 \nAverage spacing    20 m \nx-range            0 180 m \ny-range            0 180 m \n\nUsage range by occasion\n    1 2 3 4 5\nmin 1 2 2 3 3\nmax 1 2 2 3 3\n\nCounts by occasion \n                    1   2   3   4   5 Total\nn                  15  18  23  29  23   108\nu                  15   7   6   7   1    36\nf                   8   6   9   4   9    36\nM(t+1)             15  22  28  35  36    36\nlosses              0   0   0   0   0     0\ndetections         26  32  39  54  55   206\ndetectors visited  24  28  33  41  44   170\ndetectors used    100 100 100 100 100   500\n\n\nNow we fit four models with a half-normal detection function. The firest model (fit.null) has no adjustment because we ignore the usage information. The second (fit.usage) automatically adjusts for effort. The third (fit.tcov1) again ignores effort, but fits a distinct g0 for each level of effort. The fourth (fit.tcov2) uses a numerical covariate equal to the known effort. The setting fastproximity = FALSE allows all models can be compared by AIC.\n\n# shared arguments for model fits 1-4\ntimedf &lt;- data.frame(tfactor = factor(c(1,2,2,3,3)), tnumeric = \n    c(1,2,2,3,3))\nargs &lt;- list(capthist = simCH, buffer = 100, biasLimit = NA, \n    timecov = timedf, trace = FALSE)\nmodels &lt;- c(g0 ~ 1, g0 ~ 1, g0 ~ tfactor, g0 ~ tnumeric)\ndetails &lt;- rep(list(list(ignoreusage = TRUE, fastproximity = \n    FALSE)), 4)\ndetails[[2]]$ignoreusage &lt;- FALSE\n\n# review arguments\ndata.frame(model = format(models), ignoreusage = sapply(details,\n    '[[', 'ignoreusage'))\n\n          model ignoreusage\n1        g0 ~ 1        TRUE\n2        g0 ~ 1       FALSE\n3  g0 ~ tfactor        TRUE\n4 g0 ~ tnumeric        TRUE\n\n# fit\nfits &lt;- list.secr.fit(model = models, details = details, constant = \n    args, names = c('null','usage','tfactor','tnumeric'))\nAIC(fits)[,-c(2,5,6)]\n\n                           model npar  logLik   dAIC  AICwt\nusage           D~1 g0~1 sigma~1    3 -737.20  0.000 0.4212\ntnumeric D~1 g0~tnumeric sigma~1    4 -736.24  0.072 0.4063\ntfactor   D~1 g0~tfactor sigma~1    5 -736.09  1.785 0.1725\nnull            D~1 g0~1 sigma~1    3 -744.90 15.390 0.0000\n\n\nFrom the likelihoods we can see that failure to allow for effort (model ‘null’) dramatically reduces model fit. The model with a factor covariate (‘tfactor’) captures the variation in detection probability, but at the cost of fitting two additional parameters. The model with built-in adjustment for effort (‘usage’) has AIC similar to one with effort as a numeric covariate (‘tnumeric’). How do the estimates compare? This is a task for the collate function.\n\ncollate(fits, newdata = timedf)[,,'estimate','g0']\n\n                        null   usage tfactor tnumeric\ntfactor=1,tnumeric=1 0.21001 0.10112 0.13349  0.12551\ntfactor=2,tnumeric=2 0.21001 0.10112 0.18047  0.18851\ntfactor=2,tnumeric=2 0.21001 0.10112 0.18047  0.18851\ntfactor=3,tnumeric=3 0.21001 0.10112 0.27772  0.27324\ntfactor=3,tnumeric=3 0.21001 0.10112 0.27772  0.27324\n\n\nThe ‘null’ model fits a single g0 across all occasions that is approximately twice the true rate on occasion 1 (0.1). The estimates of g0 from ‘tfactor’ and ‘tnumeric’ mirror the variation in effort. The effort-adjusted ‘usage’ model estimates the fundamental rate for one unit of effort (0.1).\n\ncollate(fits)[,,,'D']\n\n         estimate SE.estimate    lcl    ucl\nnull       4.9709     0.84830 3.5663 6.9287\nusage      4.9755     0.84881 3.5700 6.9344\ntfactor    4.9703     0.84811 3.5660 6.9277\ntnumeric   4.9698     0.84805 3.5656 6.9271\n\n\nThe density estimates themselves are almost entirely unaffected by the choice of model for g0. This is not unusual (Sollmann, 2024). Nevertheless, the example shows how ‘usage’ allows unbalanced data to be analysed with a minimum of fuss.\n\n10.3.3 Further notes on varying effort\n\nAdjustment for varying effort will be more critical in analyses where (i) the variation is confounded with temporal (between-session) or spatial variation in density, and (ii) it is important to estimate the temporal or spatial pattern. For example, if detector usage was consistently high in one part of a landscape, while true density was constant, failure to allow for varying usage might produce a spurious density pattern.\nThe units of usage determine the units of g_0 or \\lambda_0 in the fitted model. This must be considered when choosing starting values for likelihood maximization. Ordinarily one relies on secr.fit to determine starting values automatically (via autoini), and a simple linear adjustment for usage, averaged across non-zero detectors and occasions, is applied to the value of g0 from autoini.\nWhen occasions are collapsed or detectors are lumped with the reduce method for capthist objects, usage is summed for each aggregated unit.\nThe function usagePlot displays a bubble plot of spatially varying detector usage on one occasion. The arguments ‘markused’ and ‘markvarying’ of plot.traps may also be useful.\nAbsolute duration does not always equate with effort. Animal activity may be concentrated in part of the day, or older DNA samples from hair snares may fail to amplify (Efford et al., 2013).\nBinary or count data from searches of polygons or transects (Efford, 2011) do not raise any new issues for including effort, at least when effort is homogeneous across each polygon or transect. Effects of varying polygon or transect size are automatically accommodated in the models of Chapter 4. Models for varying effort within polygons or transects have not been needed for problems encountered to date. Such variation might in any case be accommodated by splitting the searched areas or transects into smaller units that were more nearly homogeneous (see the snip function for splitting transects).\n\n\n\n\nFigure 10.1: Four detection functions fitted to snowshoe hare data.\nFigure 10.2: Parameter estimates from four detection functions (95% CI).\nFigure 10.3: HVP function fitted to snowshoe hare data with z parameter fixed at four different values.\nFigure 10.4: Confidence ellipse for HVP detection parameters plotted on the link scale and back-transformed to natural scale. ‘+’ indicates MLE.\n\n\n\nBorchers, D. L., & Kidney, D. (2014). Flexible density surface estimation for spatially explicit capture–recapture surveys. University of St Andrews. https://research-repository.st-andrews.ac.uk/bitstream/handle/10023/9147/secrgam_techrep.pdf?sequence=1\n\n\nBorchers, D. L., Stevenson, B. C., Kidney, D., Thomas, L., & Marques, T. A. (2015). A unifying model for capture–recapture and distance sampling surveys of wildlife populations. Journal of the American Statistical Association, 110(509), 195–204. https://doi.org/10.1080/01621459.2014.893884\n\n\nBuckland, S. T., Anderson, D. R., Burnham, K. P., Laake, J. L., Borchers, D. L., & Thomas, L. (2001). Introduction to distance sampling. Oxford University Press.\n\n\nBuckland, S. T., Rexstad, E. A., Marques, T. A., & Oedekoven, C. S. (2015). Distance sampling: Methods and applications. Springer.\n\n\nCooch, E., & White, G. (Eds.). (2023). Program MARK: A gentle introduction. (23rd ed.). http://www.phidot.org/software/mark/docs/book/\n\n\nEfford, M. G. (2011). Estimation of population density by spatially explicit capture-recapture analysis of data from area searches. Ecology, 92, 2202–2207.\n\n\nEfford, M. G., Borchers, D. L., & Mowat, G. (2013). Varying effort in capture-recapture studies. Methods in Ecology and Evolution, 4, 629–636.\n\n\nErgon, T., & Gardner, B. (2013). Separating mortality and emigration: Modelling space use, dispersal and survival with robust‐design spatial capture–recapture data. Methods in Ecology and Evolution, 5(12), 1327–1336. https://doi.org/10.1111/2041-210x.12133\n\n\nGardner, B., Royle, J. A., & Wegan, M. T. (2009). Hierarchical models for estimating density from DNA mark-recapture studies. Ecology, 90, 1106–1115.\n\n\nHayes, R. J., & Buckland, S. T. (1983). Radial-distance models for the line-transect method. Biometrics, 39, 29–42.\n\n\nKendeigh, S. C. (1944). Measurement of bird populations. Ecological Monographs, 14, 67–106. https://doi.org/10.2307/1961632\n\n\nOtis, D. L., Burnham, K. P., White, G. C., & Anderson, D. R. (1978). Statistical inference from capture data on closed animal populations. Wildlife Monographs, 62.\n\n\nRoyle, J. A., Chandler, R. B., Sollmann, R., & Gardner, B. (2014). Spatial capture-recapture. Academic Press.\n\n\nRoyle, J. A., Chandler, R. B., Sun, C. C., & Fuller, A. K. (2013). Integrating resource selection information with spatial capture–recapture. Methods in Ecology and Evolution, 4(6), 520–530. https://doi.org/10.1111/2041-210x.12039\n\n\nRoyle, J. A., Fuller, A. K., & Sutherland, C. (2015). Spatial capture–recapture models allowing markovian transience or dispersal. Population Ecology, 58(1), 53–62. https://doi.org/10.1007/s10144-015-0524-z\n\n\nRoyle, J. A., & Gardner, B. (2011). Hierarchical spatial capture-recapture models for estimating density from trapping arrays. In A. F. O’Connell, J. D. Nichols, & K. U. Karanth (Eds.), Camera traps in animal ecology: Methods and analyses (pp. 163–190). Springer.\n\n\nRoyle, J. A., Magoun, A. J., Gardner, B., Valkenburg, P., & Lowell, R. E. (2011). Density estimation in a wolverine population using spatial capture-recapture models. Journal of Wildlife Management, 75, 604–611.\n\n\nSollmann, R. (2024). Mt or not mt: Temporal variation in detection probability in spatial capture-recapture and occupancy models. Peer Community Journal, 4. https://doi.org/10.24072/pcjournal.357\n\n\nWood, S. N. (2006). Generalized additive models: An introduction with R. Chapman; Hall/CRC.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Detection model</span>"
    ]
  },
  {
    "objectID": "10-detection-model.html#footnotes",
    "href": "10-detection-model.html#footnotes",
    "title": "10  Detection model",
    "section": "",
    "text": "The transformation is non-linear so, for example, a half-normal form for g(.) does not correspond to half-normal form for \\lambda(.).↩︎\nThis use of ‘hazard’ has historical roots in distance sampling (Hayes & Buckland, 1983) and has no real connection to models for hazard as a function of distance.↩︎\nTechnically this is the cumulative hazard rather than the instantaneous hazard, but we get tired of using the full term.↩︎\nExamples are Gardner et al. (2009), Royle & Gardner (2011) and Royle et al. (2011), but not Royle et al. (2014) and Royle et al. (2015)).↩︎\nIn the C++ code, two real-valued 3-dimensional arrays are populated with pre-computed values of p_{sk}(\\mathbf x) (gk) and h_{sk}(\\mathbf x) (hk). The dimensions are the number of unique parameter combinations C, the number of detectors K and the number of mask points M. The memory requirement for these arrays alone is 2.8.C.K.M bytes, which for 200 detectors, 10000 mask points, and 100 parameter levels is 3.2 Gb. This is on top of the two parameter index arrays requiring 2 . 4 . R. n. S. K. U bytes for R sessions and U mixture classes (e.g. 10 sessions, 200 animals, 6 occasions, 200 detectors and 2 mixture classes, 0.0384 Gb), and a number of smaller objects.↩︎\nWe use ‘link-linear’ to describe a linear model on the link scale, where this may be log-linear, logit-linear etc.↩︎",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Detection model</span>"
    ]
  },
  {
    "objectID": "11-density-model.html",
    "href": "11-density-model.html",
    "title": "11  Density model",
    "section": "",
    "text": "11.1 Absolute vs relative density\nThe conventional approach to density surfaces is to fit a model of absolute density by maximizing the ‘full’ likelihood. Until recently, maximizing the likelihood conditional on n, the number detected, was thought to work only when density was assumed to be uniform (homogeneous), placing it outside the scope of this chapter. However, recent extensions to secr allow models of relative density to be fitted by maximizing the conditional likelihood (CL = TRUE in secr.fit), and this has some advantages (Efford, 2025). Relative density differs from absolute density by a constant factor. This chapter stresses modelling absolute density and keeps relative density for a section at the end.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Density model</span>"
    ]
  },
  {
    "objectID": "11-density-model.html#brushtail-possum-example",
    "href": "11-density-model.html#brushtail-possum-example",
    "title": "11  Density model",
    "section": "\n11.2 Brushtail possum example",
    "text": "11.2 Brushtail possum example\n\nFor illustration we use a brushtail possum (Trichosurus vulpecula) dataset from the Orongorongo Valley, New Zealand. Possums were live-trapped in mixed evergreen forest near Wellington for nearly 40 years (Efford & Cowan, 2004). Single-catch traps were set for 5 consecutive nights, three times a year. The dataset ‘OVpossumCH’ has data from the years 1996 and 1997. The study grid was bounded by a shingle riverbed to the north and west. See ?OVpossum in secr for more details.\nFirst we import data for the habitat mask from a polygon shapefile included with the package:\n\ndatadir &lt;- system.file(\"extdata\", package = \"secr\")\nOVforest &lt;- sf::st_read (paste0(datadir, \"/OVforest.shp\"), \n    quiet = TRUE)\n# drop points we don't need\nleftbank &lt;- read.table(paste0(datadir,\"/leftbank.txt\"))[21:195,]  \noptions(digits = 6, width = 95)       \n\nOVforest is now a simple features (sf) object defined in package sf. We build a habitat mask object, selecting the first two polygons in OVforest and discarding the third that lies across the river. The attribute table of the shapefile (and hence OVforest) includes a categorical variable ‘forest’ that is either ‘beech’ (Nothofagus spp.) or ‘nonbeech’ (mixed podocarp-hardwood); addCovariates attaches these data to each cell in the mask.\n\novtrap &lt;- traps(OVpossumCH[[1]])\novmask &lt;- make.mask(ovtrap, buffer = 120, type = \"trapbuffer\",\n    poly = OVforest[1:2,], spacing = 7.5, keep.poly = FALSE)\novmask &lt;- addCovariates(ovmask, OVforest[1:2,])\n\nPlotting is easy:\n\ncode to plot possum maskpar(mar = c(1,6,2,8))\nforestcol &lt;- terrain.colors(6)[c(4,2)]\nplot(ovmask, cov=\"forest\", dots = FALSE, col = forestcol)\nplot(ovtrap, add = TRUE)\npar(cex = 0.8)\nterra::sbar(d = 200, xy = c(2674670, 5982930), type = 'line', \n    divs = 2, below = \"metres\", labels = c(\"0\",\"100\",\"200\"), \n    ticks = 10)\nterra::north(xy = c(2674670, 5982830), d = 40, label = \"N\")\n\n\n\n\n\n\nFigure 11.1: Orongorongo Valley possum study area\n\n\n\n\nWe next fit some simple models to data from February 1996 (session 49). Some warnings are suppressed for clarity.\n\nargs &lt;- list(capthist = OVpossumCH[['49']], mask = ovmask, trace = FALSE)\nmodels &lt;- list(D ~ 1, D ~ x + y, D ~ x + y + x2 + y2 + xy, D ~ forest)\nnames &lt;- c('null','Dxy','Dxy2', 'Dforest')\nfits &lt;- list.secr.fit(model = models, constant = args, names = names)\n\n\nAIC(fits)[,-c(1,2,5,6)]\n\n        npar   logLik  dAIC  AICwt\nDxy2       8 -1549.32 0.000 0.4429\nDxy        5 -1552.86 1.086 0.2573\nDforest    4 -1554.15 1.653 0.1938\nnull       3 -1555.75 2.860 0.1060\n\n\nEach of the inhomogeneous models seems marginally better than the null model, but there is little to choose among them.\nTo visualise the entire surface we compute predicted density at each mask point. For example, we can plot the quadratic surface like this:\n\npar(mar = c(1,6,2,8))\nsurfaceDxy2 &lt;- predictDsurface(fits$Dxy2)\nplot(surfaceDxy2, plottype = \"shaded\", poly = FALSE, breaks = \n      seq(0,22,2), title = \"Density / ha\", text.cex = 1)\n# graphical elements to be added, including contours of Dsurface\nplot(ovtrap, add = TRUE)\nplot(surfaceDxy2, plottype = \"contour\", poly = FALSE, breaks = \n    seq(0,22,2), add = TRUE)\nlines(leftbank)\n\n\n\n\n\n\nFigure 11.2: Quadratic possum density surface\n\n\n\n\nFollowing sections expand on the options for specifying and displaying density models.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Density model</span>"
    ]
  },
  {
    "objectID": "11-density-model.html#using-the-model-argument",
    "href": "11-density-model.html#using-the-model-argument",
    "title": "11  Density model",
    "section": "\n11.3 Using the ‘model’ argument",
    "text": "11.3 Using the ‘model’ argument\nA model formula defines variation in each parameter as a function of covariates (including geographic coordinates and their polynomial terms) that is linear on the ‘link’ scale, as in a generalized linear model.\nThe options differ between the state and observation models. D may vary with respect to group, session or point in space\nThe predictors ‘group’ and ‘session’ behave for D as they do for other real parameters. They determine variation in the expected density for each group or session that is (by default) uniform across space, leading to a homogeneous Poisson model and a flat surface. No further explanation is therefore needed.\n\n11.3.1 Link function\n \nThe default link for D is ‘log’. It is equally feasible in most cases to choose ‘identity’ as the link (see the secr.fit argument ‘link’), and for the null model D \\sim 1 the estimate will be the same to numerical accuracy, as will estimates involving only categorical variables (e.g., session). However, with an ‘identity’ link the usual (asymptotic) confidence limits will be symmetrical (unless truncated at zero) rather than asymmetrical. In models with continuous predictors, including spatial trend surfaces, the link function will affect the result, although the difference may be small when the amplitude of variation on the surface is small. Otherwise, serious thought is needed regarding which model is biologically more appropriate: logarithmic or linear.\nThe ‘identity’ link may cause problems when density is very small or very large because, by default, the maximization method assumes all parameters have similar scale (e.g., typsize = c(1,1,1) for default constant models). Setting typsize manually in a call to secr.fit can fix the problem and speed up fitting. For example, if density is around 0.001/ha (10 per 100 km^2) then call secr.fit(..., typsize = c(0.001,1,1)) (typsize has one element for each beta parameter). Problems with the identity link mostly disappear when modelling relative density (CL = TRUE) because coefficients are automatically scaled by the intercept. See Appendix I for more on link functions.\nYou may wonder why secr.fit is ambivalent regarding the link function: link functions have seemed a necessary part of the machinery for capture–recapture modelling since Lebreton et al. (1992). Their key role is to keep the ‘real’ parameter within feasible bounds (e.g., 0-1 for probabilities). In secr.fit any modelled value of D that falls below zero is truncated at zero (of course this condition will not arise with a log link).\n\n11.3.2 Built-in variables\n\nsecr.fit automatically recognises the spatial variables x, y, x2, y2 and xy if they appear in the formula for D. These refer to the x-coordinate, y-coordinate, x-coordinate2 etc. for each mask point, and will be constructed automatically as needed. The built-in spatial variables offer limited model possibilities (Table 11.2).\nThe formula for D may also include the non-spatial variables g (group), session (categorical), and Session (continuous), defined as for modelling g0 and sigma in Chapter 10.\n\n\nTable 11.2: Examples of density models using built-in variables.\n\n\n\nFormula\nInterpretation\n\n\n\nD ~ 1\nflat surface (default)\n\n\nD ~ x + y\nlinear trend surface (planar)\n\n\nD ~ x + x2\nquadratic trend in east-west direction only\n\n\nD ~ x + y + x2 + y2 + xy\nquadratic trend surface\n\n\n\n\n\n\n\n\n11.3.3 User-provided variables\n\nMore interesting models can be made with variables provided by the user. These are stored in a data frame as the ‘covariates’ attribute of a mask object. Covariates must be defined for every point on a mask.\nVariables may be categorical (a factor or character value that can be coerced to a factor) or continuous (a numeric vector). The habitat variable ‘habclass’ constructed in the Examples section of the skink help is an example of a two-class categorical covariate. Remember that categorical variables entail one additional parameter for each extra level.\nThere are several ways to create or input mask covariates.\n\nRead columns of covariates along with the x- and y-coordinates when creating a mask from a dataframe or external file (read.mask)\nRead the covariates dataframe separately from an external file (read.table)\nInfer covariate values by computation on in existing mask (see below).\nInfer values for points on an existing mask from a GIS data source, such as a polygon shapefile or other spatial data source (see Appendix C).\n\nUse the function addCovariates for the third and fourth options.\n\n11.3.4 Covariates computed from coordinates\nHigher-order polynomial terms may be added as covariates if required. For example,\n\ncovariates(ovmask)[,\"x3\"] &lt;- covariates(ovmask)$x^3 \n\nallows a model like D ~ x + x2 + x3.\nIf you have a strong prior reason to suspect a particular ‘grain’ to the landscape then this may be also be computed as a new, artificial covariate. This code gives a covariate representing a northwest – southeast trend:\n\ncovariates(ovmask)[,\"NWSE\"] &lt;- ovmask$y - ovmask$x - \n    mean(ovmask$y - ovmask$x)\n\nAnother trick is to compute distances to a mapped landscape feature. For example, possum density in our Orongorongo example may relate to distance from the river; this corresponds roughly to elevation, which we do not have to hand. The distancetotrap function of secr computes distances from mask cells to the nearest vertex on the riverbank, which are precise enough for our purpose.\n\ncovariates(ovmask)[,\"DTR\"] &lt;- distancetotrap(ovmask, leftbank)\n\n\npar(mar = c(1,6,2,8))\nplot(ovmask, covariate = \"DTR\", breaks = seq(0,500,50), \n     title = \"Distance to river m\", dots = FALSE, inset= 0.07)\n\n\n\n\n\n\nFigure 11.3: Orongorongo Valley possum study: distance to river\n\n\n\n\n\n11.3.5 Pre-computed resource selection functions\n\nA resource selection function (RSF) was defined by Boyce et al. (2002) as “any model that yields values proportional to the probability of use of a resource unit”. An RSF combines habitat information from multiple sources in a single variable. Typically the function is estimated from telemetry data on marked individuals, and primarily describes individual-level behaviour (3rd-order habitat selection of Johnson, 1980).\nHowever, the individual-level RSF is also a plausible hypothesis for 2nd-order habitat selection i.e. for modelling the relationship between habitat and population density. Then we interpret the RSF as a single variable that is believed to be proportional to the expected population density in each cell of a habitat mask.\nSuppose, for example, in folder datadir we have a polygon shapefile (RSF.shp, RSF.dbf etc.) with the attribute “rsf” defined for each polygon. Given mask and capthist objects “habmask” and “myCH”, this code fits a SECR model that calibrates the RSF in terms of population density:\n\nrsfshape &lt;- sf::st_read(paste0(datadir, \"/RSF.shp\"))\nhabmask &lt;- addCovariates(habmask, rsfshape, columns = \"rsf\")\nsecr.fit (myCH, mask = habmask, model = D ~ rsf - 1)\n\n\n“rsf” must be known for every pixel in the habitat mask\nUsually it make sense to fit the density model through the origin (rsf = 0 implies D = 0). This is not true of habitat suitability indices in general.\n\nThis is a quite different approach to fitting multiple habitat covariates within secr, and one that should be considered. There are usually too few individuals in a SECR study to usefully fit models with multiple covariates of density, even given a large dataset such as our possum example. However, 3rd-order and 2nd-order habitat selection are conceptually distinct, and their relationship is an interesting research topic.\n\n11.3.6 Regression splines\n\nRegression splines are a flexible alternative to polynomials for spatial trend analysis. Regression splines are familiar as the smooth terms in ‘generalized additive models’ (gams) implemented (differently) in the base R package gam and in R package mgcv (Wood, 2006).\nSome of the possible smooth terms from mgcv can be used in model formulae for secr.fit – see the help page for ‘smooths’ in secr. Smooths are specified with terms that look like calls to the functions s and te. Smoothness is determined by the number of knots which is set by the user via the argument ‘k’. The number of knots cannot be determined automatically by the penalty algorithms of mgcv.\nHere we fit a regression spline with the same number of parameters as a quadratic polynomial, a linear effect of the ‘distance to river’ covariate on log(D), and a nonlinear smooth.\n\nargs &lt;- list(capthist = OVpossumCH[[1]], mask = ovmask, trace = \n    FALSE)\nmodels &lt;- list(D ~ s(x,y, k = 6), D ~ DTR, D ~ s(DTR, k = 3))\nRSfits &lt;- list.secr.fit(model = models, constant = args, \n    prefix = \"RS\")\n\nNow add these to the AIC table and plot the ‘AIC-best’ model:\n\nAIC(c(fits, RSfits))[,-c(1,2,5,6)]\n\n        npar   logLik  dAIC  AICwt\nRS3        5 -1552.00 0.000 0.2667\nDxy2       8 -1549.32 0.628 0.1948\nRS2        4 -1553.36 0.705 0.1875\nDxy        5 -1552.86 1.714 0.1132\nRS1        8 -1549.93 1.847 0.1059\nDforest    4 -1554.15 2.281 0.0853\nnull       3 -1555.75 3.488 0.0466\n\n\n\nnewdat &lt;- data.frame(DTR = seq(0,400,5))\ntmp &lt;- predict(RSfits$RS3, newdata = newdat)\npar(mar=c(5,8,2,4), pty = \"s\")\nplot(seq(0,400,5), sapply(tmp, \"[\", \"D\",\"estimate\"), \n    ylim = c(0,20), xlab = \"Distance from river (m)\", \n    ylab = \"Density / ha\", type = \"l\")\n\n\n\n\n\n\nFigure 11.4: Possum density vs distance to river: regression spline k = 3\n\n\n\n\nConfidence intervals are computed in predictDsurface by back-transforming \\pm 2SE from the link (log) scale:\n\ncode for lower and upper confidence surfacespar(mar = c(1,1,1,1), mfrow = c(1,2), xpd = FALSE)\nsurfaceDDTR3 &lt;- predictDsurface(RSfits$RS3, cl.D = TRUE)\nplot(surfaceDDTR3, covariate= \"lcl\", breaks = seq(0,22,2), \n    legend = FALSE)\nmtext(side = 3,line = -1.5, cex = 0.8,\n      \"Lower 95% confidence limit of D (possums / ha)\")\nplot(surfaceDDTR3, plottype = \"contour\", breaks = seq(0,22,2), \n    add = TRUE)\nlines(leftbank)\nplot(surfaceDDTR3, covariate= \"ucl\", breaks = seq(0,22,2), \n    legend = FALSE)\nmtext(side = 3, line = -1.5, cex = 0.8,\n    \"Upper 95% confidence limit of D (possums / ha)\")\nplot(surfaceDDTR3, covariate = \"ucl\", plottype = \"contour\", \n    breaks = seq(0,22,2), add = TRUE)\nlines(leftbank)\nmtext(side=3, line=-1, outer=TRUE, \"s(DTR, k = 3) model\", cex = 0.9)\n\n\n\n\n\n\nFigure 11.5: Confidence surfaces\n\n\n\n\n\nMultiple predictors may be included in one ‘s’ smooth term, implying interaction. This assumes isotropy – equality of scales on the different predictors – which is appropriate for geographic coordinates such as x and y in this example. In other cases, predictors may be measured on different scales (e.g., structural complexity of vegetation and elevation) and isotropy cannot be assumed. In these cases a tensor-product smooth (te) is appropriate because it is scale-invariant. For te, ‘k’ represents the order of the smooth on each axis, and we must fix the number of knots with ‘fx = TRUE’ to override automatic selection.\nFor more on the use of regression splines see the documentation for mgcv, the secr help page `?smooths’, Wood (2006), and Borchers & Kidney (2014).\n\n11.3.7 Scale of effect\n\nModelling density as a function of covariate(s) at a point (the centroid of a mask cell) lacks biological realism. Individuals exploit resources across their home range, and density may be affected by regional dynamics over a much wider area (e.g., Jackson & Fahrig, 2014).\nThe scale at which environmental predictors influence density is usually unknown, and almost certainly does not correspond to either a point or the arbitrary size of a mask cell. Some progress has been made in estimating the scale from data (e.g., Chandler & Hepinstall-Cymerman, 2016), but methods have not yet been integrated into SECR.\nThe best we can do at present is to construct spatial layers in GIS software that represent various levels of smoothing or spatial aggregation, corresponding to different scales of effect. Each layer may then be imported as a mask covariate that is evaluated at the cell centroids to represent the surrounding area.\nThis example uses the focal function in the terra package (Hijmans, 2023) to compute both the proportion of forest cells in a square window and a Gaussian-smoothed proportion.\n\ncode for smoothed covariates# binary SpatRaster 0 = nonbeech, 1 = beech\ncovariates(ovmask)$forest_num &lt;- ifelse (covariates(ovmask)$forest == 'beech', 1,0)\ntmp &lt;- terra::rast(ovmask, covariate = 'forest_num')\n\n# square smoothing window: 5 cells\ntmpw5 &lt;- terra::focal(tmp, w = 5, \"mean\", na.policy = \"omit\", na.rm = TRUE)\nnames(tmpw5) &lt;- 'w5'\n\n# Gaussian window, sigma 50 m\n# window radius is 3 sigma\ng50 &lt;- terra::focalMat(tmp, d = 50, type = \"Gauss\")\ntmpg50 &lt;- terra::focal(tmp, w = g50, \"sum\", na.policy = \"omit\", na.rm = TRUE)\nnames(tmpg50) &lt;- 'g50'\n\n# add to mask\novmask &lt;- addCovariates(ovmask, tmpw5)\novmask &lt;- addCovariates(ovmask, tmpg50)\n\n# plot\npar(mar = c(1,1,1,1), mfrow = c(1,2), xpd = FALSE)\n\nplot(ovmask, cov = 'w5', dots = FALSE, legend = FALSE)\nplot(ovtrap, add = TRUE)\ntext (2674600, 5982892, 'a.', cex = 1.2)\n\nplot(ovmask, cov = 'g50', dots = FALSE, legend = FALSE)\nplot(ovtrap, add = TRUE)\ntext (2674600, 5982892, 'b.', cex = 1.2)\n\n\n\n\n\n\nFigure 11.6: Smoothed forest covariate (proportion ‘beech’: 0% dark green, 100% white). (a) square window (5 \\times 5 cells), (b) Gaussian smooth (\\sigma = 50 m).",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Density model</span>"
    ]
  },
  {
    "objectID": "11-density-model.html#prediction-and-plotting",
    "href": "11-density-model.html#prediction-and-plotting",
    "title": "11  Density model",
    "section": "\n11.4 Prediction and plotting",
    "text": "11.4 Prediction and plotting\n \nFitting a model provides estimates of its coefficients or ‘beta parameters’; use the coef method to extract these from an secr object. The coefficients are usually of little use in themselves, but we can use them to make predictions. In order to plot a fitted model we first predict the height of the density surface at each point on a mask. As we have seen, this is done with predictDsurface, which has arguments (object, mask = NULL, se.D = FALSE, cl.D = FALSE, alpha = 0.05). By default, prediction is at the mask points used when fitting the model (i.e. object$mask); specify the mask argument to extrapolate the model to a different area.\nThe output from predictDsurface is a specialised mask object called a Dsurface (class “c(‘Dsurface’, ‘mask’, ‘data.frame’)”). The covariate dataframe of a Dsurface has columns for the predicted density of each group (D.0 if there is only one). Usually when you print a mask you see only the x- and y-coordinates. The print method for Dsurface objects displays both the coordinates and the density values as one dataframe, as also do the head and tail methods.\nUse the arguments ‘se.D’ and ‘cl.D’ to request computation of the estimated standard error and/or upper and lower confidence limits for each mask point3. If requested, values are saved as additional covariates of the output Dsurface (SE.0, lcl.0, and ucl.0 if there is only one group).\nThe plot method for a Dsurface object has arguments (x, covariate = \"D\", group = NULL, plottype = \"shaded\", scale = 1, ...). covariate may either be a prefix (one of “D”, “SE”, “lcl”, “ucl”) or any full covariate name. ‘plottype’ may be one of “shaded”, “dots”, “persp”, or “contour”. A coloured legend is displayed centre-right (see ?plot.mask and ?strip.legend for options).\nFor details on how to specify colours, levels etc. read the help pages for plot.mask, contour and persp (these functions may be controlled by extra arguments to plot.Dsurface, using the ‘dots’ convention).\nA plot may be enhanced by the addition of contours. This is a challenge, because the contour function in R requires a rectangular matrix of values, and our mask is not rectangular. We could make it so with the secr function rectangularMask, which makes a rectangular Dsurface with missing (NA) values of density at all the external points. plot.Dsurface recognises an irregular mask and attempts to fix this with an internal call to rectangularMask.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Density model</span>"
    ]
  },
  {
    "objectID": "11-density-model.html#sec-scaling",
    "href": "11-density-model.html#sec-scaling",
    "title": "11  Density model",
    "section": "\n11.5 Scaling",
    "text": "11.5 Scaling\n\nSo far we have ignored the scaling of covariates, including geographic coordinates.\nsecr.fit scales the x- and y-coordinates of mask points to mean = 0, SD = 1 before using the coordinates in a model. Remember this when you come to use the coefficients of a density model. Functions such as predictDsurface take care of scaling automatically. predict.secr uses the scaled values (‘newdata’ x = 0, y = 0), which provides the predicted density at the mask centroid. The mean and SD used in scaling are those saved as the `meanSD’ attribute of a mask (dataframe with columns ‘x’ and ‘y’ and rows ‘mean’ and ‘SD’).\nScaling of covariates other than x and y is up to the user. It is not usually needed.\nThe numerical algorithms for maximizing the likelihood work best when the absolute expected values are roughly similar for all parameters on their respective ‘link’ scales (i.e. all beta parameters) rather than varying by orders of magnitude. The default link function for D and sigma (log) places the values of these parameters on a scale that is not wildly different to the variation in g0 or lambda0, so this is seldom an issue. In extreme cases you may want to make allowance by setting the typsize argument of nlm or the parscale control argument of optim (via the … argument of secr.fit).\nScaling is not performed routinely by secr.fit for distance calculations. Sometimes, large numeric values in coordinates can cause loss of precision in distance calculations (there are a lot of them at each likelihood evaluation). The problem is serious in datasets that combine large coordinates with small detector spacing, such as the Lake Station skink dataset. Set details = list(centred = TRUE) to force scaling; this may become the default setting in a future version of secr.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Density model</span>"
    ]
  },
  {
    "objectID": "11-density-model.html#sec-notadensitysurface",
    "href": "11-density-model.html#sec-notadensitysurface",
    "title": "11  Density model",
    "section": "\n11.6 This is not a density surface",
    "text": "11.6 This is not a density surface\n\nThe surfaces we have fitted involve inhomogeneous Poisson models for the distribution of animal home range centres. The models have parameters that determine the relationship of expected density to location or to habitat covariates.\nAnother type of plot is sometimes presented and described as a ‘density surface’ – the summed posterior distribution of estimated range centres from a Bayesian fit of a homogeneous Poisson model. A directly analogous plot may be obtained from the secr function fxTotal (see also Borchers & Efford (2008) Section 4.3). The contours associated with the home range centre of each detected individual essentially represent 2-D confidence intervals for its home range centre, given the fitted observation model. Summing these gives a summed probability density surface for the centres of the observed individuals (‘D.fx’), and to this we can add an equivalent scaled probability density surface for the individuals that escaped detection (‘D.nc’). Both components are reported by fx.total, along with their sum (‘D.sum’) which we plot here for the flat possum model:\n\nfxsurface &lt;- fxTotal(fits$null)\n\n\npar(mar = c(1,6,2,8))\nplot(fxsurface, covariate = \"D.sum\", breaks = seq(0,30,2), \n     poly = FALSE)\nplot(ovtrap, add = TRUE)\n\n\n\n\n\n\nFigure 11.7: Total fx surface\n\n\n\n\nThe plot concerns only one realisation from the underlying Poisson model. It visually invites us to interpret patterns in that realisation that we have not modelled. There are serious problems with the interpretation of such plots as ‘density surfaces’:\n\nattention is focussed on the individuals that were detected; others that were present but not detected are represented by a smoothly varying base level that dominates in the outer region of the plot (contrast this figure with the previous quadratic and DTR3 models).\nthe surface depends on sampling intensity, and as more data are added it will change shape systematically. Ultimately, the surface near the centre of a detector array becomes a set of emergent peaks rising from an underwater plain of zero density, below the plateau of average density outside the array.\nthe ‘summed confidence interval’ plot is easily confused with the 2-D surface obtained by summing utilisation distributions across animals\nconfidence intervals are not available for the height of the probability density surface.\n\nThe plots are also prone to artefacts. In some examples we see concentric clustering of estimated centres around the trapping grids, apparently ‘repelled’ from the traps themselves (e.g., plot below for a null model of the Waitarere ‘possumCH’ dataset in secr). This phenomenon appears to relate to lack of model fit (unpubl. results).\n\nfxsurfaceW &lt;- fxTotal(possum.model.0)\n\n\npar(mar = c(1,5,1,8))\nplot(fxsurfaceW, covariate = \"D.sum\", breaks = seq(0,5,0.5), \n     poly = FALSE)\nplot(traps(possumCH), add = TRUE)\n\n\n\n\n\n\nFigure 11.8: Waitarere possum fx surface\n\n\n\n\nSee Durbach et al. (2024) for further critique.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Density model</span>"
    ]
  },
  {
    "objectID": "11-density-model.html#sec-relativedensity2",
    "href": "11-density-model.html#sec-relativedensity2",
    "title": "11  Density model",
    "section": "\n11.7 Relative density",
    "text": "11.7 Relative density\n \nTheory for relative density models was given earlier (see also (Efford, 2025)). A spatial model for relative density is fitted in secr by setting CL = TRUE and providing a model for D in the call to secr.fit (secr \\ge 5.2.0). For example\n\nfitrd1 &lt;- secr.fit(capthist = OVpossumCH[[1]], mask = ovmask, \n    trace = FALSE, model = D ~ x + y + x2 + y2 + xy, CL = TRUE)\noptions(digits = 4)\ncoef(fitrd1)[,1:2]\n\n          beta SE.beta\nD.x   -0.19871 0.11851\nD.y    0.30998 0.13469\nD.x2  -0.22472 0.13345\nD.y2  -0.09673 0.12818\nD.xy   0.29672 0.13208\ng0    -2.22032 0.09422\nsigma  3.31767 0.03544\n\n# compare coefficients from full fit:\ncoef(fits$Dxy2)[1:2]\n\n          beta SE.beta\nD      2.72538 0.12781\nD.x   -0.19871 0.11851\nD.y    0.30999 0.13468\nD.x2  -0.22471 0.13344\nD.y2  -0.09674 0.12817\nD.xy   0.29672 0.13208\ng0    -2.22032 0.09422\nsigma  3.31767 0.03544\n\n\nThe relative density model is fitted by maximizing the likelihood conditional on n and has one fewer coefficients than the absolute density model. Estimates of other coefficients are the same within numerical error (log link) or are scaled by the intercept (identity link).\nThe density intercept and other coefficients may be retrieved with the function derivedDcoef:\n\nderivedDcoef(fitrd1) # delta-method variance suppressed as unreliable\n\n            beta   SE.beta        lcl        ucl\nD      2.7253890        NA         NA         NA\nD.x   -0.1987149 0.1185098 -0.4309898  0.0335599\nD.y    0.3099849 0.1346873  0.0460026  0.5739672\nD.x2  -0.2247201 0.1334477 -0.4862729  0.0368326\nD.y2  -0.0967319 0.1281839 -0.3479677  0.1545040\nD.xy   0.2967233 0.1320827  0.0378459  0.5556006\ng0    -2.2203193 0.0942197 -2.4049865 -2.0356520\nsigma  3.3176747 0.0354359  3.2482216  3.3871277\n\n\nTo plot the full density surface it is first necessary to infer the missing intercept. This is done automatically by the function derivedDsurface:\n\npar(mar = c(1,6,2,8))\nderivedD &lt;- derivedDsurface(fitrd1)\nplot(derivedD, plottype = \"shaded\", poly = FALSE, breaks = \n      seq(0,22,2), title = \"Density / ha\", text.cex = 1)\nplot(surfaceDxy2, plottype = \"contour\", poly = FALSE, breaks = \n    seq(0,22,2), add = TRUE)\nlines(leftbank)\n\n\n\n\n\n\nFigure 11.9: Possum density derived from quadratic relative density surface. Contour lines are from the previous full fit.\n\n\n\n\nThe density in Fig. 11.9 nearly matches the absolute density in Fig. 11.2. This will not be the case if the relative density model is fitted to data from animals tagged elsewhere or on a subset of the area. Tagging then imposes differential spatial weighting that must be made explicit in the model to recover the correct pattern of density in relation to covariates. One scenario involves acoustic telemetry or other automated detection for which the only animals at risk of detection are those previously marked (cf resighting data, in which unmarked animals are detected and counted, but not identified).\n\n\n\nFigure 11.1: Orongorongo Valley possum study area\nFigure 11.2: Quadratic possum density surface\nFigure 11.3: Orongorongo Valley possum study: distance to river\nFigure 11.4: Possum density vs distance to river: regression spline k = 3\nFigure 11.5: Confidence surfaces\nFigure 11.6: Smoothed forest covariate (proportion ‘beech’: 0% dark green, 100% white). (a) square window (5 \\times 5 cells), (b) Gaussian smooth (\\sigma = 50 m).\nFigure 11.7: Total fx surface\nFigure 11.8: Waitarere possum fx surface\nFigure 11.9: Possum density derived from quadratic relative density surface. Contour lines are from the previous full fit.\n\n\n\nBorchers, D. L., & Efford, M. G. (2008). Spatially explicit maximum likelihood methods for capture-recapture studies. Biometrics, 64, 377–385.\n\n\nBorchers, D. L., & Kidney, D. (2014). Flexible density surface estimation for spatially explicit capture–recapture surveys. University of St Andrews. https://research-repository.st-andrews.ac.uk/bitstream/handle/10023/9147/secrgam_techrep.pdf?sequence=1\n\n\nBoyce, M. S., Vernier, P. R., Nielsen, S. E., & Schmiegelow, F. K. A. (2002). Evaluating resource selection functions. Ecological Modelling, 157, 281–300.\n\n\nChandler, R., & Hepinstall-Cymerman, J. (2016). Estimating the spatial scales of landscape effects on abundance. Landscape Ecology, 31, 1383–1394. https://doi.org/10.1007/s10980-016-0380-z\n\n\nDurbach, I., Chopara, R., Borchers, D. L., Phillip, R., Sharma, K., & Stevenson, B. C. (2024). That’s not the mona lisa! How to interpret spatial capture-recapture density surface estimates. Biometrics, 80. https://doi.org/10.1093/biomtc/ujad020\n\n\nEfford, M. G. (2025). Spatially explicit capture–recapture models for relative density. BioRxiv. https://doi.org/10.1101/2025.01.22.634401\n\n\nEfford, M. G., & Cowan, P. E. (2004). Long-term population trend of trichosurus vulpecula in the orongorongo valley, new zealand. In R. L. Goldingay & S. M. Jackson (Eds.), The biology of australian possums and gliders (pp. 471–483). Surrey Beatty & Sons.\n\n\nHijmans, R. J. (2023). Terra: Spatial data analysis. https://CRAN.R-project.org/package=terra\n\n\nJackson, H. B., & Fahrig, L. (2014). Are ecologists conducting research at the optimal scale? Global Ecology and Biogeography, 24, 52–63. https://doi.org/10.1111/geb.12233\n\n\nJohnson, D. H. (1980). The comparison of usage and availability measurements for evaluating resource preference. Ecology, 61, 65–71.\n\n\nKendeigh, S. C. (1944). Measurement of bird populations. Ecological Monographs, 14, 67–106. https://doi.org/10.2307/1961632\n\n\nWood, S. N. (2006). Generalized additive models: An introduction with R. Chapman; Hall/CRC.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Density model</span>"
    ]
  },
  {
    "objectID": "11-density-model.html#footnotes",
    "href": "11-density-model.html#footnotes",
    "title": "11  Density model",
    "section": "",
    "text": "We can also express the model as before \\mathbf y = \\mathbf X \\pmb {\\beta}, where \\mathbf X is the design matrix, \\pmb{\\beta} is a vector of coefficients, and \\mathbf y is the resulting vector of densities on the link scale. Rows of \\mathbf X and elements of \\mathbf y correspond to points on the habitat mask, possibly replicated in the case of group and session effects.↩︎\nIt may also be specified in a user-written function supplied to secr.fit (see Appendix I), but you are unlikely to need this. ↩︎\nOption available only for models specified in generalized linear model form with the ‘model’ argument of secr.fit, not for user-defined functions.↩︎",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Density model</span>"
    ]
  },
  {
    "objectID": "12-habitat.html",
    "href": "12-habitat.html",
    "title": "12  Habitat mask",
    "section": "",
    "text": "12.1 Background\nWe start with an intuitive explanation of the need for habitat masks. Devices such as traps or cameras record animals moving in a general region. If the devices span a patch of habitat with a known boundary then we use a mask to define that geographical unit. More commonly, detectors are placed in continuous habitat and the boundary of the region sampled is ill-defined. This is because the probability of detecting an animal tapers off gradually with distance.\nVagueness regarding the region sampled is addressed in spatially explicit capture–recapture by considering a larger and more inclusive region, the habitat mask. Its extent is not critical, except that it should be at least large enough to account for all detected animals.\nNext we refine this intuitive explanation for each of the dominant methods for fitting SECR models: maximum likelihood and Markov-chain Monte Carlo (MCMC) sampling. Each grapples in a slightly different way with the awkward fact that, although we wish to model detection as a function of distance from the activity centre, the activity centre of each animal is at an unknown location.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Habitat mask</span>"
    ]
  },
  {
    "objectID": "12-habitat.html#background",
    "href": "12-habitat.html#background",
    "title": "12  Habitat mask",
    "section": "",
    "text": "12.1.1 Maximum likelihood and the area of integration\nThe likelihood developed for SECR by Borchers & Efford (2008) allows for the unknown centres by numerically integrating them out of the likelihood (crudely, by summing over all possible locations of detected animals, weighting each by a detection probability). Although the integration might, in principle, have infinite spatial bounds, it is practical to restrict attention to a smaller region, the ‘area of integration’. As long as the probability weights get close to zero before we reach the boundary, we don’t need to worry too much about the size of the region.\nIn secr the habitat mask equates to the area of integration: the likelihood is evaluated by summing values across a fine mesh of points. This is the primary function of the habitat mask; we consider other functions in Section 12.2.\n\n12.1.2 MCMC and the Bayesian ‘state space’\n\nMCMC methods for spatial capture–recapture developed by Royle and coworkers (Royle et al., 2014) take a slightly different tack. The activity centres are treated as a large number of unobserved (latent) variables. The MCMC algorithm ‘samples’ from the posterior distribution of location for each animal, whether detected or not. The term ‘state space’ is used for the set of permitted locations; usually this is a continuous (not discretized) rectangular region.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Habitat mask</span>"
    ]
  },
  {
    "objectID": "12-habitat.html#sec-maskwhatfor",
    "href": "12-habitat.html#sec-maskwhatfor",
    "title": "12  Habitat mask",
    "section": "\n12.2 What is a mask for?",
    "text": "12.2 What is a mask for?\nMasks serve multiple purposes in addition to the basic one we have just introduced. We distinguish five functions of a habitat mask and there may be more:\n\nTo define the outer limit of the area of integration. Habitat beyond the mask may be occupied, but animals centred there have negligible chance of being detected.\nTo facilitate computation. By defining the area of integration as a list of discrete points (the centres of grid cells, each with notionally uniform density) we transform the relatively messy task of numerical integration into the much simpler one of summation.\nTo distinguish habitat sites from non-habitat sites within the outer limit. Habitat cells have the potential to be occupied. Treating non-habitat as if it is habitat can cause habitat-specific density to be underestimated.\nTo store habitat covariates for spatial models of density. Covariates for modelling a density surface are provided for each point on the mask.\nTo define a region for which a post-hoc estimate of population size is required. This may differ from the mask used to fit the model.\n\nThe first point raises the question of where the outer limit should lie (i.e., the buffer width), and the second raises the question of how coarse the discretization (i.e., the cell size) can be without damaging the estimates. Later sections address each of the five points in turn, after an introductory section describing the particular implementation of habitat masks in the R package secr.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Habitat mask</span>"
    ]
  },
  {
    "objectID": "12-habitat.html#masks-in-secr",
    "href": "12-habitat.html#masks-in-secr",
    "title": "12  Habitat mask",
    "section": "\n12.3 Masks in secr\n",
    "text": "12.3 Masks in secr\n\nA habitat mask is represented in secr by a set of square grid cells. Their combined area may be almost any shape and may include holes. An object of class ‘mask’ is a 2-column dataframe with additional attributes (cell area etc.); each row gives the x- and y-coordinates of the centre of one cell.\n\n12.3.1 Masks generated automatically by secr.fit\n\nA mask is used whenever a model is fitted with the function secr.fit, even if none is specified in the ‘mask’ argument. When no mask is provided, one is constructed automatically using the value of the ‘buffer’ argument. For example\n\nlibrary(secr, quietly = TRUE)\nfit &lt;- secr.fit(captdata, buffer = 80, trace = FALSE)\n\nThe mask is saved as a component of the fitted model (‘secr’ object); we can plot it and overlay the traps:\n\ncode to plot automatically generated maskpar(mar = c(1,1,1,1))\nplot(fit$mask, dots = FALSE, mesh = \"grey\", col = \"white\")\nplot(traps(captdata), detpar = list(pch = 16, cex = 1), add = TRUE)\n\n\n\n\n\n\nFigure 12.1: Mask (grey grid) generated automatically in secr.fit by buffering around the detectors (red dots) (80-m buffer, 30-m detector spacing)\n\n\n\n\nThe mask is generated by forming a grid that extends ‘buffer’ metres north, south, east and west of the detectors and dropping centroids that are more than ‘buffer’ metres from the nearest detector (hence the rounded corners). The obvious question “How wide should the buffer be?” is addressed in a later section. The spacing of mask points (i.e. width of grid cells) is set arbitrarily to 1/64th of the east-west dimension - in this example the spacing is 6.7 metres.\n\n12.3.2 Masks constructed with make.mask\n\nA mask may also be prepared in advance and provided to secr.fit in the ‘mask’ argument. This overrides the automatic process of the preceding section, and the value of ‘buffer’ is discarded. The function make.mask provides precise control over the the size of the cells, the extent of the mask, and much more. We introduce make.mask here with a simple example based on a ‘hollow grid’:\n\nhollowgrid &lt;- make.grid(nx = 10, ny = 10, spacing = 30, hollow = TRUE)\nhollowmask &lt;- make.mask(hollowgrid, buffer = 80, spacing = 15, type = \"trapbuffer\")\n\n\ncode to plot mask from make.maskpar(mar = c(1,1,1,1))\nplot(hollowmask, dots = FALSE, mesh = \"grey\", col = \"white\")\nplot(hollowgrid, detpar = list(pch = 16, cex = 1), add = TRUE)\n\n\n\n\n\n\nFigure 12.2: Mask (grey grid) generated with make.mask (80-m buffer, 30-m trap spacing, 15-m mask spacing). Grid cells in the centre were dropped because they were more than 80 m from any trap.\n\n\n\n\nWe chose a coarser grid (spacing 15 metres) relative to the trap spacing. This, combined with the hole in the centre, results in a mask with many fewer rows (764 rows compared to 3980). Setting the type to “trapbuffer” trims grid cells from the corners and the centre.\nIf we collected data hollowCH with the hollow grid we could fit a SECR model using hollowmask. For illustration we simulate some data using default settings in sim.capthist (5 occasions, D = 5/ha, g0 = 0.2, sigma = 25 m).\n\nhollowCH &lt;- sim.capthist(hollowgrid, seed = 123)\nfit2 &lt;- secr.fit(hollowCH, mask = hollowmask, trace = FALSE)\npredict(fit2)\n\n       link estimate SE.estimate      lcl      ucl\nD       log   5.0807    1.078535  3.36683  7.66711\ng0    logit   0.2047    0.044309  0.13117  0.30497\nsigma   log  26.1801    3.241670 20.55772 33.34021\n\n\nFitting is fast because there are few traps and few mask points. As before, the mask is retained in the output, so –\n\ncat(\"Number of rows in hollow mask =\", nrow(fit2$mask), \"\\n\")\n\nNumber of rows in hollow mask = 764",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Habitat mask</span>"
    ]
  },
  {
    "objectID": "12-habitat.html#sec-bufferwidth",
    "href": "12-habitat.html#sec-bufferwidth",
    "title": "12  Habitat mask",
    "section": "\n12.4 How wide should the buffer be?",
    "text": "12.4 How wide should the buffer be?\n\nThe general answer is ‘Wide enough that any bias in estimated densities is negligible’. Excessive truncation of the mask results in positive bias that depends on the sampling regime (detector layout and sampling duration) and the detection function, particularly its spatial scale and shape.\nThe penalty for using an over-wide buffer is that fitting will be slower for a given mask spacing. It is usually smart to accept this penalty rather than search for the narrowest acceptable buffer.\nTwo factors are critical when selecting a buffer width –\n\nThe spatial scale of detection, which is usually a function of home-range movements.\nThe shape of the detection function, particularly the length of its tail.\n\nThese must be considered together. The following comments assume the default half-normal detection function, which has a short tail and spatial scale parameter \\sigma_{HN}, unless stated otherwise.\n\n12.4.1 A rule of thumb for buffer width\nAs a rule of thumb, a buffer of 4\\sigma_{HN} is likely to be adequate (result in truncation bias of less than 0.1%). A pilot estimate of \\sigma_{HN} may be found for a particular dataset (capthist object) with the function RPSV with the argument ‘CC’ set to TRUE:\n\nRPSV(captdata, CC = TRUE)\n\n[1] 25.629\n\n\nThis is an approximation based on a circular bivariate normal distribution that ignores the truncation of recaptures due to the finite extent of the detector array (Calhoun & Casby, 1958).\n\n12.4.2 Buffer width for heavy-tailed detection functions\nHeavy-tailed detection functions such as the hazard-rate (HR, HHR) can be problematic because they require an unreasonably large buffer for stable density estimates. They are better avoided unless there is a natural boundary.\n\n12.4.3 Hands-free buffer selection: suggest.buffer\n\nThe suggest.buffer function is an alternative to the 4\\sigma_{HN} rule of thumb for data from point detectors (not polygon or transect detectors). It has the advantage of allowing for the geometry of the detector array (specifically, the length of edge) and the duration of sampling. The algorithm is obscure and undocumented (this is only a suggestion!) and uses an approximation to the bias, computed by function bias.D. The first argument of suggest.buffer may be a capthist object or a fitted model. With a capthist object as input:\n\nsuggest.buffer(captdata, detectfn = 'HN', RBtarget = 0.001)\n\nWarning: using automatic 'detectpar' g0 = 0.2339, sigma = 30.75\n\n\n[1] 105\n\n\nWhen the input is only a capthist object, the suggested buffer width relies on an estimate of \\sigma_{HN} that is itself biased (RPSV(captdata, CC=TRUE)). Section 12.4.4 shows how the suggested buffer width changes when we use a better estimate of \\sigma_{HN}. Actual bias due to mask truncation will also exceed the target (RB = 0.1%) due to limitations of the ad hoc algorithm in bias.D.\n\n12.4.4 Retrospective buffer checks\nOnce a model has been fitted with a particular buffer width or mask, the estimated detection parameters may be used to check whether the buffer width is likely to have resulted in mask truncation bias. We highlight two of these:\n\n\n\nsecr.fit automatically checks a mask generated from its ‘buffer’ argument (i.e. when the ‘mask’ argument is missing), using bias.D as in suggest.buffer. A warning is given when the predicted truncation bias exceeds a threshold. The threshold is controlled by the ‘biasLimit’ argument (default 0.01). To suppress the check set ‘biasLimit = NA’, or provide a pre-defined mask.\n\nThe check is performed by a cunning custom algorithm in function bias.D. This uses one-dimensional numerical integration of a polar approximation to site-specific detection probability, coupled to a 3-part linear approximation for the length of contours of distance-to-nearest-detector. The check cannot be performed for some detector types, and the embedded integration can give rise to cryptic error messages.\n\n\n\nesaPlot provides a quick visualisation of the change in estimated density as buffer width changes. It is a handy check on any fitted model, and may also be used with pilot parameter values. The name of the function derives from its reliance on calculation of the effective sampling area (esa or a(\\hat \\theta)).\n\n\nfit &lt;- secr.fit(captdata, buffer = 100, trace = FALSE)\n\n\npar(pty = \"s\", mar = c(4,4,2,2), mgp = c(2.5,0.8,0), las = 1)\nesaPlot(fit, ylim = c(0,10))\nabline(v = 4 * 25.6, col = \"red\", lty = 2)\n\n\n\n\n\n\nFigure 12.3: Effect of varying buffer width on estimated density (y-axis). Vertical line indicates rule-of-thumb buffer width.\n\n\n\n\nThe esa plot supports the prediction that increasing buffer width beyond the rule-of-thumb value has no discernable effect on the estimated density (Fig. 12.3).\nThe function mask.check examines the effect of buffer width and mask spacing (cell size) by computing the likelihood or re-fitting an entire model. The function generates either the log likelihood or the estimated density for each cell in a matrix where rows correspond to different buffer widths and columns correspond to different mask spacings. The function is limited to single-session models and is slow compared to esaPlot. See ?mask.check for more.\nNote also that suggest.buffer may be used retrospectively (with a fitted model as input), and\n\nsuggest.buffer(fit)\n\n[1] 100\n\n\nwhich is coincidental, but encouraging!\n\n12.4.5 Buffer using non-Euclidean distances\nIf you intend to use a non-Euclidean distance metric then it makes sense to use this also when defining the mask, specifically to drop mask points that are distant from any detector according to the metric. See Appendix F for an example. Modelling with non-Euclidean distances also requires the user to provide secr.fit with a matrix of user-computed distances between detectors and mask points.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Habitat mask</span>"
    ]
  },
  {
    "objectID": "12-habitat.html#sec-maskspacing",
    "href": "12-habitat.html#sec-maskspacing",
    "title": "12  Habitat mask",
    "section": "\n12.5 Grid cell size",
    "text": "12.5 Grid cell size\n \nUsing a set of discrete locations (mask points) to represent the locations of animals is numerically convenient, and by making grid cells small enough we can certainly eliminate any effect of discretization. However, reducing cell size increases the number of cells and slows down model fitting. Trials with varying cell size (mask spacing) provide reassurance that discretization has not distorted the analysis.\nIn this section we report results from trials with four very different datasets for which details are given in the secr documentation: Maryland ovenbirds (ovenCH), Waitarere possums (possumCH), Arizona horned lizards (hornedlizardCH) and Tennessee black bears (blackbearCH). These studies used respectively mist nets, cage trapping, area search and DNA identification of hairs from barbed wire snares.\nThe reference scale was \\sigma estimated earlier by fitting a half-normal detection model. In each case masks were constructed with constant buffer width 4\\sigma and different spacings in the range 0.2\\sigma to 3\\sigma. This resulted in widely varying numbers of mask points (Fig. 12.4).\n\n\n\n\n\n\n\nFigure 12.4: Effect of mask spacing on number of mask points for four test datasets. Detector configurations varied: a single searched square (horned lizards), a single elongated hollow grid of mistnets (ovenbirds), multiple hollow grids of cage traps (Waitarere possums), hair snares along a dense irregular network of trails (black bears).\n\n\n\n\n\n\n\n\n\n\n\nFigure 12.5: Effect of mask spacing on estimates of density from null model. Bias is relative to the estimate using the narrowest spacing. The Arizona horned lizard data appeared especially robust to mask spacing, which may be due to the method (search of a large contiguous area) or duration (14 sampling occasions) (Royle & Young, 2008).\n\n\n\n\nThe results in Fig. 12.5 suggest that, for a uniform density model, any mask spacing less than the half-normal \\sigma is adequate; 0.6\\sigma provides a considerable safety margin. The effect of detector spacing on the relationship has not been examined. Referring back to Fig. 12.4, a mask of about 1000 points will usually be adequate with a 4\\sigma buffer.\nThe default spacing in secr.fit and make.mask is determined by dividing the x-dimension of the buffered area by 64. The resulting mask typically has about 4000 points, which is overkill. Substantial improvements in speed can be obtained with coarser masks, obtained by reducing ‘nx’ or ‘spacing’ arguments of make.mask.\nFor completeness, we revisit the question of buffer width using the esaPlot function with each of the four test datasets (Fig. 12.6).\n\n\n\n\n\n\n\nFigure 12.6: Approximate relative bias due to mask truncation for four datasets. Bias is relative to the estimate using the widest buffer.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Habitat mask</span>"
    ]
  },
  {
    "objectID": "12-habitat.html#excluding-non-habitat",
    "href": "12-habitat.html#excluding-non-habitat",
    "title": "12  Habitat mask",
    "section": "\n12.6 Excluding non-habitat",
    "text": "12.6 Excluding non-habitat\n\nOur focus so far has been on choosing a buffer width to set the outer boundary of a habitat mask, assuming that the actual boundary is arbitrary. We can call these ‘masks of convenience’ (Fig. 12.7 a); numerical accuracy and computation speed are the only constraints. At the other extreme, a mask may represent a natural island of habitat surrounded by non-habitat (Fig. 12.7 c). A geographical map, possibly in the form of an ESRI shapefile, is then sufficient to define the mask. Between these extremes there are may be a habitat mosaic including both some non-habitat near the detectors and some habitat further away, so neither the buffered mask of convenience nor the habitat island is a good match (Fig. 12.7 b).\n\n\n\n\n\n\n\nFigure 12.7: Types of habitat mask (grey mesh) defined in relation to habitat (green) and detectors (red dots). (a) mask of convenience defined by a buffer around detectors in continuous habitat, (b) mask of convenience excluding non-habitat (c) fully sampled habitat island.\n\n\n\n\nThe virtue of clipping non-habitat is that the estimate of density then relates to the area of habitat rather than the sum of habitat and non-habitat. For most uses habitat-based density would seem the more meaningful parameter.\nExclusion of non-habitat (Fig. 12.7 b,c) is achieved by providing make.mask with a digital map of the habitat in the ‘poly’ argument. The digital map may be an R object defining spatial polygons as described in Appendix C, or simply a 2-column matrix or dataframe of coordinates. This simple example uses the coordinates in possumarea:\n\nclippedmask &lt;- make.mask(traps(possumCH), type = 'trapbuffer', buffer = 400, \n                        poly = possumarea)\n\n\ncode to plot clipped maskpar(mfrow = c(1,1), mar = c(1,1,1,1))\nplot(clippedmask, border = 100, ppoly = FALSE)\npolygon(possumarea, col = 'lightgreen', border = NA)\nplot(clippedmask, dots = FALSE, mesh = grey(0.4), col = NA, \n    add = TRUE, polycol = 'blue')\nplot(traps(possumCH), detpar = list(pch = 16, cex = 0.8), \n    add = TRUE)\n\n\n\n\n\n\nFigure 12.8: Mask computed by clipping to a polygon – the shoreline of the ‘peninsula’ at Waitarere separating the Tasman Sea (left) from the estuary of the Manawatu River (right).\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nBy default, data for the ‘poly’ argument are retained as an attribute of the mask. With some data sources this grossly inflates the size of the mask, and it is better to discard the attribute with keep.poly = FALSE.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Habitat mask</span>"
    ]
  },
  {
    "objectID": "12-habitat.html#mask-covariates",
    "href": "12-habitat.html#mask-covariates",
    "title": "12  Habitat mask",
    "section": "\n12.7 Mask covariates",
    "text": "12.7 Mask covariates\n\nMasks may have a ‘covariates’ attribute that is a dataframe just like the ‘covariates’ attributes of traps and capthist objects. The data frame has one row for each row (point) on the mask, and one column for each covariate. The dataframe may include unused columns. Mask covariates are used for modelling [density surfaces] (D), not for modelling detection parameters (g0, lambda0, sigma).\nCovariates may be categorical (factor-valued) or continuous. Character-valued covariates will be coerced to factors. Covariates in detection models ideally take a small number of discrete values, but there is no such constraint on mask covariates, which may be continuous.\n\n12.7.1 Adding covariates\nMask covariates are always added after a mask is first constructed. Extending the earlier example, we can add a covariate for the computed distance to shore:\n\ncovariates(clippedmask) &lt;- data.frame(d.to.shore = \n    distancetotrap(clippedmask, possumarea))\n\nThe function addCovariates makes it easy to extract data for each mask cell from a spatial data source. Its usage is\n\naddCovariates (object, spatialdata, columns = NULL, \n    strict = FALSE, replace = FALSE)\n\nValues are extracted for the point in the data source corresponding to the centre point of each grid cell. The spatial data source (spatialdata) should be one of\n\nESRI polygon shapefile name (excluding .shp)\nsf spatial object, package sf\n\nRasterLayer, package raster\n\nSpatRaster, package terra\n\nSpatialPolygonsDataFrame, package sp\n\nSpatialGridDataFrame, package sp\n\nanother mask with covariates\na traps object with covariates\n\nOne or more input columns may be selected by name. The argument ‘strict’ generates a warning if points lie outside a mask used as a spatial data source. Appendix C has more about spatial data in secr.\nSee Chapter 11 for further applications, including smooths to represent the scale of effect.\n\n12.7.2 Repairing missing values\nCovariate values become NA for points not in the data source for addCovariates. Modelling will fail until a valid value is provided for every mask point (ignoring covariates not used in models). If only a few values are missing at only a few points it is usually acceptable to interpolate them from surrounding non-missing values. For continuous covariates we suggest linear interpolation with the function interp in the akima package (Akima & Gebhardt, 2022). The following short function provides an interface:\n\nrepair &lt;- function (mask, covariate, ...) {\n    NAcov &lt;- is.na(covariates(mask)[,covariate])\n    OK &lt;- subset(mask, !NAcov)\n    require(akima)\n    irect &lt;- akima::interp (x = OK$x, y = OK$y, z = \n        covariates(OK)[,covariate],...)\n    irectxy &lt;- expand.grid(x = irect$x, y = irect$y)\n    i &lt;- nearesttrap(mask[NAcov,], irectxy)\n    covariates(mask)[,covariate][NAcov] &lt;- irect[[3]][i]\n    mask\n}\n\nTo demonstrate repair we deliberately remove a swathe of covariate values from a copy of our clippedmask and then attempt to interpolate them (Fig. 12.9):\n\ndamagedmask &lt;- clippedmask\ncovariates(damagedmask)$d.to.shore[500:1000] &lt;- NA\nrepaired &lt;- repair(damagedmask, 'd.to.shore', nx=60, ny=50)\n\nThe interpolation may potentially be improved by varying the interp arguments nx and ny (passed via the … argument of repair). Although extrapolation is available (with linear = FALSE, extrap = TRUE) it did not work in this case, and there remain some unfilled cells (Fig. 12.9 c).\nCategorical covariates pose a larger problem. Simply copying the closest valid value may suffice to allow modelling to proceed, and this is a good solution for the few NA cells in Fig. 12.9 c. The result should always be checked visually by plotting the covariate: strange patterns may result.\n\ncopynearest &lt;- function (mask, cov) {\n    NAcov &lt;- is.na(covariates(mask)[,cov])\n    OK &lt;- subset(mask, !NAcov)\n    i &lt;- nearesttrap(mask, OK)\n    covariates(mask)[,cov][NAcov] &lt;- covariates(OK)[i[NAcov],cov]\n    mask\n}\ncompleted &lt;- copynearest(repaired, 'd.to.shore')\n\n\n\n\n\n\n\n\nFigure 12.9: Interpolation of missing values in mask covariate (artificial example). (a) True coverage, (b) Swathe of missing values, (c) Repaired by linear interpolation. Cells in the west and east that lie outside the convex hull of non-missing points are not interpolated and remain missing, (d) repair completed by filling remaining NA cells with value from nearest non-missing cell.\n\n\n\n\nSee Chapter 11 for more on the use of mask covariates.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Habitat mask</span>"
    ]
  },
  {
    "objectID": "12-habitat.html#regional-population-size",
    "href": "12-habitat.html#regional-population-size",
    "title": "12  Habitat mask",
    "section": "\n12.8 Regional population size",
    "text": "12.8 Regional population size\n\nPopulation density D is the primary parameter in this implementation of spatially explicit capture–recapture (SECR). The number of individuals (population size N(A)) is treated as a derived parameter. The rationale for this is that population size is ill-defined in many classical sampling scenarios in continuous habitat (Figs. 7a,b). Population size is well-defined for a habitat island (Fig. 12.7 c). Population size may also be well-defined for a persistent swarm, colony, herd, pack or flock, although group living is incompatible with the usual SECR assumption of independence e.g. Bischof et al. (2020).\nPopulation size on a habitat island A may be derived from an SECR model by the simple calculation \\hat N = \\hat D|A| if density is uniform. The same calculation yields the expected population in any area A^\\prime. Calculations get more tricky if density is not uniform as then \\hat N = \\int_{A^\\prime} D(\\mathbf x) \\, d\\mathbf x (computing the volume under the density surface) (Efford & Fewster, 2013). secr provides the function region.N for this purpose (Section 13.5).",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Habitat mask</span>"
    ]
  },
  {
    "objectID": "12-habitat.html#plotting-masks",
    "href": "12-habitat.html#plotting-masks",
    "title": "12  Habitat mask",
    "section": "\n12.9 Plotting masks",
    "text": "12.9 Plotting masks\nThe default plot of a mask shows each point as a grey dot. We have used dots = FALSE throughout this document to emphasise the gridcell structure. That is especially handy when we use the plot method to display mask covariates:\n\npar(mar = c(1,1,2,6), xpd = TRUE)\nplot(clippedmask, covariate = 'd.to.shore', dots = FALSE, \n    border=100, title = 'Distance to shore m', polycol = 'blue')\nplotMaskEdge(clippedmask, add = TRUE)\n\n\n\n\n\n\nFigure 12.10: Plot of a computed continuous covariate across a clipped mask, with outer margin.\n\n\n\n\nThe legend may be suppressed with legend = FALSE. See ?plot.mask for details. We used plotMaskEdge to add a line around the perimeter.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Habitat mask</span>"
    ]
  },
  {
    "objectID": "12-habitat.html#more-on-creating-and-manipulating-masks",
    "href": "12-habitat.html#more-on-creating-and-manipulating-masks",
    "title": "12  Habitat mask",
    "section": "\n12.10 More on creating and manipulating masks",
    "text": "12.10 More on creating and manipulating masks\n\n12.10.1 Arguments of make.mask\n\nThe usage statement for make.mask is included as a reminder of various options and defaults that have not been covered in this vignette. See ?make.mask for details.\n\nmake.mask (traps, buffer = 100, spacing = NULL, nx = 64, ny = 64,\n    type = c(\"traprect\", \"trapbuffer\", \"pdot\", \"polygon\", \n    \"clusterrect\", \"clusterbuffer\", \"rectangular\", \"polybuffer\"),\n    poly = NULL, poly.habitat = TRUE, cell.overlap = c(\"centre\",\n    \"any\", \"all\"), keep.poly = TRUE, check.poly = TRUE, \n    pdotmin = 0.001, random.origin = FALSE, ...)\n\n\n12.10.2 Mask attributes saved by make.mask\n\nMask objects generated by make.mask include several attributes not usually on view. Use str to reveal them. Three are simply saved copies of the arguments ‘polygon’, ‘poly.habitat’ and ‘type’, and ‘covariates’ has been discussed already.\nThe attribute ‘spacing’ is the distance in metres between adjacent grid cell centres in either x- or y- directions.\nThe attribute ‘area’ is the area of a single grid cell in hectares (1 ha = 10000 m2, so area = spacing2 / 10000; retrieve with attr(mask, 'area')). Use the function maskarea to find the total area of all mask cells; for example, here is the area in hectares of the clipped Waitarere possum mask.\n\nmaskarea(clippedmask)\n\n[1] 205.06\n\n\nThe attribute ‘boundingbox’ is a 2-column dataframe with the x- and y-coordinates of the corners of the smallest rectangle containing all grid cells.\nThe attribute ‘meanSD’ is a 2-column dataframe with the means and standard deviations of the x- and y-coordinates. These are used to standardize the coordinates if they appear directly in a model formula (e.g., D ~ x + y).\n\n12.10.3 Linear habitat\nModels for data from linear habitats analysed in the package secrlinear (Efford, 2024) use the class ‘linearmask’ that inherits from ‘mask’. Linear masks have additional attributes ‘SLDF’ and ‘graph’ to describe linear habitat networks. See [secrlinear-vignette.pdf] for details.\n\n12.10.4 Dropping points from a mask\nIf the mask you want cannot be obtained directly with make.mask then use either subset (batch) or deleteMaskPoints (interactive; unreliable in RStudio). This ensures that the attributes are updated properly. Do not simply extract the required points from the mask dataframe by subscripting ([).\n\n12.10.5 Multi-session masks\nFitting a SECR model to a multi-session capthist requires a mask for each session. If a single mask is passed to secr.fit then it will be replicated and must be appropriate for all sessions. The alternative is to provide a list of masks, one per session, in the correct order; make.mask generates such a list from a list of traps objects. See Chapter 14 for details.\n\n12.10.6 Artificial habitat maps\nFunction randomHabitat generates somewhat realistic maps of habitat that may be used in simulations. It assigns mask pixels to ‘habitat’ and ‘non-habitat’ according to an algorithm that clusters habitat cells together. The classification is saved as a covariate in the output mask, from which non-habitat cells may be dropped entirely (this was used to generate the green habitat background in Fig. 12.7 b).\n\n12.10.7 Mask vs raster\nMask objects have a lot in common with objects of the RasterLayer S4 class defined in the package raster (Hijmans, 2023a) and the SpatRaster class defined in the package terra (Hijmans, 2023b). However, they are much simpler: no projection is specified and grid cells must be square.\nA mask object may be exported as a RasterLayer using the raster method defined in secr for mask objects. This allows you to nominate a covariate to provide values for the RasterLayer, and to specify a projection.\nA mask object may also be exported as a SpatRaster using the rast method defined in secr for mask objects. See here for an example in which terra functions are used to smooth a mask covariate before returning it to the mask.\n\n\n\n\n\n\nWarning\n\n\n\nInappropriate mask spacing is a common source of problems. Model fitting can be painfully slow if the mask has too many cells. Choose the spacing (cell size) as described in Grid cell size. A single mask for widely scattered clusters of traps should drop cells from wide inter-cluster spaces (set ‘type = trapbuffer’).\n\n\n\n\n\nFigure 12.1: Mask (grey grid) generated automatically in secr.fit by buffering around the detectors (red dots) (80-m buffer, 30-m detector spacing)\nFigure 12.2: Mask (grey grid) generated with make.mask (80-m buffer, 30-m trap spacing, 15-m mask spacing). Grid cells in the centre were dropped because they were more than 80 m from any trap.\nFigure 12.3: Effect of varying buffer width on estimated density (y-axis). Vertical line indicates rule-of-thumb buffer width.\nFigure 12.4: Effect of mask spacing on number of mask points for four test datasets. Detector configurations varied: a single searched square (horned lizards), a single elongated hollow grid of mistnets (ovenbirds), multiple hollow grids of cage traps (Waitarere possums), hair snares along a dense irregular network of trails (black bears).\nFigure 12.5: Effect of mask spacing on estimates of density from null model. Bias is relative to the estimate using the narrowest spacing. The Arizona horned lizard data appeared especially robust to mask spacing, which may be due to the method (search of a large contiguous area) or duration (14 sampling occasions) (Royle & Young, 2008).\nFigure 12.6: Approximate relative bias due to mask truncation for four datasets. Bias is relative to the estimate using the widest buffer.\nFigure 12.7: Types of habitat mask (grey mesh) defined in relation to habitat (green) and detectors (red dots). (a) mask of convenience defined by a buffer around detectors in continuous habitat, (b) mask of convenience excluding non-habitat (c) fully sampled habitat island.\nFigure 12.8: Mask computed by clipping to a polygon – the shoreline of the ‘peninsula’ at Waitarere separating the Tasman Sea (left) from the estuary of the Manawatu River (right).\nFigure 12.9: Interpolation of missing values in mask covariate (artificial example). (a) True coverage, (b) Swathe of missing values, (c) Repaired by linear interpolation. Cells in the west and east that lie outside the convex hull of non-missing points are not interpolated and remain missing, (d) repair completed by filling remaining NA cells with value from nearest non-missing cell.\nFigure 12.10: Plot of a computed continuous covariate across a clipped mask, with outer margin.\n\n\n\nAkima, H., & Gebhardt, A. (2022). akima: Interpolation of irregularly and regularly spaced data. https://CRAN.R-project.org/package=akima\n\n\nBischof, R., Dupont, P., Milleret, C., Chipperfield, J., & Royle, J. A. (2020). Consequences of ignoring group association in spatial capture–recapture analysis. Wildlife Biology, 2020(1), 1–10. https://doi.org/10.2981/wlb.00649\n\n\nBorchers, D. L., & Efford, M. G. (2008). Spatially explicit maximum likelihood methods for capture-recapture studies. Biometrics, 64, 377–385.\n\n\nCalhoun, J. B., & Casby, J. U. (1958). Calculation of home range and density of small mammals. Public Health Monograph, 55.\n\n\nEfford, M. G. (2024). secrlinear: Spatially Explicit Capture-Recapture for Linear Habitats. https://CRAN.R-project.org/package=secrlinear\n\n\nEfford, M. G., & Fewster, R. M. (2013). Estimating population size by spatially explicit capture-recapture. Oikos, 122, 918–928.\n\n\nHijmans, R. J. (2023a). Raster: Geographic data analysis and modeling. In R package version 3.6-26. https://CRAN.R-project.org/package=raster\n\n\nHijmans, R. J. (2023b). Terra: Spatial data analysis. https://CRAN.R-project.org/package=terra\n\n\nKendeigh, S. C. (1944). Measurement of bird populations. Ecological Monographs, 14, 67–106. https://doi.org/10.2307/1961632\n\n\nRoyle, J. A., Chandler, R. B., Sollmann, R., & Gardner, B. (2014). Spatial capture-recapture. Academic Press.\n\n\nRoyle, J. A., & Young, K. V. (2008). A hierarchical model for spatial capture-recapture data. Ecology, 89, 2281–2289.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Habitat mask</span>"
    ]
  },
  {
    "objectID": "13-fittedmodels.html",
    "href": "13-fittedmodels.html",
    "title": "13  Working with fitted models",
    "section": "",
    "text": "13.1 Bundling fitted models\nYou may have fitted one model or several. A convenient way to handle several models is to bundle them together as an object of class ‘secrlist’:\novenlist &lt;- secrlist(ovenbird.model.1, ovenbird.model.D)\nMost of the following functions accept both ‘secr’ and ‘secrlist’ objects. The multi-model fitting function list.secr.fit returns an secrlist.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Working with fitted models</span>"
    ]
  },
  {
    "objectID": "13-fittedmodels.html#recognizing-failure-to-fit",
    "href": "13-fittedmodels.html#recognizing-failure-to-fit",
    "title": "13  Working with fitted models",
    "section": "\n13.2 Recognizing failure-to-fit",
    "text": "13.2 Recognizing failure-to-fit\nSometimes a model may appear to have fitted, but on inspection some values are missing or implausible (near zero or very large). The maximization function nlm may return the arcane ‘code 3’, or there may be message that a variance calculation failed.\nThese conditions must be taken seriously, but they are not always fatal to the analysis. See Appendix A for diagnosis and possible solutions.\nIf all variances (SE) are missing or extreme then the fit has indeed failed.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Working with fitted models</span>"
    ]
  },
  {
    "objectID": "13-fittedmodels.html#prediction",
    "href": "13-fittedmodels.html#prediction",
    "title": "13  Working with fitted models",
    "section": "\n13.3 Prediction",
    "text": "13.3 Prediction\nA fitted model (‘secr’ object) contains estimates of the coefficients of the model. Use the coef method for secr objects to display the coefficients (e.g., coef(secrdemo.0)). We note\n\nthe coefficients (aka ‘beta parameters’) are on the link scale, and at the very least must be back-transformed to the natural scale, and\na real parameter for which there is a linear sub-model is not unique: it takes a value that depends on covariates and other predictors.\n\nThese complexities are handled by the predict method for secr objects, as for other fitted models in R (e.g., lm). The desired levels of covariates and other predictors appear as columns of the ‘newdata’ dataframe. predict is called automatically when you print an ‘secr’ object by typing its name at the R prompt.\npredict is commonly called with a fitted model as the only argument; then ‘newdata’ is constructed automatically by the function makeNewData. The default behaviour is disappointing if you really wanted estimates for each level of a predictor. This can be overcome, at the cost of more voluminous output, by setting all.levels = TRUE, or simply all = TRUE. For more customised output (e.g., for a particular value of a continuous predictor), construct your own ‘newdata’.\nDensity surfaces are a special case. Prediction across space requires the function predictDsurface as covered in Chapter 11.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Working with fitted models</span>"
    ]
  },
  {
    "objectID": "13-fittedmodels.html#tabulation-of-estimates",
    "href": "13-fittedmodels.html#tabulation-of-estimates",
    "title": "13  Working with fitted models",
    "section": "\n13.4 Tabulation of estimates",
    "text": "13.4 Tabulation of estimates\nThe collate function is a handy way to assemble tables of coefficients or estimates from several fitted models. The input may be individual models or an secrlist. The output is a 4-dimensional array, typically with dimensions corresponding to\n\nsession\nmodel\nstatistic (estimate, SE, lcl, ucl)\nparameter (D, g0, sigma)\n\nThe parameters must be common to all models.\n\n# [1,,,] drops 'session' dimension for printing\ncollate(secrdemo.0, secrdemo.CL, realnames = c('g0','sigma'))[1,,,]\n\n, , g0\n\n            estimate SE.estimate     lcl     ucl\nsecrdemo.0   0.27319    0.027051 0.22348 0.32927\nsecrdemo.CL  0.27319    0.027051 0.22348 0.32927\n\n, , sigma\n\n            estimate SE.estimate    lcl    ucl\nsecrdemo.0    29.366      1.3049 26.918 32.037\nsecrdemo.CL   29.366      1.3050 26.918 32.037\n\n\nHere we see that the full- and conditional-likelihood fits produce identical estimates for the detection parameters of a Poisson model.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Working with fitted models</span>"
    ]
  },
  {
    "objectID": "13-fittedmodels.html#sec-regionN",
    "href": "13-fittedmodels.html#sec-regionN",
    "title": "13  Working with fitted models",
    "section": "\n13.5 Population size",
    "text": "13.5 Population size\n\nThe primary abundance parameter in secr is population density D. However, population size, the expected number of AC in a region A, may be predicted from any fitted model that includes D as a parameter.\nFunction region.N calculates \\hat N(A) along with standard errors and confidence intervals (Efford & Fewster, 2013). The default region is the mask used to fit the model, but this is generally arbitrary, as we have seen, and users would be wise to specify the ‘region’ argument explicitly.\nFor an example of region.N in action, consider a model fitted to data on black bears from DNA hair snags in Great Smoky Mountains National Park, Tennessee.\n\ncode to fit black bear modelmsk &lt;- make.mask(traps(blackbearCH), buffer = 6000, type = 'trapbuffer', \n                 poly = GSM, keep.poly = FALSE)  # clipped to park boundary\nbbfit &lt;- secr.fit(blackbearCH, model = g0~bk, mask = msk, trace = FALSE)\n\n\nThe region defaults to the extent of the original habitat mask:\n\nregion.N(bbfit)\n\n    estimate SE.estimate    lcl    ucl   n\nE.N    444.6      55.003 349.19 566.07 139\nR.N    444.6      50.801 360.11 561.36 139\n\n\nThe two output rows relate to the ‘expected’ and ‘realised’ numbers of AC in the region. The distinction between expected (random) N and realised (fixed) N is explained by Efford & Fewster (2013). Typically the expected and realised N(A) from region.N are the same or nearly so, but deviations occur when the density model is not uniform. The estimated sampling error is less for ‘realised’ N(A) as part is fixed at the observed n.\nThe nominated region is arbitrary. It may be a new mask with different extent and cell size. If the model used spatial covariates the covariates must also be present in the new mask. If the density model did not use spatial covariates then the region may be a simple polygon.\nIt is interesting to extrapolate the number of black bears in the entire park (Fig. 13.1):\n\nregion.N(bbfit, region = GSM)\n\n    estimate SE.estimate    lcl    ucl   n\nE.N   2091.9      258.80 1643.0 2663.5 139\nR.N   2091.4      254.72 1652.5 2657.6 139\n\n\n\n\n\n\n\n\n\nFigure 13.1: Simulated AC of black bears in Great Smoky Mountains National Park, USA. The number of AC (2092) was predicted from a model fitted to hair snags (red crosses) in one sector of the park. The habitat mask used to fit the model is shaded.\n\n\n\n\n\nEstimates of realised population size (but not expected population size) are misleading if the new region does not cover all n detected animals.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Working with fitted models</span>"
    ]
  },
  {
    "objectID": "13-fittedmodels.html#model-selection",
    "href": "13-fittedmodels.html#model-selection",
    "title": "13  Working with fitted models",
    "section": "\n13.6 Model selection",
    "text": "13.6 Model selection\n\nThe best model is not necessarily the one that most closely fits the data. We need a criterion that penalises unnecessary complexity. For models fitted by maximum likelihood, the obvious and widely used candidate is Akaike’s Information Criterion AIC\n\n\\mathrm{AIC} = -2\\log[L(\\hat \\theta)] +2K\n where K is the number of coefficients estimated and \\log[L(\\hat \\theta)] is the maximized log likelihood.\nBurnham & Anderson (2002), who popularised information-theoretic model selection for biologists, advocated the use of a small-sample adjustment due to Hurvich & Tsai (1989), designated AICc:\n\n\\mathrm{AIC}_c = -2\\log(L(\\hat{\\theta})) + 2K + \\frac{2K(K+1)}{n-K-1}.\n\nHere we assume the sample size n is the number of individuals observed at least once (i.e. the number of rows in the capthist). This is somewhat arbitrary, and a reason to question the routine use of AICc. The additional penalty has the effect of decreasing the attractiveness of models with many parameters. The effect is especially large when n&lt;2K.\nModel selection by information-theoretic methods is open to misinterpretation. We do not attempt here to deal with the many subtleties, but raise some caveats:\n\nModels to be compared by AIC must be based on identical data. Some options in secr.fit silently change the structure of the data and make models incompatible (‘CL’, ‘fastproximity’, ‘groups’, ‘hcov’, ‘binomN’). Comparisons should be made only within the family of models defined by constant settings of these options. A check of compatibility (standalone function AICcompatible) is applied automatically in secr, but this is not guaranteed to catch all misuse.\nAICc is widely used. However, there are doubts about the correct sample size for AICc, and AIC may be a better basis for model averaging, as demonstrated by simulation for generalised linear models (Fletcher, 2019, p. 60). We use AIC in this book.\nAIC values indicate the relative performance of models. Reporting AIC values per se is not helpful; present the differences \\Delta \\mathrm{AIC} between each model and the ‘best’ model.\n\n\n13.6.1 Model averaging\n\nModel weights may be used to form model-averaged estimates of real or beta parameters with modelAverage (see also Buckland et al. (1997) and Burnham & Anderson (2002)). Model weights are calculated as\n\nw_i = \\frac{\\exp(-\\Delta_i / 2),}{\\sum{\\exp(-\\Delta_i / 2)}},\n where \\Delta refers to differences in AIC or AICc.\nModels for which \\Delta_i exceeds an arbitrary limit (e.g., 10) are given a weight of zero and excluded from the summation.\n\n13.6.2 Likelihood ratio\n\nIf you have only two models, one a more general version of the other, then a likelihood-ratio test is appropriate. Here we compare an ovenbird model with density constant over time to one with a log-linear trend over years. \n\nLR.test(ovenbird.model.1, ovenbird.model.D)\n\n\n    Likelihood ratio test for two models\n\ndata:  ovenbird.model.1 vs ovenbird.model.D\nX-square = 0.83, df = 1, p-value = 0.36\n\n\nThere is no evidence of a trend.\n\n13.6.3 Model selection strategy\n\nIt is almost impossible to fit and compare all plausible SECR models. A strategy is needed to find a path through the possibilities. However, the available strategies are ad hoc and we cannot offer strong, evidence-based advice.\nOne strategy is to determine the best model for the detection process before proceeding to consider models for density. This is intuitively attractive.\n\n\nDoherty et al. (2010) used simulation to assess model selection strategies for Cormack-Jolly-Seber survival analysis. They reported that the choice of strategy had little effect on bias or precision, but could affect model weights and hence affect model-averaged estimates.\nWe are not aware of any equivalent study for SECR. \n\n\n\n\n13.6.4 Score tests\n\nModels must usually be fitted to compare them by AIC. Score tests allow models at the next level in a tree of nested models to be assessed without fitting (McCrea & Morgan, 2011). Only the ‘best’ model at each level is fitted, perhaps spawning a further set of comparisons. This method is provided in secr (see ?score.test), but it has received little attention and its limits are unknown.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Working with fitted models</span>"
    ]
  },
  {
    "objectID": "13-fittedmodels.html#sec-modelfit",
    "href": "13-fittedmodels.html#sec-modelfit",
    "title": "13  Working with fitted models",
    "section": "\n13.7 Assessing model fit",
    "text": "13.7 Assessing model fit\n\nRigor would seem to require that a model used for inference has been shown to fit the data. This premise has many fishhooks. Tests of absolute ‘goodness-of-fit’ for SECR have uniformly low power. Assumptions are inevitably breached to some degree, and we rely in practice on soft criteria (experience, judgement and the property of robustness) as covered in Chapter 6, and the relative fit of plausible models (indicated by AIC or similar criteria).\nNevertheless, tests of goodness-of-fit are potentially informative regarding ways a model might be improved. Early maximum likelihood and Bayesian approaches to SECR spawned different approaches to goodness-of-fit testing - the parametric bootstrap and Bayesian p-values. We present these in an updated context as this is an area of active research. Each test is introduced, along with a recent approach that attempts to bridge the divide by emulating Bayesian p-values for models fitted by maximum likelihood.\n\n\n13.7.1 Parametric bootstrap\nIf an SECR model fits a dataset then a goodness-of-fit statistic computed from the data will lie close to the median of that statistic from replicated Monte Carlo simulations. Borchers & Efford (2008), following an earlier edition of Cooch & White (2023), suggested using a test based on the scaled deviance i.e. [-2\\mathrm{log}{\\hat L} + 2 \\mathrm{log}L_{sat}] / \\Delta df where \\hat L is the likelihood evaluated at its maximum, L_{sat} is the likelihood of the saturated model (Table 13.1), and \\Delta df is the difference in degrees of freedom between the two. The distribution of the test statistic was estimated by a parametric bootstrap i.e. by simulation from the fitted model with parameters fixed at the MLE.\n\n\nTable 13.1: Expressions for the saturated likelihood of conditional, full Poisson and full binomial SECR likelihoods. n_\\omega is the number of individuals with detection history \\omega and summation is over the unique histories; N is the population in the area A and for evaluation we use an estimate \\hat N = \\hat D.|A|.\n\n\n\n\n\n\n\nModel\nLikelihood of saturated model\n\n\n\nconditional\n\\log (n!) - \\sum_\\omega \\log(n_\\omega!) + \\sum_\\omega n_\\omega \\log(\\frac{n_\\omega}{n})\n\n\nPoisson\nn\\log(n) - n - \\sum_\\omega \\log(n_\\omega!) + \\sum_\\omega n_\\omega \\log(\\frac{n_\\omega}{n})\n\n\nBinomial\nn\\log(\\frac{n}{N}) - (N-n)\\log(\\frac{N-n}{N}) + \\log(\\frac{N!}{(N-n)!})- \\sum_\\omega \\log(n_\\omega!) + \\sum_\\omega n_\\omega \\log(\\frac{n_\\omega}{n})\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nSaturated likelihoods are shown in Table 13.1 for the simplest models. There may be complications for models with groups, individual covariates etc. (cf Cooch & White, 2023, Section 5.1).\n\n\n\n13.7.2 Bayesian p-values\nRoyle et al. (2014) (pp. 232–243) treated the problem of assessing model fit in the context of Bayesian SECR using Bayesian p-values (Gelman & Stern, 1996).\nIn essence, Bayesian p-values compare two discrepancies: the discrepancy between the observed and expected values, and the discrepancy between simulated and expected values. A discrepancy is a scalar summary statistic for the match between two sets of counts; Royle et al. (2014) used the Freeman-Tukey statistic (e.g., Brooks & Morgan, 2000) in preference to the conventional Pearson statistic. The statistic has this general form for M counts y_m with expected value \\mathrm{E}(y_m): T = \\sum_m \\left[\\sqrt {y_m} - \\sqrt{E(y_m)}\\right]^2.\nEach pair of discrepancy statistics provides a binary outcome (Was the observed discrepancy greater than the simulated discrepancy?) and these are summarised as a ‘Bayesian p value’ i.e. the proportion of replicates in which the observed discrepancy exceeds the simulated discrepancy. The p value is expected to be around 0.5 for a model that fits.\n\n13.7.3 Emulation of Bayesian p values\nChoo et al. (2024) proposed novel simulation-based tests for SECR models fitted by maximum likelihood. Their idea was to emulate Bayesian p values by repeatedly –\n\nresampling density and detection parameters from a multivariate normal distribution based on the estimated variance-covariance matrix\nconditioning on the known (detected individuals) and locating each according to its post-hoc probability distribution (fxi in secr), using the resampled detection parameters\nsimulating additional individuals from the post-hoc distribution of unobserved individuals, as required to make up the resampled density, also using the resampled detection parameters\ncomputing the expected values of chosen summary statistics (e.g. margins of the capthist array)\ncomputing two discrepancy statistics (e.g. Freeman-Tukey) for each realisation: observed vs expected and simulated vs expected.\n\nThe explicit ‘point of difference’ of the Choo et al. (2024) approach is the propagation of uncertainty in the parameter values instead of relying on the central estimates for a parametric bootstrap. This alone would not be expected to increase the power of the test over a conventional test, and might even reduce power. However, the comparison of observed vs expected and simulated vs expected is performed separately for each realisation of the parameters, including the distribution of AC, and this may add power (think of a paired t-test).\n\n13.7.4 Implementation in secr\n\nThe parametric bootstrap is implemented in secr::secr.test and the Choo et al. (2024) emulation of Bayesian p-values is implemented in secr::MCgof. We have been unable to find a convincing application of goodness-of-fit to SECR data. Rather than promote either method we prefer to await developments.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 13.1: Simulated AC of black bears in Great Smoky Mountains National Park, USA. The number of AC (2092) was predicted from a model fitted to hair snags (red crosses) in one sector of the park. The habitat mask used to fit the model is shaded.\n\n\n\nBorchers, D. L., & Efford, M. G. (2008). Spatially explicit maximum likelihood methods for capture-recapture studies. Biometrics, 64, 377–385.\n\n\nBrooks, C., S. P., & Morgan, B. J. T. (2000). Bayesian animal survival estimation. Statistical Science, 15, 357–376.\n\n\nBuckland, S. T., Burnham, K. P., & Augustin, N. H. (1997). Model selection: An integral part of inference. Biometrics, 53(2), 603. https://doi.org/10.2307/2533961\n\n\nBurnham, K. P., & Anderson, D. R. (2002). Model selection and multimodel inference: A practical information-theoretic approach. Springer.\n\n\nChoo, Y. R., Sutherland, C., & Johnston, A. (2024). A monte carlo resampling framework for implementing goodness‐of‐fit tests in spatial capture‐recapture models. Methods in Ecology and Evolution. https://doi.org/10.1111/2041-210x.14386\n\n\nCooch, E., & White, G. (Eds.). (2023). Program MARK: A gentle introduction. (23rd ed.). http://www.phidot.org/software/mark/docs/book/\n\n\nDoherty, P. F., White, G. C., & Burnham, K. P. (2010). Comparison of model building and selection strategies. Journal of Ornithology, 152(S2), 317–323. https://doi.org/10.1007/s10336-010-0598-5\n\n\nEfford, M. G., & Fewster, R. M. (2013). Estimating population size by spatially explicit capture-recapture. Oikos, 122, 918–928.\n\n\nFletcher, D. (2019). Model averaging. Springer.\n\n\nGelman, M., A., & Stern, H. (1996). Posterior predictive assessment of model fitness via realized disrepancies. Statistica Sinica, 6, 733–807.\n\n\nHurvich, C. M., & Tsai, C.-L. (1989). Regression and time series model selection in small samples. Biometrika, 76(2), 297–307. https://doi.org/10.1093/biomet/76.2.297\n\n\nKendeigh, S. C. (1944). Measurement of bird populations. Ecological Monographs, 14, 67–106. https://doi.org/10.2307/1961632\n\n\nMcCrea, R. S., & Morgan, B. J. T. (2011). Multistate mark-recapture model selection using score tests. Biometrics, 67, 234–241. https://doi.org/10.1111/j.1541-0420.2010.01421.x\n\n\nRoyle, J. A., Chandler, R. B., Sollmann, R., & Gardner, B. (2014). Spatial capture-recapture. Academic Press.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Working with fitted models</span>"
    ]
  },
  {
    "objectID": "14-multisession.html",
    "href": "14-multisession.html",
    "title": "14  Multiple sessions",
    "section": "",
    "text": "14.1 Input\nA multi-session capthist object is essentially an R list of single-session capthist objects. We assume the functions read.capthist or make.capthist will be used for data input (simulated data are considered separately later on).",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multiple sessions</span>"
    ]
  },
  {
    "objectID": "14-multisession.html#input",
    "href": "14-multisession.html#input",
    "title": "14  Multiple sessions",
    "section": "",
    "text": "14.1.1 Detections\nEntering session-specific detections is simple because all detection data are placed in one file or dataframe. Each session uses a character-valued code (the session identifier) in the first column. For demonstration let’s assume you have a file ‘msCHcapt.txt’ with data for 3 sessions, each sampled on 4 occasions.\n\n\n# Session ID Occasion Detector sex \n1 1 3 H4 f\n1 10 2 A1 f\n1 11 4 D2 m\n1 12 1 B3 f\n.\n.\n2 1 2 A8 m\n2 1 4 A8 m\n2 10 4 A5 f\n2 11 2 A6 f\n.\n.\n3 1 4 D6 m\n3 10 3 A1 m\n3 11 2 G3 m\n3 11 4 H6 m\n.\n.\n\n\n(clipped lines are indicated by ‘. .’).\nGiven a trap layout file ‘msCHtrap.txt’ with the coordinates of the detector sites (A1, A2 etc.), the following call of read.capthist will construct a single-session capthist object for each unique code value and combine these in a multi-session capthist:\n\nmsCH &lt;- read.capthist('data/msCHcapt.txt', 'data/msCHtrap.txt', covnames = 'sex')\n\nNo errors found :-)\n\n\nUse the summary method or str(msCH) to examine msCH. Session-by-session output from summary can be excessive; the ‘terse’ option gives a more compact summary across sessions (columns).\n\nsummary(msCH, terse = TRUE)\n\n            1  2  3\nOccasions   4  4  4\nDetections 52 55 42\nAnimals    31 31 27\nDetectors  64 64 64\n\n\nSessions are ordered in msCH according to their identifiers (‘1’ before ‘2’, ‘Albert’ before ‘Beatrice’ etc.). The order becomes important for matching with session-specific trap layouts and masks, as we see later. The vector of session names (identifiers) may be retrieved with session(msCH) or names(msCH).\n\n14.1.2 Empty sessions\nIt is possible for there to be no detections in some sessions (but not all!). To create a session with no detections, include a dummy row with the value of the noncapt argument as the animal identifier; the default noncapt value is ‘NONE’. The dummy row should have occasion number equal to the number of occasions and some nonsense value (e.g. 0) in each of the other fields (trapID etc.).\nIncluding individual covariates as additional columns seems to cause trouble in the present version of secr if some sessions are empty, and should be avoided. We drop them from the example file ‘msCHcapt2.txt’:\n\n\n# Session ID Occasion Detector\n1 19 2 A1\n1 28 2 A5\n1 37 2 A6\n.\n.\n3 25 1 A5\n3 16 4 A5\n3 21 3 A6\n3 6 1 A7\n.\n.\n4 NONE 4 0\n\n\nThen,\n\nmsCH2 &lt;- read.capthist('data/msCHcapt2.txt', 'data/msCHtrap.txt')\n\nSession 4 \nNo live releases\n\nsummary(msCH2, terse = TRUE)\n\n            1  2  3  4\nOccasions   4  4  4  4\nDetections 63 52 50  0\nAnimals    39 29 31  0\nDetectors  64 64 64 64\n\n\nEmpty sessions trigger an error in verify.capthist; to fit a model suppress verification (e.g., secr.fit(msCH2, verify = FALSE)).\nIf the first session is empty then either direct the autoini option to a later session with e.g., details = list(autoini = 2) or provide initial values manually in the start argument.\n\n14.1.3 Detector layouts\nAll sessions may share the same detector layout. Then the ‘trapfile’ argument of read.capthist is a single name, as in the example above. The trap layout is repeated as an attribute of each component (single-session) capthist.\nAlternatively, each session may have its own detector layout. Unlike the detection data, each session-specific layout is specified in a separate input file or traps object. For read.capthist the ‘trapfile’ argument is then a vector of file names, one for each session. For make.capthist, the ‘traps’ argument may be a list of traps objects, one per session. The first trap layout is used for the first session, the second for the second session, etc.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multiple sessions</span>"
    ]
  },
  {
    "objectID": "14-multisession.html#manipulation",
    "href": "14-multisession.html#manipulation",
    "title": "14  Multiple sessions",
    "section": "\n14.2 Manipulation",
    "text": "14.2 Manipulation\nThe standard extraction and manipulation functions of secr (summary, verify, covariates, subset, reduce etc.) mostly allow for multi-session input, applying the manipulation to each component session in turn. The function ms returns TRUE if its argument is a multi-session object and FALSE otherwise.\nPlotting a multi-session capthist object (e.g., plot(msCH)) will create one new plot for each session unless you specify add = TRUE.\nMethods that extract attributes from multi-session capthist object will generally return a list in which each component is the result from one session. Thus for the ovenbird mistnetting data traps(ovenCH) extracts a list of 5 traps objects, one for each annual session 2005–2009.\nThe subset method for capthist objects has a ‘sessions’ argument for selecting particular session(s) of a multi-session object.\n\n\nTable 14.1: Manipulation of multi-session capthist objects (CH).\n\n\n\n\n\n\n\n\n\nFunction\nPurpose\nInput\nOutput\n\n\n\njoin\ncollapse sessions\nmulti-session CH\nsingle-session CH\n\n\nMS.capthist\nbuild multi-session CH\nseveral single-session CH\nmulti-session CH\n\n\nsplit\nsubdivide CH\nsingle-session CH\nmulti-session CH\n\n\n\n\nmulti-session CH\nseveral multi-session CH\n\n\n\n\n\n\nThe split method for capthist objects (?split.capthist) may be used to break a single-session capthist object into a multi-session object, segregating detections by some attribute of the individuals, or by occasion or detector groupings. Also, from secr 5.0.1, the split method may be used to group sessions in a multi-session capthist.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multiple sessions</span>"
    ]
  },
  {
    "objectID": "14-multisession.html#fitting",
    "href": "14-multisession.html#fitting",
    "title": "14  Multiple sessions",
    "section": "\n14.3 Fitting",
    "text": "14.3 Fitting\nGiven multi-session capthist input, secr.fit automatically fits a multi-session model by maximizing the product of session-specific likelihoods (Efford et al., 2009). For fitting a model separately to each session see the later section on Faster fitting….\n\n14.3.1 Habitat masks\nThe default mechanism for constructing a habitat mask in secr.fit is to buffer around the trap layout. This extends to multi-session data; buffering is applied to each trap layout in turn.\nOverride the default buffering mechanism by specifying the ‘mask’ argument of secr.fit. This is necessary if you want to –\n\nreduce or increase mask spacing (pixel size; default 1/64 x-range)\nclip the mask to exclude non-habitat\ninclude mask covariates (predictors of local density)\ndefine non-Euclidean distances (Appendix F)\nspecify a rectangular mask (type = “traprect” vs type = “trapbuffer”)\n\nFor any of these you are likely to use the make.mask function (the manual alternative is usually too painful to contemplate). If make.mask is provided with a list of traps objects as its ‘traps’ argument then the result is a list of mask objects - effectively, a multi-session mask.\nIf addCovariates receives a list of masks and a single spatial data source then it will add the requested covariate(s) to each mask and return a new list of masks. The single spatial data source is expected to span all the regions; mask points that are not covered receive NA covariate values. As an alternative to a single spatial data source, the spatialdata argument may be a list of spatial data sources, one per mask, in the order of the sessions in the corresponding capthist object.\nTo eliminate any doubt about the matching of session-specific masks to session-specific detector arrays it is always worth plotting one over the other. We don’t have an interesting example, but\n\ncode to plot multiple masksmasks &lt;- make.mask(traps(msCH), buffer = 80, nx = 32, type = 'trapbuffer')\npar (mfrow = c(1,3), mar = c(1,1,3,1))\nfor (sess in 1:length(msCH)) {\n    plot(masks[[sess]])\n    plot(traps(msCH)[[sess]], add = TRUE)\n    mtext(side=3, paste('session', sess))\n}\n\n\n\n\n\n\nFigure 14.1\n\n\n\n\n\n14.3.2 Session models\nThe default in secr.fit is to treat all parameters as constant across sessions. For detection functions parameterized in terms of cumulative hazard (e.g., ‘HHN’ or ‘HEX’) this is equivalent to model = list(D ~ 1, lambda0 ~ 1, sigma ~ 1). Two automatic predictors are provided specifically for multi-session models: ‘session’ and ‘Session’.\n\n14.3.2.1 Session-stratified estimates\nA model with lowercase ‘session’ fits a distinct value of the parameter (D, g0, lambda0, sigma) for each level of factor(session(msCH)).\n\n14.3.2.2 Session covariates\nOther variation among sessions may be modelled with session-specific covariates. These are provided to secr.fit on-the-fly in the argument ‘sessioncov’ (they cannot be embedded in the capthist object like detector or individual covariates). The value for ‘sessioncov’ should be a dataframe with one row per session. Each column is a potential predictor in a model formula; other columns are ignored.\nSession covariates are extremely flexible. The linear trend of the ‘Session’ predictor may be emulated by defining a covariate sessnum = 0:(R-1) where R is the number of sessions. Sessions of different types may be distinguished by a factor-valued covariate. Supposing for the ovenbird dataset we wished to distinguish years 2005 and 2006 from 2007, 2008 and 2009, we could use earlylate = factor(c('early','early','late','late','late')). Quantitative habitat attributes might also be coded as session covariates.\n\n14.3.2.3 Trend across sessions\n \nsecr is primarily for estimating closed population density (density at one point in time), but multi-session data may also be modelled to describe population trend over time. A trend model for density may be interesting if the sessions fall in some natural sequence, such as a series of annual samples (as in the ovenbird dataset ovenCH). A model with initial uppercase ‘Session’ fits a trend across sessions using the session number as the predictor. The fitted trend is linear on the link scale; using the default link function for density (‘log’) this corresponds to exponential growth or decline if samples are equally spaced in time.\nThe pre-fitted model ovenbird.model.D provides an example. The coefficient ‘D.Session’ is the rate of change in log(D):\n\ncoef(ovenbird.model.D)\n\n               beta  SE.beta      lcl       ucl\nD          0.031711 0.191460 -0.34354  0.406966\nD.Session -0.063859 0.070151 -0.20135  0.073635\ng0        -3.561933 0.150623 -3.85715 -3.266718\nsigma      4.364111 0.081149  4.20506  4.523159\n\n\nThe overall finite rate of increase (equivalent to Pradel’s lambda) is given by\n\nbeta &lt;- coef(ovenbird.model.D)['D.Session','beta']\nsebeta &lt;- coef(ovenbird.model.D)['D.Session','SE.beta']\nexp(beta)\n\n[1] 0.93814\n\n\nConfidence intervals may also be back-transformed with exp. To back-transform the SE use the delta-method approximation exp(beta) * sqrt(exp(sebeta^2)-1) = 0.06589.\nThis is fine for a single overall lambda. However, if you are interested in successive estimates (session 1 to session 2, session 2 to session 3 etc.) the solution is slightly more complicated. Here we describe a simple option using ‘backward difference’ coding of the levels of the factor session, specified with the details argument ‘contrasts’. This coding is provided by the function contr.sdif in the MASS package (e.g., Venables and Ripley 1999 Section 6.2).\n\nfit &lt;- secr.fit(ovenCH, model = D~session, buffer = 300, trace = FALSE,\n    details = list(contrasts = list(session = MASS::contr.sdif)))\ncoef(fit)\n\nA more sophisticated version is provided in Appendix J.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multiple sessions</span>"
    ]
  },
  {
    "objectID": "14-multisession.html#simulation",
    "href": "14-multisession.html#simulation",
    "title": "14  Multiple sessions",
    "section": "\n14.4 Simulation",
    "text": "14.4 Simulation\nBack at the start of this document we used sim.capthist to generate msCH, a simple multi-session capthist. Here we look at various extensions. Generating SECR data is a 2-stage process. The first stage simulates the locations of animals to create an object of class ‘popn’; the second stage generates samples from that population according to a particular sampling regime (detector array, number of occasions etc.).\n\n14.4.1 Simulating multi-session populations\nBy default sim.capthist uses sim.popn to generate a new population independently for each session. Centres are placed within a rectangular region obtained by buffering around a ‘core’ (the traps object passed to sim.capthist).\nThe session-specific populations may also be prepared in advance as a list of ‘popn’ objects (use nsessions &gt; 1 in sim.popn). This allows greater control. In particular, the population density may be varied among sessions by making argument D a vector of session-specific densities. Other arguments of sim.popn do not yet accept multi-session input – it might be useful for ‘core’ to accept a list of traps objects (or a list of mask objects if model2D = “IHP”).\nWe can also put aside the basic assumption of independence among sessions and simulate a single population open to births, deaths and movement between sessions. This does not correspond to any model that can be fitted in secr, but it allows the effects of non-independence to be examined. See ?turnover for further explanation.\n\n14.4.2 Multi-session sampling\nA multi-session population prepared in advance is passed as the popn argument of sim.capthist, replacing the usual list (D, buffer etc.).\nThe argument ‘traps’ may be a list of length equal to nsessions. Each component potentially differs not just in detector locations, but also with respect to detector type (‘detector’) and resighting regime (‘markocc’). The argument ‘noccasions’ may also be a vector with a different number of occasions in each session.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multiple sessions</span>"
    ]
  },
  {
    "objectID": "14-multisession.html#problems",
    "href": "14-multisession.html#problems",
    "title": "14  Multiple sessions",
    "section": "\n14.5 Problems",
    "text": "14.5 Problems\nThere are problems specific to multi-session data.\n\n14.5.1 Failure of autoini\nNumerical maximization of the likelihood requires a starting set of parameter values. This is either computed internally with the function autoini or provided by the user. Given multi-session data, the default procedure is for secr.fit to apply autoini to the first session only. If the data for that session are inadequate or result in parameter estimates that are extreme with respect to the remaining sessions then model fitting may abort. One solution is to provide start values manually, but that can be laborious. A quick fix is often to switch the session used to compute starting values by changing the details option ‘autoini’. For example\n\nfit0 &lt;- secr.fit(ovenCH, mask = msk, details = list(autoini = 2), trace = FALSE)\n\nA further option is to combine the session data into a single-session capthist object with details = list(autoini = \"all\"); the combined capthist is used only by autoini.\n\n14.5.2 Covariates with incompatible factor levels\nIndividual or detector covariates used in a multi-session model obviously must appear in each of the component sessions. It is less obvious, and sometimes annoying, that a factor (categorical) covariate should have exactly the same levels in the same order in each component session. The verify methods for capthist objects checks that this is in fact the case (remember that verify is called by secr.fit unless you suppress it).\nA common example might be an individual covariate ‘sex’ with the levels “f” and “m”. If by chance only males are detected in one of the sessions, and as a result the factor has a single level “m” in that session, then verify will give a warning.\nThe solution is to force all sessions to use the same factor levels. The function shareFactorLevels is provided for this purpose. For example\n\nmsCH &lt;- shareFactorLevels(msCH)",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multiple sessions</span>"
    ]
  },
  {
    "objectID": "14-multisession.html#sec-fasterfitting",
    "href": "14-multisession.html#sec-fasterfitting",
    "title": "14  Multiple sessions",
    "section": "\n14.6 Speed",
    "text": "14.6 Speed\nFitting a multi-session model with each parameter stratified by session is unnecessarily slow. In this case no data are pooled across sessions and it is better to fit each session separately. If your data are already in a multi-session capthist object then the speedy solution is\n\nmsk &lt;- make.mask(traps(ovenCH[[1]]), buffer = 300, nx = 25, type = 'trapbuffer')\nfits &lt;- lapply(ovenCH, secr.fit, mask = msk, trace = FALSE)\nclass(fits) &lt;- 'secrlist'\npredict(fits)\n\n$`2005`\n       link  estimate SE.estimate       lcl        ucl\nD       log  0.846489   0.3055467  0.426316   1.680780\ng0    logit  0.023247   0.0082002  0.011591   0.046079\nsigma   log 88.124224  19.4762683 57.439449 135.201136\n\n$`2006`\n       link  estimate SE.estimate       lcl       ucl\nD       log  1.026929   0.2936740  0.592771  1.779071\ng0    logit  0.030126   0.0092789  0.016396  0.054715\nsigma   log 73.020556  11.7120666 53.429792 99.794541\n\n$`2007`\n       link  estimate SE.estimate       lcl        ucl\nD       log  1.078170   0.2879556  0.644547   1.803516\ng0    logit  0.034768   0.0093266  0.020465   0.058471\nsigma   log 76.079441  11.5022360 56.663445 102.148419\n\n$`2008`\n       link  estimate SE.estimate      lcl       ucl\nD       log  1.468095    0.488689  0.77772  2.771315\ng0    logit  0.028298    0.011253  0.01289  0.060988\nsigma   log 52.223609   10.478510 35.38004 77.085985\n\n$`2009`\n       link  estimate SE.estimate       lcl        ucl\nD       log  0.531270   0.2009519  0.259448   1.087876\ng0    logit  0.024442   0.0088054  0.012003   0.049129\nsigma   log 98.909058  22.8569244 63.253482 154.663449\n\n\nThe first line (lapply) creates a list of ‘secr’ objects. The predict method works once we set the class attribute to ‘secrlist’ (or you could lapply(fits, predict)).\n\nfits2 &lt;- secr.fit(ovenCH, model=list(D~session, g0~session, \n    sigma~session), mask = msk, trace = FALSE)\n\nWhat if we wish to compare ths model with a less general one (i.e. with some parameter values shared across sessions)? For that we need the number of parameters, log likelihood and AIC summed across sessions:\n\napply(AIC(fits)[,3:5],2,sum)\n\nWarning: models not compatible for AIC\n\n\n   npar  logLik     AIC \n  15.00 -925.89 1881.78 \n\nAIC(fits2)[,3:5]\n\n      npar  logLik    AIC\nfits2   15 -925.89 1881.8\n\n\nAICc is not a simple sum of session-specific AICc and should be calculated manually (hint: use sapply(ovenCH, nrow) for session-specific sample sizes).\nThe unified model fit and separate model fits with lapply give essentially the same answers, and the latter approach is faster by a factor of 157.\nUsing lapply does not work if some arguments of secr.fit other than ‘capthist’ themselves differ among sessions (as when ‘mask’ is a list of session-specific masks). Then we can use either a ‘for’ loop or the slightly more demanding function mapply, with the same gain in speed.\n\n# one mask per session\nmasks &lt;- make.mask(traps(ovenCH), buffer = 300, nx = 32, \n    type = 'trapbuffer')  \nfits3 &lt;- list.secr.fit(ovenCH, mask = masks, constant = \n    list(trace = FALSE))",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multiple sessions</span>"
    ]
  },
  {
    "objectID": "14-multisession.html#caveats",
    "href": "14-multisession.html#caveats",
    "title": "14  Multiple sessions",
    "section": "\n14.7 Caveats",
    "text": "14.7 Caveats\n\n14.7.1 Independence is a strong assumption\nIf sessions are not truly independent then expect confidence intervals to be too short. This is especially likely when a trend model is fitted to temporal samples with incomplete population turnover between sessions. The product likelihood assumes a new realisation of the underlying population process for each session. If in actuality much of the sampled population remains the same (the same individuals in the same home ranges) then the precision of the trend coefficient will be overstated. Either an open population model is needed (e.g., openCR (Efford & Schofield, 2020)) or extra work will be needed to obtain credible confidence limits for the trend (probably some form of bootstrapping).\n\n14.7.2 Parameters are assumed constant by default\nOutput from predict.secr for a multi-session model is automatically stratified by session even when the model does not include ‘session’, ‘Session’ or any session covariate as a predictor (the output simply repeats the constant estimates for each session).\n\n\n\nFigure 14.1: \n\n\n\nEfford, M. G., Borchers, D. L., & Byrom, A. E. (2009). Density estimation by spatially explicit capture-recapture: Likelihood-based methods. In D. L. Thomson, E. G. Cooch, & M. J. Conroy (Eds.), Modeling demographic processes in marked populations (pp. 255–269). Springer.\n\n\nEfford, M. G., & Schofield, M. R. (2020). A spatial open-population capture-recapture model. Biometrics, 76, 392–402.\n\n\nKendeigh, S. C. (1944). Measurement of bird populations. Ecological Monographs, 14, 67–106. https://doi.org/10.2307/1961632\n\n\nKéry, M., & Royle, J. A. (2020). Applied hierarchical modeling in ecology. Volume 2. Elsevier Science.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multiple sessions</span>"
    ]
  },
  {
    "objectID": "15-finite-mixtures.html",
    "href": "15-finite-mixtures.html",
    "title": "15  Individual heterogeneity",
    "section": "",
    "text": "15.1 Finite mixture models in secr\nsecr allows 2- or 3-class finite mixture models for any ‘real’ detection parameter (e.g., g0, lambda0 or sigma of a halfnormal detection function). Consider a simple example in which we specify a 2-class mixture by adding the predictor ‘h2’ to the model formula:\nContinuing with the snowshoe hares:\nfit.h2 &lt;- secr.fit(hareCH6, model = lambda0~h2, detectfn = 'HEX', \n                   buffer = 250, trace = FALSE)\ncoef(fit.h2)\n\n               beta  SE.beta      lcl      ucl\nD            0.4244 0.133371  0.16300  0.68581\nlambda0     -1.7604 0.191686 -2.13612 -1.38473\nlambda0.h22  2.1000 0.912171  0.31213  3.88778\nsigma        3.6616 0.083454  3.49801  3.82515\npmix.h22    -3.6168 0.964990 -5.50815 -1.72546\n\npredict(fit.h2)\n\n$`session = wickershamunburne, h2 = 1`\n         link estimate SE.estimate      lcl      ucl\nD         log  1.52868    0.204791  1.17704  1.98537\nlambda0   log  0.17197    0.033270  0.11811  0.25039\nsigma     log 38.92275    3.253912 33.04971 45.83946\npmix    logit  0.97383    0.024589  0.84883  0.99596\n\n$`session = wickershamunburne, h2 = 2`\n         link  estimate SE.estimate        lcl      ucl\nD         log  1.528679    0.204791  1.1770389  1.98537\nlambda0   log  1.404286    1.694556  0.2190630  9.00207\nsigma     log 38.922752    3.253912 33.0497065 45.83946\npmix    logit  0.026165    0.024589  0.0040372  0.15117\nsecr.fit has expanded the model to include an extra ‘real’ parameter ‘pmix’, for the proportions in the respective latent classes. You could specify this yourself as part of the ‘model’ argument, but secr.fit knows to add it. The link function for ‘pmix’ defaults to ‘mlogit’ (after the mlogit link in MARK), and any attempt to change the link is ignored.\nThere are two extra ‘beta’ parameters: lambda0.h22, which is the difference in lambda0 between the classes on the link (logit) scale, and pmix.h22, which is the proportion in the second class, also on the logit scale. Fitted (real) parameter values are reported separately for each mixture class (h2 = 1 and h2 = 2). An important point is that exactly the same estimate of total density is reported for both mixture classes; the actual abundance of each class is D \\times pmix.\nWe now shift to a more interesting example based on the Coulombe’s house mouse Mus musculus dataset (Otis et al., 1978).\nmorning &lt;- subset(housemouse, occ = c(1,3,5,7,9))\nmodels &lt;- list(lambda0~1, lambda0~h2, sigma~h2, list(lambda0~h2, sigma~h2))\nargs &lt;- list(capthist = morning, buffer = 25, detectfn = 'HEX', trace = FALSE)\nfits &lt;- list.secr.fit(model = models, constant = args, names =\n    c('null', 'h2.lambda0', 'h2.sigma', 'h2.lambda0.sigma'))\nAIC(fits, sort = FALSE)[, c(3,4,7,8)]\n\n                 npar  logLik   dAIC  AICwt\nnull                3 -1270.4 33.345 0.0000\nh2.lambda0          5 -1268.2 32.881 0.0000\nh2.sigma            5 -1255.0  6.580 0.0359\nh2.lambda0.sigma    6 -1250.7  0.000 0.9641\n\ncollate(fits, realnames = \"D\")[1,,,]\n\n                 estimate SE.estimate    lcl    ucl\nnull               1291.6      110.91 1091.8 1527.8\nh2.lambda0         1312.2      115.63 1104.4 1559.0\nh2.sigma           1319.3      119.30 1105.4 1574.6\nh2.lambda0.sigma   1285.3      121.70 1068.0 1546.8\nAlthough the best mixture model fits substantially better than the null model (\\DeltaAIC = 33.3), there is only a 2.6% difference in \\hat D. More complex models are allowed. For example, one might, somewhat outlandishly, fit a learned response to capture that differs between two latent classes, while also allowing sigma to differ between classes:\nmodel.h2xbh2s &lt;- secr.fit(morning, model = list(lambda0~h2*bk, sigma~h2), \n                          buffer = 25, detectfn = 'HEX')",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Individual heterogeneity</span>"
    ]
  },
  {
    "objectID": "15-finite-mixtures.html#finite-mixture-models-in-secr",
    "href": "15-finite-mixtures.html#finite-mixture-models-in-secr",
    "title": "15  Individual heterogeneity",
    "section": "",
    "text": "Warning\n\n\n\nWhen more than one real parameter is modelled as a mixture, there is an ambiguity: is the population split once into latent classes common to all real parameters, or is the population split separately for each real parameter? The second option would require a distinct level of the mixing parameter for each real parameter. secr implements only the ‘common classes’ option, which saves one parameter.\n\n\n\n\n\n\n\n\n15.1.1 Number of classes\nThe theory of finite mixture models in capture–recapture (Pledger, 2000) allows an indefinite number of classes – 2, 3 or perhaps more. Programmatically, the extension to more classes is obvious (e.g., h3 for a 3-class mixture). The appropriate number of latent classes may be determined by comparing AIC for the fitted models.\nLooking on the bright side, it is unlikely that you will ever have enough data to support more than 2 classes. For the data in the example above, the 2-class and 3-class models have identical log likelihood to 4 decimal places, while the latter requires 2 extra parameters to be estimated (this is to be expected as the data were simulated from a null model with no heterogeneity).\n\n15.1.2 Label switching\nIt is a quirk of mixture models that the labeling of the latent classes is arbitrary: the first class in one fit may become the second class in another. This is the phenomenon of ‘label switching’ (Stephens, 2000).\nFor example, in the house mouse model ‘h2.lambda0’ the first class is initially dominant, but we can switch that by choosing different starting values for the maximization:\n\nargs &lt;- list(capthist = morning, model = lambda0~h2, buffer = 25, \n    detectfn = 'HEX', trace = FALSE)\nstarts &lt;- list(NULL, c(7,2,1.3,2,0))\nfitsl &lt;- list.secr.fit(start = starts, constant = args, \n    names = c('start1', 'start2'))\nAIC(fitsl)[,c(2,3,7,8)]\n\n                 detectfn npar dAIC AICwt\nstart1 hazard exponential    5    0   0.5\nstart2 hazard exponential    5    0   0.5\n\nround(collate(fitsl, realnames='pmix')[1,,,],3)\n\n       estimate SE.estimate   lcl   ucl\nstart1    0.922       0.087 0.528 0.992\nstart2    0.078       0.087 0.008 0.473\n\n\nClass-specific estimates of the detection parameter (here lambda0) are reversed, but estimates of other parameters are unaffected.\n\n15.1.3 Multimodality\n\nThe likelihood of a finite mixture model may have multiple modes (Brooks et al., 1997; Pledger, 2000). The risk is ever-present that the numerical maximization algorithm will get stuck on a local peak, and in this case the estimates are simply wrong. Slight differences in starting values or numerical method may result in wildly different answers.\nThe problem has not been explored fully for SECR models, and care is needed. Pledger (2000) recommended fitting a model with more classes as a check in the non-spatial case, but this is not proven to work with SECR models. It is desirable to try different starting values. This can be done simply using another model fit. For example:\n\nfit.h2.2 &lt;- secr.fit(hareCH6, model = lambda0~h2, buffer = 250, \n    detectfn = 'HEX', trace = FALSE, start = fit.h2)\n\nA more time consuming, but illuminating, check on a 2-class model is to plot the profile log likelihood for a range of mixture proportions (Brooks et al., 1997). We can use the function pmixprofileLL in secr to calculate these profile likelihoods. This requires a maximization step for each value of ‘pmix’; multiple cores may be used in parallel to speed up the computation. pmixprofileLL expects the user to identify the coefficient or ‘beta parameter’ corresponding to ‘pmix’ (argument ‘pmi’):\n\ncode for profile likelihood plotpmvals &lt;- seq(0.01,0.99,0.01)\n# use a coarse mask to make it faster\nmask &lt;- make.mask(traps(ovenCH[[1]]), nx = 32, buffer = 200, \n    type = \"trapbuffer\")\nprofileLL &lt;- pmixProfileLL(ovenCH[[1]], model = list(lambda0~h2, \n    sigma~h2), pmi = 5, detectfn = 'HEX', CL = TRUE, pmvals = \n    pmvals, mask = mask, trace = FALSE)\npar(mar = c(4,4,2,2))\nplot(pmvals, profileLL, xlim = c(0,1), xlab = 'Fixed pmix', \n     ylab = 'Profile log-likelihood')\n\n\n\n\n\n\nFigure 15.1: Profile log-likelihood for mixing proportion between 0.01 and 0.99 in a 2-class finite mixture model for ovenbird data from 2005\n\n\n\n\n\nMultimodality is likely to show up as multiple rounded peaks in the profile likelihood. Label switching may cause some ghost reflections about pmix = 0.5 that can be ignored. If multimodality is found one should accept only estimates for which the maximized likelihood matches that from the highest peak. In the ovenbird example, the maximized log likelihood of the fitted h2 model was -163.8 and the estimated mixing proportion was 0.51, so the correct maximum was found.\nMaximization algorithms (argument ‘method’ of secr.fit) differ in their tendency to settle on local maxima; ‘Nelder-Mead’ is probably better than the default ‘Newton-Raphson’. Simulated annealing is sometimes advocated, but it is slow and has not been tried with SECR models.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Individual heterogeneity</span>"
    ]
  },
  {
    "objectID": "15-finite-mixtures.html#mitigation",
    "href": "15-finite-mixtures.html#mitigation",
    "title": "15  Individual heterogeneity",
    "section": "\n15.2 Mitigating factors",
    "text": "15.2 Mitigating factors\nHeterogeneity may be demonstrably present yet have little effect on density estimates. Bias in density is a non-linear function of the coefficient of variation of a_i(\\theta). For CV &lt;20\\% the bias is likely to be negligible (Efford & Mowat, 2014).\nIndividual variation in \\lambda_0 and \\sigma may be inversely correlated and therefore compensatory, reducing bias in \\hat D (Efford & Mowat, 2014). Bias is a function of heterogeneity in the effective sampling area a(\\theta) which may vary less than each of the components \\lambda_0 and \\sigma.\nIt can be illuminating to re-parameterize the detection model.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Individual heterogeneity</span>"
    ]
  },
  {
    "objectID": "15-finite-mixtures.html#hybrid-hcov-model",
    "href": "15-finite-mixtures.html#hybrid-hcov-model",
    "title": "15  Individual heterogeneity",
    "section": "\n15.3 Hybrid ‘hcov’ model",
    "text": "15.3 Hybrid ‘hcov’ model\n\nThe hybrid mixture model accepts a 2-level categorical (factor) individual covariate for class membership that may be missing (NA) for any fraction of animals. The name of the covariate to use is specified as argument ‘hcov’ in secr.fit. If the covariate is missing for all individuals then a full 2-class finite mixture model will be fitted (i.e. mixture as a random effect). Otherwise, the random effect applies only to the animals of unknown class; others are modelled with detection parameter values appropriate to their known class. If class is known for all individuals the model is equivalent to a covariate (CL = TRUE) or grouped (CL = FALSE) model. When many or all animals are of known class the mixing parameter may be treated as an estimate of population proportions (probability a randomly selected individual belongs to class u). This is obviously useful for estimating sex ratio free of detection bias. See the hcov help page (?hcov) for implementation details, and here for the theory. \nThe house mouse dataset includes an individual covariate ‘sex’ with 81 females, 78 males and one unknown.\n\nfit.h &lt;- secr.fit(morning, model = list(lambda0~h2, sigma~h2), \n    hcov = 'sex', buffer = 25, detectfn = 'HEX', trace = FALSE)\npredict(fit.h)\n\n$`session = coulombe, h2 = f`\n         link   estimate SE.estimate        lcl        ucl\nD         log 1310.94427  113.457150 1106.75992 1552.79826\nlambda0   log    0.28640    0.041401    0.21605    0.37965\nsigma     log    2.17857    0.153650    1.89763    2.50109\npmix    logit    0.46346    0.043121    0.38077    0.54822\n\n$`session = coulombe, h2 = m`\n         link   estimate SE.estimate        lcl        ucl\nD         log 1310.94427  113.457150 1106.75992 1552.79826\nlambda0   log    0.19836    0.035648    0.13986    0.28133\nsigma     log    2.04752    0.173458    1.73479    2.41663\npmix    logit    0.53654    0.043121    0.45178    0.61923\n\n\n\n\n\nFigure 15.1: Profile log-likelihood for mixing proportion between 0.01 and 0.99 in a 2-class finite mixture model for ovenbird data from 2005\n\n\n\nBrooks, S. P., Morgan, B. J. T., Ridout, M. S., & Pack, S. E. (1997). Finite mixture models for proportions. Biometrics, 53(3), 1097. https://doi.org/10.2307/2533567\n\n\nEfford, M. G., & Mowat, G. (2014). Compensatory heterogeneity in spatially explicit capture-recapture data. Ecology, 95, 1341–1348.\n\n\nKendeigh, S. C. (1944). Measurement of bird populations. Ecological Monographs, 14, 67–106. https://doi.org/10.2307/1961632\n\n\nOtis, D. L., Burnham, K. P., White, G. C., & Anderson, D. R. (1978). Statistical inference from capture data on closed animal populations. Wildlife Monographs, 62.\n\n\nPledger, S. (2000). Unified maximum likelihood estimates for closed capture-recapture models using mixtures. Biometrics, 56, 434–442.\n\n\nStephens, M. (2000). Dealing with label switching in mixture models. Journal of the Royal Statistical Society Series B, 62, 795–809.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Individual heterogeneity</span>"
    ]
  },
  {
    "objectID": "16-sex.html",
    "href": "16-sex.html",
    "title": "16  Sex differences",
    "section": "",
    "text": "16.1 Models\nWe list the possible models in order of usefulness (your mileage may vary; see also the decision chart below):",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Sex differences</span>"
    ]
  },
  {
    "objectID": "16-sex.html#models",
    "href": "16-sex.html#models",
    "title": "16  Sex differences",
    "section": "",
    "text": "Hybrid mixture model\nConditional likelihood with individual covariate\nSeparate sessions\nFull likelihood with groups\n\n\n16.1.1 Hybrid mixtures\nThis method accommodates occasional missing values and estimates the sex ratio as a parameter (pmix). The method works for both conditional and full likelihood. If estimated (CL = FALSE) the absolute density D includes both classes. The sex ratio is estimated as a parameter (pmix).\n\n16.1.2 Individual covariate\nIncluding sex as an individual covariate in the model requires the model to be fitted by maximizing the conditional likelihood (CL = TRUE). Include a categorical (factor) covariate in model formulae (e.g., g0 \\sim sex).\n\n\n\n\n\n\nTip\n\n\n\nDerived densities are computed from a CL model with function derived. To get sex-specific densities specify groups = \"sex\" in derived.\n\n\n\n16.1.3 Sex as session\nIt is possible to model data for the two sexes as different sessions (most easily, by coding ‘female’ or ‘male’ in the first column of the capture file read with read.capthist). Sex differences are then modelled by including a ‘session’ term in relevant model formulae (e.g., g0 \\sim session).\n\n\n\n\n\n\nWarning\n\n\n\nParameters not explicitly modelled as session-specific are assumed to be shared. In a full-likelihood model you also need D \\sim session to allow for an uneven sex ratio.\n\n\n\n16.1.4 Groups\nGroups were described earlier. Use full likelihood (CL = FALSE), define groups = \"sex\" or similar, and include a group term ‘g’ in relevant formulae (e.g., g0 \\sim g). ‘CL’ and ‘groups’ are arguments of secr.fit. The preceding Warning applies also to groups.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Sex differences</span>"
    ]
  },
  {
    "objectID": "16-sex.html#sec-demonstration",
    "href": "16-sex.html#sec-demonstration",
    "title": "16  Sex differences",
    "section": "\n16.2 Demonstration",
    "text": "16.2 Demonstration\nWe re-analyse the morning house mouse data already analysed in Chapter 15. For all methods except the hybrid mixture we have to discard one individual of unknown sex (hence capthist marked with ‘x’). Close inspection of the results shows that the methods are equivalent.\n\nmorning &lt;- subset(housemouse, occ = c(1,3,5,7,9)) # includes one mouse with sex unknown\nmorningx &lt;- subset(housemouse, !is.na(covariates(housemouse)$sex), occ = c(1,3,5,7,9))\nmorningxs &lt;- split(morningx, covariates(morningx)$sex)  # split into two sessions\n\n\n# conditional likelihood\nhybridxCL  &lt;- secr.fit(morningx, model = list(lambda0~h2, sigma~h2),\n                CL = TRUE, buffer = 25, detectfn = 'HEX', \n                trace = FALSE,  hcov = 'sex')\nxCL        &lt;- secr.fit(morningx, model = list(lambda0~sex, \n                sigma~sex), CL = TRUE, buffer = 25, detectfn = 'HEX', \n                trace = FALSE)\nsessionxCL &lt;- secr.fit(morningxs, model = list(lambda0~session, \n                sigma~session), CL = TRUE, buffer = 25, \n                detectfn = 'HEX', trace = FALSE)\n\n\nfitsCL &lt;- secrlist(hybridxCL,xCL,sessionxCL)\npredict(fitsCL, all.levels = TRUE)\n\n$hybridxCL\n$hybridxCL$`session = coulombe, h2 = f`\n         link estimate SE.estimate     lcl     ucl\nlambda0   log  0.28819    0.041538 0.21758 0.38171\nsigma     log  2.17887    0.153222 1.89866 2.50043\npmix    logit  0.46619    0.043007 0.38363 0.55064\n\n$hybridxCL$`session = coulombe, h2 = m`\n         link estimate SE.estimate     lcl     ucl\nlambda0   log  0.20098    0.036023 0.14183 0.28478\nsigma     log  2.05123    0.173554 1.73830 2.42051\npmix    logit  0.53381    0.043007 0.44936 0.61637\n\n\n$xCL\n$xCL$`session = coulombe, sex = f`\n        link estimate SE.estimate     lcl     ucl\nlambda0  log  0.28819    0.041539 0.21758 0.38171\nsigma    log  2.17887    0.153222 1.89866 2.50043\n\n$xCL$`session = coulombe, sex = m`\n        link estimate SE.estimate     lcl     ucl\nlambda0  log  0.20098    0.036023 0.14183 0.28478\nsigma    log  2.05123    0.173555 1.73829 2.42051\n\n\n$sessionxCL\n$sessionxCL$`session = f`\n        link estimate SE.estimate     lcl     ucl\nlambda0  log  0.28818    0.041538 0.21758 0.38171\nsigma    log  2.17887    0.153223 1.89866 2.50044\n\n$sessionxCL$`session = m`\n        link estimate SE.estimate     lcl     ucl\nlambda0  log  0.20098    0.036023 0.14184 0.28478\nsigma    log  2.05123    0.173554 1.73829 2.42050\n\n\n\n# full likelihood\nhybridx  &lt;- secr.fit(morningx, model = list(lambda0~h2, sigma~h2), \n              hcov = 'sex', CL = FALSE, buffer = 25, detectfn = 'HEX', \n              trace = FALSE)\nsessionx &lt;- secr.fit(morningxs, model = list(D ~ session, \n              lambda0~session, sigma~session), CL = FALSE, buffer = 25,\n              detectfn = 'HEX', trace = FALSE)\n# suppress bias check that gives ignorable error in secr 5.2\ngroupx   &lt;- secr.fit(morningx, model = list(D ~ g, lambda0~g, sigma~g), \n              groups = 'sex', CL = FALSE, buffer = 25, detectfn = 'HEX', \n              biasLimit = NA, trace = FALSE)\n\n\nfits &lt;- secrlist(hybridx,sessionx, groupx)\npredict(fits, all.levels = TRUE)\n\n$hybridx\n$hybridx$`session = coulombe, h2 = f`\n         link   estimate SE.estimate        lcl        ucl\nD         log 1296.94081  112.480096 1094.55112 1536.75368\nlambda0   log    0.28819    0.041538    0.21758    0.38171\nsigma     log    2.17887    0.153221    1.89866    2.50043\npmix    logit    0.46619    0.043008    0.38363    0.55064\n\n$hybridx$`session = coulombe, h2 = m`\n         link   estimate SE.estimate        lcl        ucl\nD         log 1296.94081  112.480096 1094.55112 1536.75368\nlambda0   log    0.20098    0.036023    0.14183    0.28478\nsigma     log    2.05123    0.173553    1.73830    2.42051\npmix    logit    0.53381    0.043008    0.44936    0.61637\n\n\n$sessionx\n$sessionx$`session = f`\n        link  estimate SE.estimate       lcl       ucl\nD        log 604.61389   72.458365 478.44449 764.05511\nlambda0  log   0.28819    0.041538   0.21758   0.38171\nsigma    log   2.17887    0.153221   1.89866   2.50043\n\n$sessionx$`session = m`\n        link  estimate SE.estimate       lcl       ucl\nD        log 692.32722   86.800581 542.01062 884.33136\nlambda0  log   0.20098    0.036023   0.14184   0.28478\nsigma    log   2.05123    0.173551   1.73830   2.42050\n\n\n$groupx\n$groupx$`session = coulombe, g = f`\n        link  estimate SE.estimate       lcl       ucl\nD        log 604.61767   72.458613 478.44780 764.05939\nlambda0  log   0.28819    0.041538   0.21758   0.38171\nsigma    log   2.17886    0.153220   1.89866   2.50042\n\n$groupx$`session = coulombe, g = m`\n        link  estimate SE.estimate       lcl       ucl\nD        log 692.32687   86.800590 542.01026 884.33104\nlambda0  log   0.20098    0.036023   0.14183   0.28478\nsigma    log   2.05123    0.173552   1.73830   2.42050\n\n\nExecution time varies considerably:\n\n\nTiming, conditional likelihood\n\n\n hybridxCL.elapsed        xCL.elapsed sessionxCL.elapsed \n              6.75               3.63              20.06 \n\n\nTiming, full likelihood\n\n\n hybridx.elapsed sessionx.elapsed   groupx.elapsed \n            9.87            41.67             8.25",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Sex differences</span>"
    ]
  },
  {
    "objectID": "16-sex.html#sec-decisionchart",
    "href": "16-sex.html#sec-decisionchart",
    "title": "16  Sex differences",
    "section": "\n16.3 Choosing a model",
    "text": "16.3 Choosing a model\n\n16.3.1 Sex-specific detection\nWe suggest the decision chart in Fig. 16.1. This omits ‘session’ approaches that we previously included for comparison, because they are rather slow and clunky and group models are equivalent.\n\n\n\n\n\n\n\nFigure 16.1: Decision chart for including sex in detection model. `sexcov’ is a character value naming a 2-level character or factor individual covariate after some trial and error with out.width units etc.\n\n\n\n\n\n16.3.2 Sex-specific density\nFig. 16.1 is concerned solely with the detection model. Code for estimating sex-specific densities is available for each option as shown in Table 16.1. Sex ratio (pmix) is estimated directly only from hybrid mixture models. Post-hoc specification of ‘groups’ in derived works for both conditional and full likelihood models when density is constant. It is a common mistake to omit D~g or D~session from full-likelihood sex models – this forces a 1:1 sex ratio.\n\n\nTable 16.1: Sex-specific estimates of density from various models. Any detection parameter may precede ‘~’\n\n\n\n\n\n\n\n\nFitting method\nModel\nSex-specific density\n\n\n\nConditional likelihood\n\n~sex1\n\nderived(fit, groups = sexcov)\n\n\n\n~h2, hcov = sexcov\nderived(fit, groups = sexcov)\n\n\n\n~session\nderived(fit)\n\n\nFull likelihood\nD~g, ~g, groups = sexcov\n\npredict(fit))\n\n\n\n~h2, hcov = sexcov\n\nderived(fit, groups = sexcov)2\n\n\n\n\nD~session, ~session\npredict(fit)",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Sex differences</span>"
    ]
  },
  {
    "objectID": "16-sex.html#final-comments",
    "href": "16-sex.html#final-comments",
    "title": "16  Sex differences",
    "section": "\n16.4 Final comments",
    "text": "16.4 Final comments\nWe note\n\nOnly hybrid mixtures can cope with missing values of the sex covariate.\nThe implementation of groups is less comprehensive and may not be available for extensions in the Appendices.\nOptions should not be mixed for comparing model fit by AIC.\n\nSex differences in home-range size (and hence \\sigma) may be mitigated by compensatory variation in g_0 or \\lambda_0 (Efford & Mowat, 2014) (see also Mitigating factors in Chapter 15).\n\n\n\n\nFigure 16.1: Decision chart for including sex in detection model. `sexcov’ is a character value naming a 2-level character or factor individual covariate after some trial and error with out.width units etc.\n\n\n\nEfford, M. G., & Mowat, G. (2014). Compensatory heterogeneity in spatially explicit capture-recapture data. Ecology, 95, 1341–1348.\n\n\nKendeigh, S. C. (1944). Measurement of bird populations. Ecological Monographs, 14, 67–106. https://doi.org/10.2307/1961632",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Sex differences</span>"
    ]
  },
  {
    "objectID": "17-simulation.html",
    "href": "17-simulation.html",
    "title": "17  Simulation",
    "section": "",
    "text": "17.1 Simulation step by step\nThe essential steps for each replicate of a de novo SECR simulation are\nIt is simple enough to perform steps 1 & 2 directly in R without the use of secr functions. We give an example before describing functions that cover many variations with less code.\n# Step 1. Generate population\nD     &lt;- 20                               # AC per hectare\nEN    &lt;- D * 9                            # expected number in 9 ha\nN     &lt;- rpois(1, EN)                     # realised number of AC\npopxy &lt;- matrix(runif(2*N)*300, ncol = 2) # points in 300-m square\n# Step 2a. Sample from population with central 6 x 6 detector array\ndetxy &lt;- expand.grid(x = seq(100,200,20), y = seq(100,200,20))\nK     &lt;- nrow(detxy)                      # number of detectors\nS     &lt;- 5                                # number of occasions\ng0    &lt;- 0.2                              # detection parameter\nsigma &lt;- 20                               # detection parameter\nd     &lt;- secr::edist(popxy, detxy)        # distance to detectors\np     &lt;- g0 * exp(-d^2/2/sigma^2)         # probability detected\ndetected &lt;- as.numeric(runif(N*K*S) &lt; rep(p,S))\nch    &lt;- array(detected, dim = c(N,K,S), dimnames = list(1:N, 1:K, 1:S))\nch    &lt;- ch[apply(ch,1,sum)&gt;0,,]          # reject null histories\n\n# Step 2b. Cast simulated data as an secr 'capthist' object\nclass(detxy)    &lt;- c('traps', 'data.frame')\ndetector(detxy) &lt;- 'proximity'\nch              &lt;- aperm(ch, c(1,3,2))   # permute dim to N, S, K\nclass(ch)       &lt;- 'capthist'\ntraps(ch)       &lt;- detxy\nNow that the data are in the right shape we can proceed to fit a model:\n# Step 3. Fit model to simulated capthist\nfit &lt;- secr.fit(ch, buffer = 100, trace = FALSE)\npredict(fit)\n\n       link estimate SE.estimate      lcl      ucl\nD       log 18.53043      2.7611 13.85955 24.77548\ng0    logit  0.22101      0.0235  0.17838  0.27046\nsigma   log 20.11668      1.0532 18.15608 22.28900\nThis is rather laborious and it’s easy to slip up. We next outline the secr functions sim.popn and sim.capthist that conveniently wrap Steps 1 & 2, respectively, along with many extensions.\nA further level of wrapping is provided by package secrdesign that manages simulation scenarios, their replicated execution (Steps 1–3), and the summarisation of results.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "17-simulation.html#simulation-step-by-step",
    "href": "17-simulation.html#simulation-step-by-step",
    "title": "17  Simulation",
    "section": "",
    "text": "Generate an instance of the population process, i.e. the distribution of activity centres AC.\nGenerate a set of ‘observed’ spatial detection histories from the AC distribution according to some study design (detector layout, sampling duration), and detection parameters (e.g., \\lambda_0, \\sigma).\nFit an SECR model to estimate parameters of the population and detection processes, along with their sampling variances.\n\n\n\n\n\n\n\n\n\n17.1.1 Simulating AC distributions with sim.popn\n\n\nA simulated AC distribution is an object of class ‘popn’ in secr. This is usually a distribution of points within a rectangular region that is the bounding box of a ‘core’ object (most likely a ‘traps’ object) inflated by a certain ‘buffer’ distance in metres. sim.popn generates a ‘popn’ object, for which there is a plot method.\n\ntr &lt;- make.grid()  # a traps object, for demonstration\npop &lt;- sim.popn(D = 10, core = tr, buffer = 100, \n    model2D = \"poisson\")\npar(mar = c(1,1,1,1))\nplot(pop)\nplot(tr, add = TRUE)\n\n\n\n\n\n\nFigure 17.1\n\n\n\n\nThe default 2-D distribution is random uniform - a Poisson distribution, as shown here. The expected number of activity centres E(NA) is determined by the argument D, the density in AC per hectare. By default NA is a Poisson random variable, but it may be fixed by setting Ndist = \"fixed\". Published simulations often fix NA for reasons of convenience such as avoiding realisations that may be hard to model or reducing the variance among replicates. Comparisons must therefore be made with care.\nFor Ndist = \"poisson\" the choice of buffer is not critical so long as it is large enough to include all potentially detected AC. Simulated populations with excessively large NA take longer to sample (the next step - generating a capthist object), but the ultimate capthist is no larger and model fitting takes the same time.\nInhomogeneous alternatives to a uniform random distribution may be specified using the arguments ‘model2D’ and ‘details’. We elaborate on these later.\n\n17.1.2 Simulating detection with sim.capthist\n\n\nTo simulate sampling of a given population we specify the type and spatial distribution of detectors, the detection function, and the duration of sampling. Detectors are prepared in advance as an object of class ‘traps’, either input from a data file or generated by one of the functions for a specific geometry (make.grid, trap.builder, make.systematic, make.circle etc.).\n\nch &lt;- sim.capthist(traps = tr, popn = pop, detectpar = list(\n    g0 = 0.1, sigma = 20), noccasions = 5, renumber = FALSE)\nsummary(ch)\n\nObject class       capthist \nDetector type      multi (5) \nDetector number    36 \nAverage spacing    20 m \nx-range            0 100 m \ny-range            0 100 m \n\nCounts by occasion \n                   1  2  3  4  5 Total\nn                 10  7  9  5  7    38\nu                 10  7  0  0  1    18\nf                  5  8  3  2  0    18\nM(t+1)            10 17 17 17 18    18\nlosses             0  0  0  0  0     0\ndetections        10  7  9  5  7    38\ndetectors visited 10  7  7  5  6    35\ndetectors used    36 36 36 36 36   180\n\nIndividual covariates\n sex  \n F:9  \n M:9  \n\npar(mar = c(1,1,1,1))\nplot(ch, tracks = TRUE, rad = 3)\nplot(pop, add = TRUE)\ncaptured &lt;- subset(pop, rownames(pop) %in% rownames(ch))\nplot(captured, pch = 16, add = TRUE)\n\n\n\n\n\n\nFigure 17.2\n\n\n\n\n\n17.1.3 Combining data generation and model fitting\nWe loop over the AC-generation, sampling, and model-fitting steps, recording the results for later.\n\ntr &lt;- make.grid()  # a traps object\nnrepl &lt;- 10\nset.seed(123)\nout &lt;- list()\nfor (r in 1:nrepl) {\n    pop &lt;- sim.popn(D = 20, core = tr, buffer = 100, \n        model2D = \"poisson\")\n    ch &lt;- sim.capthist(traps = tr, popn = pop, detectpar = list(\n        g0 = 0.2, sigma = 20), noccasions = 5)\n    fit &lt;- secr.fit(ch, buffer=100, trace = FALSE)\n    out[[r]] &lt;- predict(fit)\n}\n\nEach output returned by the predict method is a dataframe from which we extract the density estimate and its standard error to form the summary:\n\ntrueD  &lt;- 20\nDhat   &lt;- sapply(out, '[', 'D', 'estimate')\nseDhat &lt;- sapply(out, '[', 'D', 'SE.estimate')\ncat (\"mean  \", mean(Dhat), '\\n')\ncat (\"RB    \", (mean(Dhat)-trueD) / trueD, '\\n')\ncat (\"RSE   \", mean(seDhat / Dhat), '\\n')\ncat (\"rRMSE \", sqrt(mean((Dhat-trueD)^2)) / mean(Dhat), '\\n')\n\nmean   20.52 \nRB     0.025987 \nRSE    0.15481 \nrRMSE  0.14281",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "17-simulation.html#sec-secrdesign",
    "href": "17-simulation.html#sec-secrdesign",
    "title": "17  Simulation",
    "section": "\n17.2 Simulation in secrdesign\n",
    "text": "17.2 Simulation in secrdesign\n\n \nThe R package secrdesign takes care of a lot of the coding needed to specify, execute and summarise SECR simulations. Full details are in its documentation, particularly secrdesign-vignette.pdf.\nThe heart of secrdesign is a dataframe of one or more scenarios. Usually there is one row per scenario. The scenario dataframe is constructed with make.scenarios. Some properties of scenarios, such as expected sample sizes, may be extracted without simulation using scenarioSummary. This is a good first check.\nSimulations are executed with run.scenarios, with or without model fitting. The resulting object is summarised either directly (with the summary method) or after further processing to select fields and statistics. Several arguments of run.scenarios comprise lookup lists from which each scenario selects according to a numeric index field. For example, ‘trapset’ is a list of detector layouts, from which each scenario selects via its ‘trapsindex’ column. Likewise ‘pop.args’, ‘det.args’, and ‘fit.args’ are lists corresponding to the ‘popindex’, ‘detindex’ and ‘fitindex’ columns of the scenario data.frame. These optionally provide fine control of sim.popn, sim.capthist and secr.fit, respectively. The saved output may be customized via the ‘extractfn’ argument.\nThis small demonstration repeats the simulation in the preceding code. ‘xsigma’ specifies the buffer width as a multiple of ‘sigma’.\n\n\nWarning: package 'secrdesign' was built under R version 4.4.1\n\n\n\nlibrary(secrdesign)\ntr   &lt;- make.grid()  \nscen &lt;- make.scenarios (D = 20, g0 = 0.2, sigma = 20, \n    noccasions = 5)\nsims &lt;- run.scenarios(10, scen, tr, xsigma = 5, fit = TRUE, \n    seed = 123)    \n## Completed scenario 1\n## Completed in 0.044 minutes\n\nWe can get a compact summary like this:\n\nestimateSummary(sims)\n\n  scenario true.D nvalid    EST  seEST       RB     seRB     RSE   RMSE   rRMSE COV\n1        1     20     10 20.511 1.2833 0.025526 0.064167 0.15498 3.8837 0.19419 0.9\n\n\nThe default sampling function sim.capthist is usually adequate, but an alternative may be specified via the argument ‘CH.function’.\nSimulation quite often results in data that are too sparse for analysis, and other malfunctions are possible. As a user you should be on your guard for meaningless, extreme values in the estimates. The validate function provides a mechanism for filtering these out.",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "17-simulation.html#advanced-simulations",
    "href": "17-simulation.html#advanced-simulations",
    "title": "17  Simulation",
    "section": "\n17.3 Advanced simulations",
    "text": "17.3 Advanced simulations\n\n17.3.1 Inhomogeneous populations\nVariations on a uniform distribution of AC are selected with the sim.popn argument ‘model2D’. This may be done directly, as when we first introduced sim.popn, or indirectly via the ‘pop.args’ argument of run.scenarios. While almost any process may be used to generate data, only the parameters of an inhomogeneous Poisson process may be estimated by fitting models in secr. Alternative methods are available for some other generating models (Dey et al., 2023; e.g., Stevenson et al., 2021).\nWe next consider three types of point process that may be used to simulate inhomogeneous distributions of activity centres in sim.popn.\n\n17.3.1.1 Inhomogeneous Poisson Process\nAn inhomogeneous Poisson process (IHPP) is specified by setting ‘model2D = “IHP”’. The argument ‘core’ should be a habitat mask. Cell-specific density (expected number of individuals per hectare) may be given either in a covariate of the mask (named in argument ‘D’) or as a vector of values in argument ‘D’. The covariate option allows you to simulate from a previously fitted Dsurface (output from predictDsurface). Special cases of IHPP are provided in the ‘hills’ and ‘coastal’ options for ‘model2D’.\n\n17.3.1.2 Cox Processes\nIn a Cox process the expected density surface of the IHPP varies randomly between realisations (i.e. replicates). For example, secr has the function randomDensity that may be used in sim.popn to generate a new binary density mosaic at each call (see Examples on its help page).\nA flexible model for continuous variation in density is the log-Gaussian Cox Process (LGCP) (Dupont et al., 2021; Efford & Fletcher, 2024; Johnson et al., 2010). The IHPP log-intensity surface of a LGCP is Gaussian (normally distributed) at each point, but adjacent points co-vary; autocorrelation declines with distance. The notation and parameterisation can be confusing. The term ‘Gaussian’ refers to the marginal intensity, not the autocorrelation function which is commonly exponential. The Gaussian variance is on the log scale, so even a variance as low as 1.0 indicates substantial heterogeneity. The exponential scale parameter naturally has the same units as the SECR detection function, although Dupont et al. (2021) rescaled it as 6% or 100% of the width of the study area. secr uses the function rLGCP from spatstat (Baddeley et al., 2015) which in turn depends on RandomFields (Schlather et al., 2015).\n\n\n\n\n\n\n17.3.1.3 Cluster Processes\nThe points in a 2-D cluster process are no longer independent, as in an IHPP. Each belongs to a cluster. For the Thomas process that has been applied in SECR [e.g., ebb09] there is a Poisson distribution of ‘parents’ and a bivariate-normal scatter of offspring points about each parent. Both the number of parents and the number of offspring per parent are Poisson variables, and hence some clusters may be empty. Thomas processes belong to the broader class of Neyman-Scott cluster processes. secr implements an interface to the function rThomas from spatstat (Baddeley et al., 2015) (‘model2D = “rThomas”’).\n\n17.3.2 Summary of inhomogeneous options\nBoth Cox and cluster processes require additional parameters to be specified in the ‘details’ argument of sim.popn (Table 17.1). Fig. 17.3 provides examples.\n\n\nTable 17.1: Distribution of activity centres in sim.popn controlled by argument ‘model2D’ with additional parameters specified in the ‘details’ argument\n\n\n\nType\nmodel2D\ndetails\nNote\n\n\n\nUniform\n“poisson”\n—\n\n\n\nIHPP\n“IHP”\n—\ncell densities pre-computed\n\n\n\n“hills”\nhills\nsine-curve density in 2-D\n\n\n\n“coastal”\nBeta\nextreme gradients cf Fewster & Buckland (2004)\n\n\n\nCox process\n“rLGCP”\nvar, scale\n\n\n\nCluster process\n“rThomas”\nmu, scale\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 17.3: Examples of AC distributions simulated with sim.popn. Yellow dots indicate location of cluster ‘parents’, some of which lie outside the frame.\n\n\n\n\n\n17.3.3 Spatial repulsion\nWe can imagine scenarios in which AC are distributed more evenly than expected from a random uniform distribution, perhaps due to territorial behaviour. A point process model with repulsion between AC was considered by Reich & Gardner (2014). There is no equivalent model in sim.popn. However, the option ‘model2D = “even”’ goes some way towards it: space is divided into notional square grid cells of area 1/D and one AC is located randomly within each cell (Fig. 17.3).\n\n17.3.4 Inhomogeneous detection\nSimulating spatial variation in detection parameters is a minefield: there are many subtly different approaches and ad hoc variations (Dey et al., 2023; Efford, 2014; Hooten et al., 2024; Moqanaki et al., 2021; Royle et al., 2013; Stevenson et al., 2021).\nThe first question is whether to focus on the location of the detector or the location of the activity centres. We label these approaches ‘detector-centric’ and ‘AC-centric’. Focusing on detectors is simpler as they comprise a few discrete locations whereas AC are at unknown locations in continuous space.\nsim.capthist allows values of g0 and lambda0 to be detector-specific. Values are provided as an occasion x detector matrix.\nThe authors cited at the start of this section were mainly concerned with [detector-centric] spatially auto-correlated random effects (‘SARE’, following Dey et al. (2023)). Random variation among detectors may be modelled on the link scale by a Gaussian random field. Simulation is simpler than the LGCP for AC because values are required only at the K detectors. Deviates are obtained from a random K-dimensional multivariate normal distribution with distance-dependent covariance matrix (e.g., Moqanaki et al., 2021). Simulation in secr uses custom functions, for which there are several examples on GitHub with comments on parameterisation.\nA purely detector-centric approach does not allow for a model in which the activity of an individual at one point in its home range depends on resource availability both there and at other points. Normalisation of activity must then account for resource distribution in continuous space (see Efford, 2014 and the preceding GitHub site). Moqanaki et al. (2021) transformed to uniform (-1.96, 1.96)\nThere are more arcane options for simulating AC-centric variation in detection:\n\ncovariates ‘lambda0’ and ‘sigma’ may be added to simulated populations on the basis of the x-y coordinates of the AC, and used in sim.capthist by setting ‘detectpar = list(individual = TRUE)’ (requires secr \\ge 4.6.7)\nthe ‘userdist’ argument may be used to ‘warp’ space and hence the effective sigma (cf Appendix F)\ndiscrete non-overlapping populations may be simulated and pasted together post hoc.\n\n\n\n\nFigure 17.1: \nFigure 17.2: \nFigure 17.3: Examples of AC distributions simulated with sim.popn. Yellow dots indicate location of cluster ‘parents’, some of which lie outside the frame.\n\n\n\nBaddeley, A., Rubak, E., & Turner, R. (2015). Spatial point patterns. Chapman; Hall/CRC. https://doi.org/10.1201/b19708\n\n\nDey, S., Moqanaki, E., Milleret, C., Dupont, P., Tourani, M., & Bischof, R. (2023). Modelling spatially autocorrelated detection probabilities in spatial capture-recapture using random effects. Ecological Modelling, 479, 110324. https://doi.org/10.1016/j.ecolmodel.2023.110324\n\n\nDupont, G., Royle, J. A., Nawaz, M. A., & Sutherland, C. (2021). Optimal sampling design for spatial capture-recapture. Ecology, 102, e03262.\n\n\nEfford, M. G. (2014). Bias from heterogeneous usage of space in spatially explicit capture-recapture analyses. Methods in Ecology and Evolution, 5, 599–602.\n\n\nEfford, M. G. (2023). ipsecr: An R package for awkward spatial capture–recapture data. Methods in Ecology and Evolution, 14(5), 1182–1189. https://doi.org/10.1111/2041-210x.14088\n\n\nEfford, M. G., & Fletcher, D. (2024). Effect of spatial overdispersion on confidence intervals for population density estimated by spatial capture-recapture. bioRxiv. https://doi.org/10.1101/2024.03.12.584742\n\n\nFewster, R. M., & Buckland, S. T. (2004). Assessment of distance sampling estimators. In S. T. Buckland, D. R. Anderson, K. P. Burnham, J. L. Laake, D. L. Borchers, & L. Thomas (Eds.), Advanced distance sampling (pp. 281–306). Oxford University Press.\n\n\nHooten, M. B., Schwob, M. R., Johnson, D. S., & Ivan, J. S. (2024). Geostatistical capture–recapture models. Spatial Statistics, 59, 100817. https://doi.org/10.1016/j.spasta.2024.100817\n\n\nJohnson, D. S., Laake, J. L., & Ver Hoef, J. M. (2010). A model‐based approach for making ecological inference from distance sampling data. Biometrics, 66(1), 310–318. https://doi.org/10.1111/j.1541-0420.2009.01265.x\n\n\nKendeigh, S. C. (1944). Measurement of bird populations. Ecological Monographs, 14, 67–106. https://doi.org/10.2307/1961632\n\n\nMoqanaki, E. M., Milleret, C., Tourani, M., Dupont, P., & Bischof, R. (2021). Consequences of ignoring variable and spatially autocorrelated detection probability in spatial capture-recapture. Landscape Ecology, 36(10), 2879–2895. https://doi.org/10.1007/s10980-021-01283-x\n\n\nReich, B. J., & Gardner, B. (2014). A spatial capture‐recapture model for territorial species. Environmetrics, 25(8), 630–637. https://doi.org/10.1002/env.2317\n\n\nRoyle, J. A., Chandler, R. B., Sun, C. C., & Fuller, A. K. (2013). Integrating resource selection information with spatial capture–recapture. Methods in Ecology and Evolution, 4(6), 520–530. https://doi.org/10.1111/2041-210x.12039\n\n\nSchlather, M., Malinowski, A., Menck, P. J., Oesting, M., & Strokorb, K. (2015). Analysis, simulation and prediction of multivariate random fields with package RandomFields. Journal of Statistical Software, 63(8). https://doi.org/10.18637/jss.v063.i08\n\n\nStevenson, B. C., Fewster, R. M., & Sharma, K. (2021). Spatial correlation structures for detections of individuals in spatial capture–recapture models. Biometrics, 78(3), 963–973. https://doi.org/10.1111/biom.13502",
    "crumbs": [
      "Practice",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Simulation</span>"
    ]
  },
  {
    "objectID": "A01-troubleshooting.html",
    "href": "A01-troubleshooting.html",
    "title": "Appendix A — Troubleshooting secr",
    "section": "",
    "text": "A.1 Bias limit exceeded\nSuppose we omitted to specify the buffer argument for the first snowshoe hare model in Chapter 2:\nfit &lt;- secr.fit (hareCH6, trace = FALSE)\n\nWarning: using default buffer width 100 m\n\n\nWarning: predicted relative bias exceeds 0.01 with buffer = 100\nThe second warning message is clearly a consequence of the first: relying on the 100-m default buffer for an animal as mobile as the snowshoe hare is likely to cause problems. See the advice on choosing the buffer width in Chapter 12.\nDensity is overestimated when the buffer width for a detector array in continuous habitat is too small to encompass the centres of all animals potentially detected. A check for this ‘mask truncation bias’ is performed routinely by secr.fit after fitting a model that uses on the ‘buffer’ argument. It may be avoided by setting biasLimit = NA or providing a pre-computed habitat mask in the ‘mask’ argument.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Troubleshooting **secr**</span>"
    ]
  },
  {
    "objectID": "A01-troubleshooting.html#initial-log-likelihood-na",
    "href": "A01-troubleshooting.html#initial-log-likelihood-na",
    "title": "Appendix A — Troubleshooting secr",
    "section": "\nA.2 Initial log likelihood NA",
    "text": "A.2 Initial log likelihood NA\n\nMaximization will fail completely if the likelihood cannot be evaluated at the starting values. This will be obvious with trace = TRUE. Otherwise, the first indication will be a premature end to fitting and a lot of NAs in the estimates.\n\n\n\n\n\n\nTip\n\n\n\nIt is common to see occasional NA values among the likelihoods evaluated during maximization. This is not a problem.\n\n\nFor an example, this section previously used the default starting values for the dataset infraCH (Oligosoma infrapunctatum skinks sampled with pitfall traps over two 3-occasion sessions labelled ‘6’ and ‘7’). Unfortunately, those now seem to work, so we have to contrive an example by specifying a bad starting value for sigma:\n\nfit &lt;- secr.fit(infraCH, buffer = 25, start = list(sigma = 2), \n                trace = TRUE)\n\nChecking data \nPreparing detection design matrices \nPreparing density design matrix \nFinding initial parameter values... \nInitial values  D = 282.348, g0 = 0.12788, sigma = 2.87777 \nMaximizing likelihood... \nEval     Loglik        D       g0    sigma \n   1         NA   5.6431  -1.9198   0.6931 \n   2         NA   5.6431  -1.9198   0.6931 \n   3         NA   5.6431  -1.9198   0.6931 \n   4         NA   5.6431  -1.9198   0.6931 \n   5         NA   5.6431  -1.9198   0.6931 \n   6         NA   5.6437  -1.9198   0.6931 \n   7         NA   5.6431  -1.9197   0.6931 \n   8         NA   5.6431  -1.9198   0.6932 \n   9         NA   5.6443  -1.9198   0.6931 \n  10         NA   5.6437  -1.9197   0.6931 \n  11         NA   5.6437  -1.9198   0.6932 \n  12         NA   5.6431  -1.9196   0.6931 \n  13         NA   5.6431  -1.9197   0.6932 \n  14         NA   5.6431  -1.9198   0.6933 \nCompleted in 4.19 seconds at 18:17:36 04 Feb 2025 \n\n\nUnsurprisingly, the problem can be addressed by manually providing a better starting value for sigma:\n\nfit1 &lt;- secr.fit(infraCH, buffer = 25, start = list(sigma = 5), \n                 trace = FALSE)\n\nThe original problem seems to have been due to a discrepancy between the two sessions (try RPSV(infraCH, CC = TRUE)). The default sigma was suitable for the first session and not the second, whereas a larger sigma suits both. Rather than manually providing the starting value we could have directed secr.fit to use the second session for determining starting values:\n\nfit2 &lt;- secr.fit(infraCH, buffer = 25, details = list(autoini = 2),\n                 trace = FALSE)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Troubleshooting **secr**</span>"
    ]
  },
  {
    "objectID": "A01-troubleshooting.html#sec-variancefailed",
    "href": "A01-troubleshooting.html#sec-variancefailed",
    "title": "Appendix A — Troubleshooting secr",
    "section": "\nA.3 Variance calculation failed",
    "text": "A.3 Variance calculation failed\n\nIf warnings had not been suppressed in the preceding example we would have seen\n\n\nWarning in secr.fit(infraCH, buffer = 25, details = list(autoini = 2), trace =\nFALSE): at least one variance calculation failed\n\n\n\nExamination of the output would reveal missing values for SE, lcl and ucl in both the coefficients and predicted values for g0.\nFailure to compute the variance can be a sign that the model is inherently non-identifiable or that the particular dataset is inadequate (e.g., Gimenez et al., 2004). However, here it is due to a weakness in the default algorithm. Call secr.fit with method = \"none\" to recompute the variance-covariance matrix using a more sophisticated approach:\n\nfit2r &lt;- secr.fit(infraCH, buffer = 25, start = fit2, method = \"none\")\n\nChecking data \nPreparing detection design matrices \nPreparing density design matrix \nComputing Hessian with fdHess in nlme \n   1  -2260.348   5.5124  -2.2366   1.4921 \n   2  -2260.355   5.5179  -2.2366   1.4921 \n   3  -2260.348   5.5124  -2.2344   1.4921 \n   4  -2260.351   5.5124  -2.2366   1.4936 \n   5  -2260.355   5.5069  -2.2366   1.4921 \n   6  -2260.350   5.5124  -2.2388   1.4921 \n   7  -2260.350   5.5124  -2.2366   1.4906 \n   8  -2260.357   5.5179  -2.2344   1.4921 \n   9  -2260.361   5.5179  -2.2366   1.4936 \n  10  -2260.353   5.5124  -2.2344   1.4936 \n  11  -2260.348   5.5124  -2.2366   1.4921 \nCompleted in 3.19 seconds at 18:19:07 04 Feb 2025 \n\npredict(fit2r)\n\n$`session = 6`\n       link   estimate SE.estimate        lcl       ucl\nD       log 247.736992  13.9705898 221.833584 276.66513\ng0    logit   0.096511   0.0083617   0.081319   0.11419\nsigma   log   4.446466   0.1603473   4.143136   4.77200\n\n$`session = 7`\n       link   estimate SE.estimate        lcl       ucl\nD       log 247.736992  13.9705898 221.833584 276.66513\ng0    logit   0.096511   0.0083617   0.081319   0.11419\nsigma   log   4.446466   0.1603473   4.143136   4.77200\n\n\nThe trapping sessions were only 4 weeks apart in spring 1995. We can further investigate session differences by fitting a session-specific model. The fastest way to fit fully session-specific models is to fit each session separately; lapply here applies secr.fit separately to each component of infraCH:\n\nfits3 &lt;- lapply(infraCH, secr.fit, buffer = 25, trace = FALSE)\nclass(fits3) &lt;- \"secrlist\"  # ensure secr will recognise the fitted models\npredict(fits3)\n\n$`6`\n       link  estimate SE.estimate      lcl       ucl\nD       log 255.75743   28.377392 205.9077 317.67570\ng0    logit   0.17127    0.029904   0.1203   0.23799\nsigma   log   2.51903    0.181296   2.1880   2.90012\n\n$`7`\n       link   estimate SE.estimate        lcl       ucl\nD       log 278.011710   18.823824 243.497884 317.41759\ng0    logit   0.098947    0.011107   0.079208   0.12295\nsigma   log   4.951468    0.218378   4.541623   5.39830\n\n\nNotice that there is no issue with starting values when the sessions are treated separately. The skinks appeared to enlarge their home ranges as the weather warmed; they may also have become more active overall1. It is plausible that density did not change: the estimate increased slightly, but there is substantial overlap of confidence intervals.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Troubleshooting **secr**</span>"
    ]
  },
  {
    "objectID": "A01-troubleshooting.html#log-likelihood-becomes-na-or-improbably-large-after-a-few-evaluations",
    "href": "A01-troubleshooting.html#log-likelihood-becomes-na-or-improbably-large-after-a-few-evaluations",
    "title": "Appendix A — Troubleshooting secr",
    "section": "\nA.4 Log likelihood becomes NA or improbably large after a few evaluations",
    "text": "A.4 Log likelihood becomes NA or improbably large after a few evaluations\nThe default maximization method (Newton-Raphson in function nlm) takes a large step away from the initial values at evaluation np + 3 where np is the number of estimated coefficients. This often results in a very negative or NA log likelihood, from which the algorithm quickly returns to a reasonable part of the parameter space. However, for some problems it does not return and estimation fails, possibly with a weird message about infinite density. Two solutions are suggested:\n\nchange to the more robust Nelder-Mead maximization algorithm\n\n\nsecr.fit(CH, method = \"Nelder-Mead\", ...)\n\n\nvary the scaling of each parameter in nlm by passing the typsize (typical size) argument. The default is typsize = rep(1, np). Suppose your model has four coefficients and it is the second one that appears to be behaving wildly:\n\n\nsecr.fit(CH, typsize = c(1, 0.1, 1, 1), ...)\n\nIn these examples CH is your capthist object and ... indicates other arguments of secr.fit.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Troubleshooting **secr**</span>"
    ]
  },
  {
    "objectID": "A01-troubleshooting.html#possible-maximization-error-nlm-code-3",
    "href": "A01-troubleshooting.html#possible-maximization-error-nlm-code-3",
    "title": "Appendix A — Troubleshooting secr",
    "section": "\nA.5 Possible maximization error: nlm code 3",
    "text": "A.5 Possible maximization error: nlm code 3\n\nThe default algorithm for numerical maximization of the likelihood is nlm in the base R stats package. That uses a Newton-Raphson algorithm and numerical derivatives. It was chosen because it is significantly faster than the alternatives. However, it sometimes returns estimates with the ambiguous result code 3, which means that the optimization process terminated because “[the] last global step failed to locate a point lower than estimate. Either estimate is an approximate local minimum of the function or steptol is too small.”\nHere is an example:\n\nfit3 &lt;- secr.fit(infraCH, buffer = 25, model = list(g0~session, \n    sigma~session), details = list(autoini = 2), trace = FALSE)\n\n\nThe results seem usually to be reliable even when this warning is given. If you are nervous, you can try a different algorithm in secr.fit – “Nelder-Mead” is recommended. We can derive starting values from the earlier fit:\n\nfit3nm &lt;- secr.fit(infraCH, buffer = 25, model = list(g0~session, \n    sigma~session), method = \"Nelder-Mead\", start = fit3, trace = FALSE)\n\nThere is no warning. Comparing the density estimates we see a trivial difference in the SE and confidence limits and none at all in the estimates themselves:\n\ncollate(fit3, fit3nm)[1,,,'D']\n\n       estimate SE.estimate    lcl    ucl\nfit3     271.94      15.863 242.58 304.85\nfit3nm   271.94      15.809 242.68 304.73\n\n\nThis suggests that only the variance-covariance estimates were in doubt, and it would have been much quicker merely to check them with method = \"none\" as in the previous section.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Troubleshooting **secr**</span>"
    ]
  },
  {
    "objectID": "A01-troubleshooting.html#possible-maximization-error-nlm-code-4",
    "href": "A01-troubleshooting.html#possible-maximization-error-nlm-code-4",
    "title": "Appendix A — Troubleshooting secr",
    "section": "\nA.6 Possible maximization error: nlm code 4",
    "text": "A.6 Possible maximization error: nlm code 4\n\nThe nlm Newton-Raphson algorithm may also finish with the result code 4, which means that the optimization process terminated when the maximum number of iterations was reached (“iteration limit exceeded”). The maximum is set by the argument iterlim which defaults to 100 (each ‘iteration’ uses several likelihood evaluations to numerically determine the gradient for the Newton-Raphson algorithm).\nThe number of iterations can be checked retrospectively by examining the nlm output saved in the ‘fit’ component of the fitted model. Ordinarily nlm uses less than 50 iterations (for example fit3$fit$iterations = 26).\nA ‘brute force’ solution is to increase iterlim (secr.fit() passes iterlim directly to nlm()) or to re-fit the model starting at the previous solution (start = oldfit). This is not guaranteed to work. Alternative algorithms such as method = 'Nelder-Mead' are worth trying, but they may struggle also.\nThere does not appear to be a universal solution. Slow or poor fitting seems more common when there are many beta parameters, and when one or more parameters is very imprecise, at a boundary, or simply unidentifiable. It is suggested that you examine the coefficients of the provisional result with coef(fit) and seek to eliminate those that are not identifiable.\nTricks include:\n\ncombining levels of poorly supported factor covariates\nfixing the value of non-identifiable beta parameters with details argument fixedbeta\n\nensuring that all levels of a factor x factor interaction are represented in the data (possibly by defining a single factor with valid levels)\nchanging the coding of factor covariates with details argument contrasts.\n\nThe following code demonstrates fixing a beta parameter, although it is neither needed nor recommended in this case.\n\ncode to fix beta# review the fitted values\ncoef(fit3)\n\n                   beta  SE.beta      lcl      ucl\nD               5.60558 0.058282  5.49135  5.71981\ng0             -1.62710 0.197141 -2.01349 -1.24071\ng0.session7    -0.57521 0.225301 -1.01679 -0.13363\nsigma           0.91986 0.071333  0.78005  1.05967\nsigma.session7  0.68197 0.082388  0.52049  0.84345\n\ncode to fix beta# extract the coefficients\nbetafix &lt;- coef(fit3)$beta\n# set first 4 values to NA as we want to estimate these\nbetafix[1:4] &lt;- NA\nbetafix\n\n[1]      NA      NA      NA      NA 0.68197\n\ncode to fix beta# refit, holding last coefficient constant\nfit3a &lt;- secr.fit(infraCH, buffer = 25, model = list(g0~session, sigma~session),\n         details = list(autoini = 2, fixedbeta = betafix), trace = FALSE)\ncoef(fit3a)\n\n                beta  SE.beta      lcl     ucl\nD            5.60565 0.058123  5.49173  5.7196\ng0          -1.62716 0.137059 -1.89579 -1.3585\ng0.session7 -0.57523 0.129459 -0.82897 -0.3215\nsigma        0.91984 0.034977  0.85129  0.9884\n\n\nNote that the estimated coefficients (‘beta’) have not changed, but the estimated ‘SE.beta’ of each detection parameter has dropped - a result of our spurious claim to know the true value of ‘sigma.session7’.\nThere is no direct mechanism for holding the beta parameters for different levels of a factor (e.g., session) at a single value. The effect can be achieved by defining a new factor covariate with combined levels.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Troubleshooting **secr**</span>"
    ]
  },
  {
    "objectID": "A01-troubleshooting.html#secr.fit-requests-more-memory-than-is-available",
    "href": "A01-troubleshooting.html#secr.fit-requests-more-memory-than-is-available",
    "title": "Appendix A — Troubleshooting secr",
    "section": "\nA.7 secr.fit requests more memory than is available",
    "text": "A.7 secr.fit requests more memory than is available\n\nIn secr 5.2 the memory required by the external C code is at least 32 \\times C \\times M \\times K bytes, where C is the number of distinct sets of values for the detection parameters (across all individuals, occasions, detectors and finite mixture classes), M is the number of points in the habitat mask and K is the number of detectors. Each distinct set of values appears as a row in a lookup table2 whose columns correspond to real parameters; a separate parameter index array (PIA) has entries that point to rows in the lookup table. Four arrays with dimension C \\times M \\times K are pre-filled with, for example, the double-precision (8-byte) probability an animal in mask cell m is caught in detector k under parameter values c.\nThe number of distinct parameter sets C can become large when any real parameter (g0, lambda0, sigma) is modelled as a function of continuous covariates, because each unit (individual, detector, occasion) potentially has a unique level of the parameter. A rough calculation may be made of the maximum size of C for a given amount of available RAM. Given say 6GB of available RAM, K = 200 traps, and M = 4000 mask cells, C should be substantially less than 6e9 / 200 / 4000 / 32 \\approx 234. Allowance must be made for other memory allocations; this is simply the largest.\nThere is a different lookup table for each session; the limiting C is for the most complex session. The memory constraint concerns detection parameters only.\nMost analyses can be configured to bring the memory request down to a reasonable number.\n\n\nC may be reduced by replacing each continuous covariate with one using a small number of discrete levels (e.g. the mid-points of weight classes). For example, weightclass &lt;- 10 * trunc(weight/10) + 5 for midpoints of 10-g classes.\n\nM can be reduced by building a habitat mask with an appropriate spacing (see [secr-habitatmasks.pdf]).\n\nK might seem to be fixed by the design, but in extreme cases it may be appropriate to combine data from adjacent detectors (see Collapsing detectors).\n\nThe mash function (see Mashing) may be used to reduce the number of detectors when the design uses many identical and independent clusters. Otherwise, apply your ingenuity to simplify your model, e.g., by casting ‘groups’ as ‘sessions’. Memory is less often an issue on 64-bit systems (see also ?\"Memory-limits\").",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Troubleshooting **secr**</span>"
    ]
  },
  {
    "objectID": "A01-troubleshooting.html#covariate-factor-levels-differ-between-sessions",
    "href": "A01-troubleshooting.html#covariate-factor-levels-differ-between-sessions",
    "title": "Appendix A — Troubleshooting secr",
    "section": "\nA.8 Covariate factor levels differ between sessions",
    "text": "A.8 Covariate factor levels differ between sessions\n\nThis is fairly explicit; secr.fit will stop if you include in a model any covariate whose factor levels vary among sessions, and verify will warn if it finds any covariate like this. This commonly occurs in multi-session datasets with ‘sex’ as an individual covariate when only males or only females are detected in one session. Use the function shareFactorLevels to force covariates to use the same superset of levels in all sessions.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Troubleshooting **secr**</span>"
    ]
  },
  {
    "objectID": "A01-troubleshooting.html#estimates-depend-on-starting-values",
    "href": "A01-troubleshooting.html#estimates-depend-on-starting-values",
    "title": "Appendix A — Troubleshooting secr",
    "section": "\nA.9 Estimates depend on starting values",
    "text": "A.9 Estimates depend on starting values\nInstability of the estimates can result when the likelihood surface has a local maximum and is said to be ‘multimodal’. Numerical maximization may then fail to find the true maximum from a given starting point. Nelder-Mead is more robust than other methods.\nFinite mixture models have known problems due to multimodality of the likelihood, as discussed separately (Chapter 15). See Dawson & Efford (2009) and the vignette secr-sound.pdf for another example of a multimodal likelihood in SECR.\n\n\n\n\nDawson, D. K., & Efford, M. G. (2009). Bird population density estimated from acoustic signals. Journal of Applied Ecology, 46, 1201–1209.\n\n\nGimenez, O., Viallefont, A., Catchpole, E. A., Choquet, R., & Morgan, B. J. T. (2004). Methods for investigating parameter redundancy. Animal Biodiversity and Conservation, 27, 561–572.\n\n\nKendeigh, S. C. (1944). Measurement of bird populations. Ecological Monographs, 14, 67–106. https://doi.org/10.2307/1961632",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Troubleshooting **secr**</span>"
    ]
  },
  {
    "objectID": "A01-troubleshooting.html#footnotes",
    "href": "A01-troubleshooting.html#footnotes",
    "title": "Appendix A — Troubleshooting secr",
    "section": "",
    "text": "Home-range area increased about 4-fold; g0 showed some compensatory decrease, but compensation was incomplete, implying increased total activity (treating g_0 as an approximation to \\lambda_0; see Efford and Mowat 2014).↩︎\nYou can see this table by running secr.fit with details = list(debug = 3) and typing Xrealparval at the browser prompt (type Q to exit).↩︎",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Troubleshooting **secr**</span>"
    ]
  },
  {
    "objectID": "A02-speed.html",
    "href": "A02-speed.html",
    "title": "Appendix B — Faster is better",
    "section": "",
    "text": "B.1 Mask tuning\nConsider carefully the necessary extent of your habitat mask and the acceptable cell size (Chapter 12 has detailed advice). If your detectors are clustered then your mask may have gaps between the clusters (use type = ‘trapbuffer’ in make.mask). Masks with more than 2000 points are generally excessive (and the default is about 4000!).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Faster is better</span>"
    ]
  },
  {
    "objectID": "A02-speed.html#conditional-likelihood",
    "href": "A02-speed.html#conditional-likelihood",
    "title": "Appendix B — Faster is better",
    "section": "\nB.2 Conditional likelihood",
    "text": "B.2 Conditional likelihood\nThe default in secr.fit is to maximize the full likelihood (i.e., to jointly fit both the state process and the observation process). If you do not need to model spatial, temporal or group-specific variation in density (the sole real parameter of the state model) then you can save time by first fitting only the observation process. This is achieved by maximizing only the likelihood conditional on n, the number of detected individuals (Borchers & Efford, 2008). Conditional likelihood maximization is selected in secr.fit by setting CL = TRUE.\nSelecting an observation model with CL = TRUE (and first focussing on detection parameters) is a good strategy even if you intend to model density later. It may occasionally be desirable to re-visit the selection if covariates can affect both density and detection parameters.\n\nfit &lt;- secr.fit(hareCH6, buffer = 250, CL = FALSE, trace = FALSE) # default\nfitCL &lt;- secr.fit(hareCH6, buffer = 250, CL = TRUE, trace = FALSE)\n\nFitting time is reduced by 43% because maximization is over two parameters (g0, sigma) instead of three. The relative reduction will be less for more complex detection models, but still worthwhile.\nHaving selected a suitable observation model with CL = TRUE you can then either resort to a full-likelihood fit to estimate density, or compute a Horvitz-Thompson-like (HT) estimate in function derived. In models without individual covariates the HT estimate is n/a(\\hat \\theta) where n is the number of detected individuals, \\theta represents the parameters of the observation model, and a is the effective sampling area as a function of the estimated detection parameters.\nCompare\n\npredict(fit)     # CL = FALSE\n\n       link  estimate SE.estimate       lcl       ucl\nD       log  1.465987   0.1913125  1.136361  1.891228\ng0    logit  0.061584   0.0093004  0.045686  0.082536\nsigma   log 68.345777   4.4693114 60.132427 77.680970\n\nderived(fitCL)   # CL = TRUE\n\n    estimate SE.estimate    lcl    ucl     CVn      CVa     CVD\nesa   46.385          NA     NA     NA      NA       NA      NA\nD      1.466     0.19051 1.1376 1.8892 0.12127 0.046715 0.12995\n\n\nEstimated density is exactly the same, to 6 significant figures (1.46599). This is expected when n is Poisson; slight differences arise when n is binomial (because the number of animals N in the masked area is considered fixed rather than random). The estimated variance differs slightly - that from derived follows an unpublished and slightly ad hoc procedure (Borchers & Efford, 2007).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Faster is better</span>"
    ]
  },
  {
    "objectID": "A02-speed.html#sec-mash",
    "href": "A02-speed.html#sec-mash",
    "title": "Appendix B — Faster is better",
    "section": "\nB.3 Mashing",
    "text": "B.3 Mashing\nMashing is a very effective way of speeding up estimation when the design uses many replicate clusters of detectors, each with the same geometry, and clusters are far enough apart that animals are not detected on more than one. The approach for M clusters is to overlay all data as if from a single cluster; the estimated density will be M times the per-cluster estimate, and SE etc. will be inflated by the same factor. This relies on individuals being detected independently of each other, which is a standard assumption in any case. The present implementation assumes density is uniform.\nThe capthist object is first collapsed with function mash into one using the geometry of a single cluster. The object retains a memory of the number of individuals from each original cluster in the attribute ‘n.mash’. Functions derived, derivedMash and the method predict.secr use ‘n.mash’ to adjust their output density, SE, and confidence limits.\nWe describe in general terms an actual example in which 18 separated clusters of 12 traps were operated on 6 occasions. Each cluster had the same geometry (two parallel rows of traps separated by 200 m along and between rows). Trap numbering was consistently up one row and down the other. The capthist object CH included detections of 150 animals in the 216 traps. To mash these data we first assign attributes for the cluster number (clusterID) and the sequence number of each trap within its cluster (clustertrap). The function mash then collapses the data as if all detections were made on one cluster. A mask based on this single notional cluster has many fewer points than a mask with the same spacing spanning all clusters.\n\nclustertrap(traps(CH)) &lt;- rep(1:12,18)\nclusterID(traps(CH)) &lt;- rep(1:18, each = 12)\nmashedCH &lt;- mash(CH)\nmashedmask &lt;- make.mask(traps(mashedCH), buffer = 900, spacing = 100, \n                        type = \"trapbuffer\")\nfitmash &lt;- secr.fit(mashedCH, mask = mashedmask)\n\nThe model for the mashed data fitted in 4% of the time required for the original. The mashed estimate of density shrank by 2% in this case, which is probably due to slight variation among clusters in the actual spacing of traps (one cluster was arbitrarily chosen to represent all clusters). Mashing prevents the inclusion of cluster-specific detail in the model (such as discontinuous habitat near the traps). For further details see ?mash.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Faster is better</span>"
    ]
  },
  {
    "objectID": "A02-speed.html#parallel-fitting",
    "href": "A02-speed.html#parallel-fitting",
    "title": "Appendix B — Faster is better",
    "section": "\nB.4 Parallel fitting",
    "text": "B.4 Parallel fitting\nYour computer almost certainly has multiple cores, allowing computations to run in parallel.\nMulti-threading in secr.fit uses multiple cores. By default only 2 cores are used (a limit set by CRAN), and this should almost certainly be increased. The number of threads is set with setNumThreads() or the ‘ncores’ argument. The marginal benefit of increasing the number of threads declines as more are added. Modify the following benchmark code for your own example. For models that are very slow to fit, relative values may be got more quickly by performing a single likelihood evaluation with secr.fit(..., details = list(LLonly = TRUE)).\n\ncode for benchmark timinglibrary(microbenchmark)   # install this package first\ncores &lt;- c(2,4,6,8)\njobs &lt;- lapply(cores, function(nc) {\n    bquote(suppressWarnings(\n        secr.fit(captdata, trace = FALSE, ncores = .(nc))\n    ))})\nnames(jobs) &lt;- paste(\"ncores = \", cores)\nmicrobenchmark(list = jobs, times = 10, unit = \"seconds\")\n\nUnit: seconds\n        expr     min      lq    mean  median     uq    max neval\n ncores =  2 2.03325 2.08653 2.56186 2.31389 3.1715 3.2989    10\n ncores =  4 1.09336 1.21461 1.45341 1.33114 1.7241 1.8581    10\n ncores =  6 0.86954 1.02736 1.10523 1.06195 1.1171 1.6480    10\n ncores =  8 0.81599 0.83314 0.98474 0.96208 1.0265 1.3466    10\n\n\nSee ?Parallel for more.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Faster is better</span>"
    ]
  },
  {
    "objectID": "A02-speed.html#reducing-complexity-session--or-group-specific-models",
    "href": "A02-speed.html#reducing-complexity-session--or-group-specific-models",
    "title": "Appendix B — Faster is better",
    "section": "\nB.5 Reducing complexity (session- or group-specific models)",
    "text": "B.5 Reducing complexity (session- or group-specific models)\nSimultaneous estimation of many parameters can be painfully slow, but it can be completely avoided. If your model is fully session- or group-specific then it is much faster to analyse each group separately. For sessions this is can be done simply with lapply above and in Chapter 14. For groups you may need to construct new capthist objects using subset to extract groups corresponding to the levels of one or more individual covariates.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Faster is better</span>"
    ]
  },
  {
    "objectID": "A02-speed.html#reducing-number-of-levels-of-detection-covariates",
    "href": "A02-speed.html#reducing-number-of-levels-of-detection-covariates",
    "title": "Appendix B — Faster is better",
    "section": "\nB.6 Reducing number of levels of detection covariates",
    "text": "B.6 Reducing number of levels of detection covariates\nsecr.fit pre-computes a lookup table of values for detection parameters. The size of the table increases with the number of unique levels of any covariates. A continuous individual or detector covariate can have many similar levels. Binning covariate values can give a large saving in memory and time (see ?binCovariate).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Faster is better</span>"
    ]
  },
  {
    "objectID": "A02-speed.html#sec-collapseoccasions",
    "href": "A02-speed.html#sec-collapseoccasions",
    "title": "Appendix B — Faster is better",
    "section": "\nB.7 Collapsing occasions",
    "text": "B.7 Collapsing occasions\nIf there is no temporal aspect to the model you want to fit (such as a behavioural response) and detectors are independent (not true for traps i.e. “multi”) then it is attractive to collapse all sampling occasions. This happens automatically by default for ‘proximity’ and ‘count’ detectors (details argument ‘fastproximity = TRUE’). For example, with the Tennessee black bear data we get:\n\ncode to compare fastproximity on/off# Great Smoky Mountains black bear hair snag data\n# Deliberately slow both fits down by setting ncores = 2\nold &lt;- setNumThreads(2)\n\n# mask using park boundary GSM\nmsk &lt;- make.mask(traps(blackbearCH), buffer = 6000, type = 'trapbuffer', \n                 poly = GSM, keep.poly = FALSE)\n    \n# 'fastproximity' On (default for 'proximity' detectors)\nbbfitfaston &lt;- secr.fit(blackbearCH, mask = msk, trace = FALSE)\n\n# 'fastproximity' Off\nbbfitfastoff &lt;- secr.fit(blackbearCH, mask = msk, trace = FALSE, \n                      details = list(fastproximity = FALSE))\n\n# summary\nfits &lt;- secrlist(bbfitfaston, bbfitfastoff)\n\ncat(\"Compare density estimates\\n\")\ncollate(fits)[,,,'D']\ncat (\"\\nCompare timing\\n\")\nsapply(fits, '[[', 'proctime')\n\nCompare density estimates\n              estimate SE.estimate      lcl      ucl\nbbfitfaston  0.0084355  0.00081891 0.006977 0.010199\nbbfitfastoff 0.0084355  0.00081890 0.006977 0.010199\n\nCompare timing\n bbfitfaston.elapsed bbfitfastoff.elapsed \n                3.97                13.82 \n\n\n\nData may be collapsed in reduce.capthist without loss of data by choosing ‘outputdetector’ carefully and setting by = “ALL”. The collapsed capthist object receives a usage attribute equal to the sum of occasion-specific usages. In this case usage is the number of collapsed occasions (10) and the collapsed model fits a Binomial probability with size = 10 rather than a Bernoulli probability per occasion.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Faster is better</span>"
    ]
  },
  {
    "objectID": "A02-speed.html#sec-masksubset",
    "href": "A02-speed.html#sec-masksubset",
    "title": "Appendix B — Faster is better",
    "section": "\nB.8 Individual mask subsets",
    "text": "B.8 Individual mask subsets\nsecr allows the user to customise the mask for each detected animal by considering only a subset of points. The subset is defined by a radius in metres around the centroid of detections; set this using the details argument ‘maxdistance’. Speed gains vary with the layout, but can exceed 2-fold. Bias results when the radius is too small (try 5\\sigma or the buffer distance).\nHere we extend the preceding black bear example. The mask used for each bear is restricted to points within 6 km of the centroid of its detections. There is a &gt;2\\times speed gain without significant change in the density estimates.\n\ncode to plot sample of individual maskscentr &lt;- centroids(blackbearCH)\npar(mfrow=c(3,4), mar=c(1,1,1,1))\nfor (i in sort(sample.int(139, size=12))) {\n    plot(msk, border = 10)\n    plot(subset(msk, distancetotrap(msk, centr[i,])&lt;6000), add = TRUE, \n         col = 'red')\n    plot(subset(blackbearCH, i), varycol = FALSE, add = TRUE, title = '', \n         subtitle = '')\n    mtext(side = 3, i, line = -1, cex = 0.9 )\n}\n\n\n\n\n\n\nFigure B.1: Sample of individual blackbear masks (red) with detection sites overplotted in blue.\n\n\n\n\n\ncode to compare individual mask subsetsold &lt;- setNumThreads(2)\n# fastproximity default\nbbfitfaston &lt;- secr.fit(blackbearCH, mask = msk, trace = FALSE)\nbbfitfastonmaxd &lt;- secr.fit(blackbearCH, mask = msk, trace = FALSE, \n                      details = list(maxdistance = 6000))\n\nfits &lt;- secrlist(bbfitfaston, bbfitfastonmaxd)\n\ncat(\"Compare density estimates\\n\")\ncollate(fits)[,,,'D']\ncat (\"\\nCompare timing\\n\")\nsapply(fits, '[[', 'proctime')\n\nCompare density estimates\n                 estimate SE.estimate       lcl      ucl\nbbfitfaston     0.0084355  0.00081891 0.0069770 0.010199\nbbfitfastonmaxd 0.0084421  0.00081855 0.0069841 0.010204\n\nCompare timing\n    bbfitfaston.elapsed bbfitfastonmaxd.elapsed \n                   4.00                    1.33",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Faster is better</span>"
    ]
  },
  {
    "objectID": "A02-speed.html#sec-collapsedetectors",
    "href": "A02-speed.html#sec-collapsedetectors",
    "title": "Appendix B — Faster is better",
    "section": "\nB.9 Collapsing detectors",
    "text": "B.9 Collapsing detectors\nThe benefit from collapsing occasions has a spatial analogue: if there are many detectors and they are closely spaced relative to animal movements \\sigma then nearby detectors may be aggregated into new notional detectors located at the centroid. The reduce.traps method has an argument ‘span’ explained as follows in the help –\n\nIf ‘span’ is specified a clustering of detector sites will be performed with hclust and detectors will be assigned to groups with cutree. The default algorithm in hclust is complete linkage, which tends to yield compact, circular clusters; each will have diameter less than or equal to ‘span’.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Faster is better</span>"
    ]
  },
  {
    "objectID": "A02-speed.html#multi-detector-type-instead-of-proximity",
    "href": "A02-speed.html#multi-detector-type-instead-of-proximity",
    "title": "Appendix B — Faster is better",
    "section": "\nB.10 “multi” detector type instead of “proximity”",
    "text": "B.10 “multi” detector type instead of “proximity”\nThe type of the detectors is usually determined by the sampling reality. For example, if individuals can physically be detected at several sites on one occasion then the “proximity” detector type is preferred over “multi”. However, if data are very sparse, so that individuals in practice are almost never observed at multiple sites on one occasion, then the detectors may as well by of type “multi”, in the sense that there is no observable difference between the two types of detection process. “multi” models used to fit much more quickly than “proximity” models, and this is still true for elaborate or time-dependent models that cannot use the ‘fastproximity’ option.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Faster is better</span>"
    ]
  },
  {
    "objectID": "A02-speed.html#some-models-are-just-slower-than-others",
    "href": "A02-speed.html#some-models-are-just-slower-than-others",
    "title": "Appendix B — Faster is better",
    "section": "\nB.11 Some models are just slower than others",
    "text": "B.11 Some models are just slower than others\nDetector covariates pose a particular problem. Models with learned responses take slightly longer to fit.\n\n\n\nFigure B.1: Sample of individual blackbear masks (red) with detection sites overplotted in blue.\n\n\n\nBorchers, D. L., & Efford, M. G. (2007). Supplements to biometrics paper. https://www.otago.ac.nz/density\n\n\nBorchers, D. L., & Efford, M. G. (2008). Spatially explicit maximum likelihood methods for capture-recapture studies. Biometrics, 64, 377–385.\n\n\nKendeigh, S. C. (1944). Measurement of bird populations. Ecological Monographs, 14, 67–106. https://doi.org/10.2307/1961632",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Faster is better</span>"
    ]
  },
  {
    "objectID": "A03-spatial-data.html",
    "href": "A03-spatial-data.html",
    "title": "Appendix C — Spatial data",
    "section": "",
    "text": "C.1 Spatial data in R\nTo use spatial data or functions from external sources in secr it helps to know a little about the expanding spatial ecosystem in R.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Spatial data</span>"
    ]
  },
  {
    "objectID": "A03-spatial-data.html#spatial-data-in-r",
    "href": "A03-spatial-data.html#spatial-data-in-r",
    "title": "Appendix C — Spatial data",
    "section": "",
    "text": "C.1.1 R packages for spatial data\nSeveral widely used packages define classes and methods for spatial data (‘Used by’ in the following table is the number of CRAN packages from crandep::get_dep on 2025-02-10).\n\n\n\n\n\n\n\n\n\n\nPackage\nScope\nYear\nUsed by\nCitation\nRelevant S4 classes\n\n\n\nsp\nvector\n2005–\n465\nPebesma & Bivand (2005)\nSpatialPolygons, SpatialPolygonsDataFrame, SpatialGridDataFrame, SpatialLinesDataFrame\n\n\nraster\nraster\n2010–\n339\nHijmans (2023a)\nRasterLayer\n\n\nsf\nvector\n2016–\n936\nPebesma (2018)\nsfg, sfc, sf\n\n\nstars\nboth\n2018–\n79\nEdzer & Bivand (2023)\nstars\n\n\nterra\nboth\n2020–\n398\nHijmans (2023b)\nSpatVector, SpatRaster\n\n\n\nThe reader should already understand the distinction between vector and raster spatial data. There are many resources for learning about spatial analysis in R that may be found by web search on, for example ‘R spatial data’. The introduction by Claudia Engel covers both sp and sf.\nThe capability of sp is being replaced by sf and raster is being replaced by terra. The more recent packages tend to be faster. sf implements the ‘simple features’ standard.\n\nC.1.2 Geographic vs projected coordinates\nQGIS has an excellent introduction to coordinate reference systems (CRS) for GIS. Coordinate reference systems may be specified in many ways; the most simple is the 4- or 5-digit EPSG code (search for EPSG on the web).\nGeographic coordinates (EPSG 4326, ignoring some details) specify a location on the earth’s surface by its latitude and longitude. This is the standard in Google Earth and Geographic Positioning Systems (GPS).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Spatial data</span>"
    ]
  },
  {
    "objectID": "A03-spatial-data.html#spatial-data-in-secr",
    "href": "A03-spatial-data.html#spatial-data-in-secr",
    "title": "Appendix C — Spatial data",
    "section": "\nC.2 Spatial data in secr\n",
    "text": "C.2 Spatial data in secr\n\n\nC.2.1 Input of detector locations\nsecr uses relative Cartesian coordinates. Detector coordinates from GPS should therefore be projected from geographic coordinates before input to secr. Most of the R spatial packages include projection functions. Here is a simple example using st_transform from the sf package:\n\nlibrary(sf)\n\nLinking to GEOS 3.11.2, GDAL 3.7.2, PROJ 9.3.0; sf_use_s2() is TRUE\n\n# unprojected (geographic) coordinates (decimal degrees) \n# longitude before latitude\ndf &lt;- data.frame(x = c(174.9713, 174.9724, 174.9738), \n                 y = c(-41.3469, -41.3475, -41.3466))\n# construct sf object\nlatlon &lt;- st_as_sf(df, coords = 1:2)\n# specify initial CRS: WGS84 lat-lon\nst_crs(latlon) &lt;- 4326  \n# project to Cartesian coordinate system, units metres\n# EPSG:27200 is the old (pre-2001) NZMG\ntrps &lt;- st_transform(latlon, crs = 27200) \n# print\nst_coordinates(latlon)  \n\n          X       Y\n[1,] 174.97 -41.347\n[2,] 174.97 -41.347\n[3,] 174.97 -41.347\n\nst_coordinates(trps)\n\n           X       Y\n[1,] 2674948 5982573\n[2,] 2675038 5982504\n[3,] 2675157 5982602\n\n\n\nC.2.2 Adding spatial covariates to a traps or mask object\nSECR models may include covariates for each detector (e.g., trap or searched polygon) in the detection model (parameters g_0, \\lambda_0, \\sigma etc.) and for each point on the discretized habitat mask in the density model (parameter D).\nCovariates measured at detector locations may be included in the text files read by read.traps or read.capthist.\nCovariates measured at each point on a habitat mask may be included in a file or data.frame input to read.mask, but this is an uncommon way to establish mask covariates. More commonly, a habitat mask is built using make.mask and initially has no covariates,\nThe function addCovariates is a convenient way to attach covariates to a traps or mask object post hoc. The function extracts covariate values from the ‘spatialdata’ argument by a spatial query for each point on a mask. Options are\n\n\nspatialdata\nNotes\n\n\n\ncharacter\nname of ESRI shapefile, excluding ‘.shp’\n\n\nsp::SpatialPolygonsDataFrame\n\n\n\nsp::SpatialGridDataFrame\n\n\n\nraster::RasterLayer\n\n\n\nsecr::mask\ncovariates of nearest point\n\n\nsecr::traps\ncovariates of nearest point\n\n\nterra::SpatRaster\nnew in 4.5.3\n\n\nsf::sf\nnew in 4.5.3\n\n\n\nData sources should use the coordinate reference system of the target detectors and mask (see previous section).\n\nC.2.3 Functions with ‘poly’ or ‘region’ spatial argument\nSeveral secr functions use spatial data to define a region of interest (i.e. one or more polygons). All such polygons may be defined as\n\n2-column matrix or data.frame of x- and y-coordinates\nSpatialPolygons or SpatialPolygonsDataFrame S4 classes from package sp\n\nSpatRaster S4 class from package terra\n\nsf or sfc S4 classes from package sf (POLYGON or MULTIPOLYGON geometries)\n\nData in these formats are converted to an object of class sfc by the documented internal function boundarytoSF. The S4 classes allow complex regions with multiple polygons (islands), possibly containing ‘holes’ (lakes).\nThis applies to the following functions and arguments:\n\n\n\nsecr function\nArgument\n\n\n\nbufferContour\npoly\n\n\ndeleteMaskPoints\npoly\n\n\nesaPlot\npoly\n\n\nmake.mask\npoly\n\n\nmake.systematic\nregion\n\n\nmask.check\npoly\n\n\npdotContour\npoly\n\n\nPG\npoly\n\n\npointsInPolygon\npoly*\n\n\nregion.N\nregion*\n\n\nsim.popn\npoly\n\n\nsubset.popn\npoly\n\n\ntrap.builder\nregion\n\n\ntrap.builder\nexclude\n\n\n\n* pointsInPolygon and region.N also accept a habitat mask.\n\nC.2.4 GIS functionality imported from other R packages\nSome specialised spatial operations are out-sourced by secr:\n\n\n\n\n\n\n\n\n\nsecr function\nOperation\nOther-package function\nReference\n\n\n\nrandomHabitat\nsimulated habitat\nraster::adjacent\nHijmans (2023a)\n\n\n\n\nraster::clump\n\n\n\nnedist\nnon-Euclidean distances\ngdistance::transition\nvan Etten (2023)\n\n\n\n\ngdistance::geoCorrection\n\n\n\n\n\ngdistance::costDistance\n\n\n\ndiscretize\ncell overlap with polygon(s)\nsf::st_intersection\nPebesma (2018)\n\n\n\n\nsf::st_area\n\n\n\npolyarea\narea of polygon(s)\nsf::st_area\n\n\n\nmake.mask\npolybuffer mask type\nsf::st_buffer\n\n\n\nrbind.capthist\nmerge polygon detectors\nsf::st_union\n\n\n\ntrap.builder\nSRS sample\nsf::st_sample\n\n\n\ntrap.builder\nGRTS sample (spsurvey &gt;= 5.3.0)\nspsurvey::grts\nDumelle et al. (2024)\n\n\n\nC.2.5 Exporting raster data for use in other packages\nA mask or predicted density surface (Dsurface) generated in secr may be used or plotted as a raster layer in another R package. secr provides rast and raster methods for secr mask and Dsurface objects, based on the respective generic functions exported by terra and raster. These return SpatRaster and RasterLayer objects respectively. For example,\n\nlibrary(secr)\nsummary(possummask)\n\nObject class      mask \nMask type         trapbuffer \nNumber of points  5120 \nSpacing m         20 \nCell area ha      0.04 \nTotal area ha     204.8 \nx-range m         2697463 2699583 \ny-range m         6077080 6078580 \nBounding box      \n        x       y\n1 2697453 6077070\n2 2699593 6077070\n4 2699593 6078590\n3 2697453 6078590\n\nSummary of covariates \n   d.to.shore    \n Min.   :  2.24  \n 1st Qu.:200.11  \n Median :370.80  \n Mean   :389.24  \n 3rd Qu.:560.82  \n Max.   :916.69  \n\n# make SpatRaster object from mask covariate\nr &lt;- rast(possummask, covariate = 'd.to.shore')\nprint(r)\n\nclass       : SpatRaster \ndimensions  : 76, 107, 1  (nrow, ncol, nlyr)\nresolution  : 20, 20  (x, y)\nextent      : 2697453, 2699593, 6077070, 6078590  (xmin, xmax, ymin, ymax)\ncoord. ref. :  \nsource(s)   : memory\nname        :      tmp \nmin value   :   2.2361 \nmax value   : 916.6924 \n\nterra::plot(r)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Spatial data</span>"
    ]
  },
  {
    "objectID": "A03-spatial-data.html#limits-of-the-cartesian-model-in-secr",
    "href": "A03-spatial-data.html#limits-of-the-cartesian-model-in-secr",
    "title": "Appendix C — Spatial data",
    "section": "\nC.3 Limits of the Cartesian model in secr\n",
    "text": "C.3 Limits of the Cartesian model in secr\n\n\nC.3.1 Distances computed in large studies\nDistances on the curved surface of the earth are not well represented by straight-line Euclidean distances when the study area is very large, as happens with large carnivores such as grizzly bears and wolverines. That has led some authors to use more rigorous (great-circle) distances. This is not possible in secr because there is no record of the projected coordinate reference system used for the detectors and habitat mask.\n\n\n\n\nDumelle, M., Kincaid, T. M., Olsen, A. R., & Weber, M. H. (2024). Spsurvey: Spatial sampling design and analysis. In R package version 5.5.1. https://CRAN.R-project.org/package=spsurvey\n\n\nEdzer, E., & Bivand, R. (2023). Spatial Data Science: With applications in R (p. 352). Chapman; Hall/CRC. https://doi.org/10.1201/9780429459016\n\n\nHijmans, R. J. (2023a). Raster: Geographic data analysis and modeling. In R package version 3.6-26. https://CRAN.R-project.org/package=raster\n\n\nHijmans, R. J. (2023b). Terra: Spatial data analysis. https://CRAN.R-project.org/package=terra\n\n\nKendeigh, S. C. (1944). Measurement of bird populations. Ecological Monographs, 14, 67–106. https://doi.org/10.2307/1961632\n\n\nPebesma, E. J. (2018). Simple Features for R: Standardized Support for Spatial Vector Data. The R Journal, 10(1), 439–446. https://doi.org/10.32614/RJ-2018-009\n\n\nPebesma, E. J., & Bivand, R. (2005). Classes and methods for spatial data in R. R News, 5(2), 9–13. https://CRAN.R-project.org/doc/Rnews/\n\n\nvan Etten, J. (2023). Gdistance: Distances and routes on geographical grids. https://CRAN.R-project.org/package=gdistance",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Spatial data</span>"
    ]
  },
  {
    "objectID": "A04-polygondetectors.html",
    "href": "A04-polygondetectors.html",
    "title": "Appendix D — Area and transect search",
    "section": "",
    "text": "D.1 Parameterisation\nThe detection model is fundamentally different for polygon detectors and detectors at a point (“single”, “multi”, “proximity”, “count”):\nWe use a parameterisation that separates two aspects of detection – the expected number of cues from an individual (\\lambda_c) and their spatial distribution given the animal’s location (h(\\mathbf u| \\mathbf x) normalised by dividing by H(\\mathbf x) (Eq. 4.1). The parameters of h() are those of a typical detection function in secr (e.g., \\lambda_0, \\sigma), except that the factor \\lambda_0 cancels out of the normalised expression. The expected number of cues, given an unbounded search area, is determined by a different parameter here labelled \\lambda_c.\nThere are complications:",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Area and transect search</span>"
    ]
  },
  {
    "objectID": "A04-polygondetectors.html#parameterisation",
    "href": "A04-polygondetectors.html#parameterisation",
    "title": "Appendix D — Area and transect search",
    "section": "",
    "text": "For point detectors, the detection function directly models the probability of detection or the expected number of detections. All that matters is the distance between the animal’s centre and the detector.\nFor polygon detectors, these quantities (probability or expected number) depend also on the geometrical relationships and the integration in Eq. 4.1. The detection function serves only to define the potential detections if the search area was unbounded (blue contours in Fig. 4.1).\n\n\n\n\nRather than designate a new ‘real’ parameter lambdac, secr grabs the redundant intercept of the detection function (lambda0) and uses that for \\lambda_c. Bear this in mind when reading output from polygon or transect models.\nIf each animal can be detected at most once per detector per occasion (as with exclusive detector types ‘polygonX’ and ‘transectX’) then instead of \\lambda(\\mathbf x) we require a probability of detection between 0 and 1, say g(\\mathbf x). In secr the probability of detection is derived from the cumulative hazard using g(\\mathbf x) = 1-\\exp(-\\lambda(\\mathbf x)). The horned lizard dataset of Royle & Young (2008) has detector type ‘polygonX’ and their parameter ‘p’ was equivalent to 1 - \\exp(-\\lambda_c) (0 &lt; p \\le 1). For the same scenario and parameter Efford (2011) used p_\\infty.\nUnrelated to (2), detection functions in secr may model either the probability of detection (HN, HR, EX etc.) or the cumulative hazard of detection (HHN, HHR, HEX etc.) (see ?detectfn for a list). Although probability and cumulative hazard are mostly interchangeable for point detectors, it’s not so simple for polygon and transect detectors. The integration always uses the hazard form for h(\\mathbf u | \\mathbf x) (secr \\ge 3.0.0)1, and only hazard-based detection functions are allowed (HHN, HHR, HEX, HAN, HCG, HVP). The default function is HHN.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Area and transect search</span>"
    ]
  },
  {
    "objectID": "A04-polygondetectors.html#example-data-flat-tailed-horned-lizards",
    "href": "A04-polygondetectors.html#example-data-flat-tailed-horned-lizards",
    "title": "Appendix D — Area and transect search",
    "section": "\nD.2 Example data: flat-tailed horned lizards ",
    "text": "D.2 Example data: flat-tailed horned lizards \nRoyle & Young (2008) reported a Bayesian analysis of data from repeated searches for flat-tailed horned lizards (Phrynosoma mcallii) on a 9-ha square plot in Arizona, USA. Their dataset is included in secr as hornedlizardCH and will be used for demonstration. See ‘?hornedlizard’ for more details.\nThe lizards were free to move across the boundary of the plot and often buried themselves when approached. Half of the 134 different lizards were seen only once in 14 searches over 17 days. Fig. 2 shows the distribution of detections within the quadrat; lines connect successive detections of the individuals that were recaptured.\n\npar(mar=c(1,1,2,1))\nplot(hornedlizardCH, tracks = TRUE, varycol = FALSE, lab1cap = \n    TRUE, laboffset = 8, border = 10, title ='')\n\n\n\nLocations of horned lizards on a 9-ha plot in Arizona (Royle & Young, 2008). Grid lines are 100 m apart.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Area and transect search</span>"
    ]
  },
  {
    "objectID": "A04-polygondetectors.html#data-input",
    "href": "A04-polygondetectors.html#data-input",
    "title": "Appendix D — Area and transect search",
    "section": "\nD.3 Data input",
    "text": "D.3 Data input\nInput of data for polygon and transect detectors is described in secr-datainput.pdf. It is little different to input of other data for secr. The key function is read.capthist, which reads text files containing the polygon or transect coordinates2 and the capture records. Capture data should be in the ‘XY’ format of Density (one row per record with fields in the order Session, AnimalID, Occasion, X, Y). Capture records are automatically associated with polygons on the basis of X and Y (coordinates outside any polygon give an error). Transect data are also entered as X and Y coordinates and automatically associated with transect lines.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Area and transect search</span>"
    ]
  },
  {
    "objectID": "A04-polygondetectors.html#fitting",
    "href": "A04-polygondetectors.html#fitting",
    "title": "Appendix D — Area and transect search",
    "section": "\nD.4 Fitting",
    "text": "D.4 Fitting\nThe function secr.fit is used to fit polygon or transect models by maximum likelihood, exactly as for other detectors. Any model fitting requires a habitat mask – a representation of the region around the detectors possibly occupied by the detected animals (aka the ‘area of integration’ or ‘state space’). It’s simplest to use a simple buffer around the detectors, specified via the ‘buffer’ argument of secr.fit3. For the horned lizard dataset it is safe to use the default buffer width (100 m) and the default detection function (circular bivariate normal). We use trace = FALSE to suppress intermediate output that would be untidy here.\n\nFTHL.fit &lt;- secr.fit(hornedlizardCH, buffer = 80, trace = FALSE)\npredict(FTHL.fit)\n\n        link estimate SE.estimate      lcl      ucl\nD        log  8.01307    1.061701  6.18740 10.37742\nlambda0  log  0.13171    0.015128  0.10524  0.16485\nsigma    log 18.50490    1.199388 16.29950 21.00871\n\n\nThe estimated density is 8.01 / ha, somewhat less than the value given by Royle & Young (2008); see Efford (2011) for an explanation, also Marques et al. (2011) and Dorazio (2013). The parameter labelled ‘lambda0’ (i.e. \\lambda_c) is equivalent to p in Royle & Young (2008) (using \\hat p \\approx 1 - \\exp(-\\hat \\lambda_c)).\nFTHL.fit is an object of class ‘secr’. We would use the ‘plot’ method to graph the fitted detection function :\n\nplot(FTHL.fit, xv = 0:70, ylab = 'p')",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Area and transect search</span>"
    ]
  },
  {
    "objectID": "A04-polygondetectors.html#cue-data",
    "href": "A04-polygondetectors.html#cue-data",
    "title": "Appendix D — Area and transect search",
    "section": "\nD.5 Cue data",
    "text": "D.5 Cue data\nBy ‘cue’ in this context we mean a discrete sign identifiable to an individual animal by means such as microsatellite DNA. Faeces and passive hair samples may be cues. Animals may produce more than one cue per occasion. The number of cues in a specific polygon then has a discrete distribution such as Poisson, binomial or negative binomial.\nA cue dataset is not readily available, so we simulate some cue data to demonstrate the analysis. The text file ‘polygonexample1.txt’ contains the boundary coordinates.\n\ndatadir &lt;- system.file(\"extdata\", package = \"secr\")\nfile1 &lt;- paste0(datadir, '/polygonexample1.txt')\nexample1 &lt;- read.traps(file = file1, detector = 'polygon')\npolygonCH &lt;- sim.capthist(example1, popn = list(D = 1, \n    buffer = 200), detectfn = 'HHN', detectpar = list(\n    lambda0 = 5, sigma = 50), noccasions = 1, seed = 123)\n\n\npar(mar = c(1,2,3,2))\nplot(polygonCH, tracks = TRUE, varycol = FALSE, lab1cap = TRUE,\n    laboffset = 15, title = paste(\"Simulated 'polygon' data\", \n    \"D = 1, lambda0 = 5, sigma = 50\"))\n\n\n\n\n\n\nFigure D.1: Simulated cue data from a single search of two irregular polygons.\n\n\n\n\nOur simulated sampling was a single search (noccasions = 1), and the intercept of the detection function (lambda0 = 5) is the expected number of cues that would be found per animal if the search was unbounded. The plot (Fig. D.1) is slightly misleading because, although ‘tracks = TRUE’ serves to link cues from the same animal, the cues are not ordered in time.\nTo fit the model by maximum likelihood we use secr.fit as before:\n\ncuesim.fit &lt;- secr.fit(polygonCH, buffer = 200, trace = FALSE)\npredict(cuesim.fit)\n\n        link estimate SE.estimate      lcl     ucl\nD        log   1.1035     0.15366  0.84099  1.4478\nlambda0  log   4.3764     0.41885  3.62938  5.2771\nsigma    log  49.4464     2.43133 44.90613 54.4457",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Area and transect search</span>"
    ]
  },
  {
    "objectID": "A04-polygondetectors.html#sec-discretize",
    "href": "A04-polygondetectors.html#sec-discretize",
    "title": "Appendix D — Area and transect search",
    "section": "\nD.6 Discretizing polygon data",
    "text": "D.6 Discretizing polygon data\nAn alternative way to handle polygon capthist data is to convert it to a raster representation i.e. to replace each polygon with a set of point detectors, each located at the centre of a square pixel. Point detectors (‘multi’, ‘proximity’, ‘count’ etc.) tend to be more robust and models often fit faster. They also allow habitat attributes to be associated with detectors at the scale of a pixel rather than the whole polygon. The secr function discretize performs the necessary conversion in a single step. Selection of an appropriate pixel size (spacing) is up to the user. There is a tradeoff between faster execution (larger pixels are better) and controlling artefacts from the discretization, which can be checked by comparing estimates with different levels of spacing.\nTaking our example from before,\n\ndiscreteCH &lt;- discretize (polygonCH, spacing = 20)\npar(mar = c(1,2,3,2))\nplot(discreteCH, varycol = FALSE, tracks = TRUE)\n\n\n\nDiscretized area search data\n\n\n\n\ndiscrete.fit &lt;- secr.fit(discreteCH, buffer = 200, detectfn = \n    'HHN', trace = FALSE)\npredict(discrete.fit)\n\n        link estimate SE.estimate       lcl      ucl\nD        log  1.09535    0.152768  0.834463  1.43780\nlambda0  log  0.11205    0.014492  0.087048  0.14422\nsigma    log 49.83842    2.673999 44.867021 55.36066\n\n\nPost-hoc discretization is also considered by Russell et al. (2012), Milleret et al. (2018) and Paterson et al. (2019).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Area and transect search</span>"
    ]
  },
  {
    "objectID": "A04-polygondetectors.html#transectsearch",
    "href": "A04-polygondetectors.html#transectsearch",
    "title": "Appendix D — Area and transect search",
    "section": "\nD.7 Transect search",
    "text": "D.7 Transect search\nTransect data, as understood here, include the positions from which individuals are detected along a linear route through 2-dimensional habitat. They do not include distances from the route to the location of the individual, at least, not yet. A route may be searched multiple times, and a dataset may include multiple routes, but neither of these is necessary. Searches of linear habitat such as river banks require a different approach - see the package secrlinear.\nWe simulate some data for an imaginary wiggly transect.\n\nx &lt;- seq(0, 4*pi, length = 20)\ntransect &lt;- make.transect(x = x*100, y = sin(x)*300, \n    exclusive = FALSE)\nsummary(transect)\n\nObject class       traps \nDetector type      transect \nNumber vertices    20 \nNumber transects   1 \nTotal length       2756.1 m \nx-range            0 1256.6 m \ny-range            -298.98 298.98 m \n\ntransectCH &lt;- sim.capthist(transect, popn = list(D = 2, \n    buffer = 300), detectfn = 'HHN', detectpar = list(\n    lambda0 = 1.0, sigma = 50), binomN = 0, seed = 123)\n\nBy setting exclusive = FALSE we signal that there may be more than one detection per animal per occasion on this single transect (i.e. this is a ‘transect’ detector rather than ‘transectX’).\nConstructing a habitat mask explicitly with make.mask (rather than relying on ‘buffer’ in secr.fit) allows us to specify the point spacing and discard outlying points (Fig. D.2).\n\ntransectmask &lt;- make.mask(transect, type = 'trapbuffer', buffer = 300, spacing = 20)\npar(mar = c(3,1,3,1))\nplot(transectmask, border = 0)\nplot(transect, add = TRUE, detpar = list(lwd = 2))\nplot(transectCH, tracks = TRUE, add = TRUE, title = '')\n\n\n\n\n\n\nFigure D.2: Habitat mask (grey dots) and simulated transect data from five searches of a 2.8-km transect. Colours differ between individuals, but are not unique.\n\n\n\n\nModel fitting uses secr.fit as before. We specify the distribution of the number of detections per individual per occasion as Poisson (binomN = 0), although this also happens to be the default. Setting method = ‘Nelder-Mead’ is slightly more likely to yield valid estimates of standard errors than using the default method (see Technical notes).\n\ntransect.fit &lt;- secr.fit(transectCH, mask = transectmask, \n    binomN = 0, method = 'Nelder-Mead', trace = FALSE)\n\nOccasional ‘ier’ error codes may be ignored (see Technical notes). The estimates are close to the true values except for sigma, which may be positively biased.\n\npredict (transect.fit)\n\n        link estimate SE.estimate      lcl     ucl\nD        log   1.8527    0.196674  1.50552  2.2798\nlambda0  log   1.0415    0.080433  0.89539  1.2114\nsigma    log  53.6290    2.313387 49.28317 58.3581\n\n\nAnother way to analyse transect data is to discretize it. We divide the transect into 25-m segments and then change the detector type. In the resulting capthist object the transect has been replaced by a series of proximity detectors, each at the midpoint of a segment.\n\nsnippedCH &lt;- snip(transectCH, by = 25)\nsnippedCH &lt;- reduce(snippedCH, outputdetector = 'proximity')\n\nThe same may be achieved with newCH &lt;- discretize(transectCH, spacing = 25). We can fit a model using the same mask as before. The result differs in the scaling of the lambda0 parameter, but in other respects is similar to that from the transect model.\n\nsnipped.fit &lt;- secr.fit(snippedCH, mask = transectmask, \n    detectfn = 'HHN', trace = FALSE)\npredict(snipped.fit)\n\n        link estimate SE.estimate      lcl      ucl\nD        log  1.84217    0.195625  1.49689  2.26708\nlambda0  log  0.19312    0.018034  0.16089  0.23182\nsigma    log 53.67901    2.321128 49.31908 58.42436",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Area and transect search</span>"
    ]
  },
  {
    "objectID": "A04-polygondetectors.html#sec-polygonshape",
    "href": "A04-polygondetectors.html#sec-polygonshape",
    "title": "Appendix D — Area and transect search",
    "section": "\nD.8 More on polygons",
    "text": "D.8 More on polygons\nThe implementation in secr allows any number of disjunct polygons or non-intersecting transects.\nPolygons may be irregularly shaped, but there are some limitations in the default implementation. Polygons may not be concave in an east-west direction, in the sense that there are more than two intersections with a vertical line. Sometimes east-west concavity may be fixed by rotating the polygon and its associated data points (see function rotate). Polygons should not contain holes, and the polygons used on any one occasion should not overlap.\n\n\n\n\nThe polygon on the left is not allowed because its boundary is intersected by a vertical line at more than two points.\n\n\n\n\nD.8.1 Solutions for non-conforming polygons\n\nBreak into parts\n\nOne solution to ‘east-west concavity’ is to break the offending polygon into two or more parts. For this you need to know which vertices belong in which part, but that is (usually) easily determined from a plot. In this real example we recognise vertices 11 and 23 as critical, and split the polygon there. Note the need to include the clip vertices in both polygons, and to maintain the order of vertices. Both example2 and newpoly are traps objects.\n\nfile2 &lt;- paste0(datadir, '/polygonexample2.txt')\nexample2 &lt;- read.traps(file = file2, detector = 'polygon')\npar(mfrow = c(1,2), mar = c(2,1,1,1))\nplot(example2)\ntext(example2$x, example2$y, 1:nrow(example2), cex = 0.7)\nnewpoly &lt;- make.poly (list(p1 = example2[11:23,], \n  p2 = example2[c(1:11, 23:27),]))\n\nNo errors found :-)\n\nplot(newpoly, label = TRUE)\n\n\n\nSplitting a non-conforming polygon\n\n\n\nAttributes such as covariates and usage must be rebuilt by hand.\n\nPointwise testing\n\nAnother solution is to evaluate whether each point chosen dynamically by the integration code lies inside the polygon of interest. This is inevitably slower than the default algorithm that assumes all points between the lower and upper intersections lie within the polygon. Select the slower, more general option by setting details = list(convexpolygon = FALSE).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Area and transect search</span>"
    ]
  },
  {
    "objectID": "A04-polygondetectors.html#sec-polygontechnotes",
    "href": "A04-polygondetectors.html#sec-polygontechnotes",
    "title": "Appendix D — Area and transect search",
    "section": "\nD.9 Technical notes",
    "text": "D.9 Technical notes\nFitting models for polygon detectors with secr.fit requires the hazard function to be integrated in two-dimensions many times. In secr &gt;= 4.4 this is done with repeated one-dimensional Gaussian quadrature using the C++ function ‘integrate’ in RcppNumerical (Qiu et al., 2023).\nPolygon and transect SECR models seem to be prone to numerical problems in estimating the information matrix (negative Hessian), which flow on into poor variance estimates and missing values for the standard errors of ‘real’ parameters. At the time of writing these seem to be overcome by overriding the default maximization method (Newton-Raphson in ‘nlm’) and using, for example, “method = ‘BFGS’”. Another solution, perhaps more reliable, is to compute the information matrix independently by setting ‘details = list(hessian = ’fdhess’)’ in the call to secr.fit. Yet another approach is to apply secr.fit with “method = ‘none’” to a previously fitted model to compute the variances.\nThe algorithm for finding a starting point in parameter space for the numerical maximization is not entirely reliable; it may be necessary to specify the ‘start’ argument of secr.fit manually, remembering that the values should be on the link scale (default log for D, lambda0 and sigma).\nData for polygons and transects are unlike those from detectors such as traps in several respects:\n\nThe association between vertices in a ‘traps’ object and polygons or transects resides in an attribute ‘polyID’ that is out of sight, but may be retrieved with the polyID or transectID functions. If the attribute is NULL, all vertices are assumed to belong to one polygon or transect.\nThe x-y coordinates for each detection are stored in the attribute ‘detectedXY’ of a capthist object. To retrieve these coordinates use the function xy. Detections are ordered by occasion, animal, and detector (i.e., polyID).\nsubset or split applied to a polygon or transect ‘traps’ object operate at the level of whole polygons or transects, not vertices (rows).\nusage also applies to whole polygons or transects. The option of specifying varying usage by occasion is not fully tested for these detector types.\nThe interpretation of detection functions and their parameters is subtly different; the detection function must be integrated over 1-D or 2-D rather than yielding a probability directly (see Efford (2011)).\n\n\n\n\nLocations of horned lizards on a 9-ha plot in Arizona (Royle & Young, 2008). Grid lines are 100 m apart.\nFigure D.1: Simulated cue data from a single search of two irregular polygons.\nDiscretized area search data\nFigure D.2: Habitat mask (grey dots) and simulated transect data from five searches of a 2.8-km transect. Colours differ between individuals, but are not unique.\nThe polygon on the left is not allowed because its boundary is intersected by a vertical line at more than two points.\nSplitting a non-conforming polygon\n\n\n\nDorazio, R. M. (2013). Bayes and empirical bayes estimators of abundance and density from spatial capture-recapture data. PLoS ONE, 8, e84017.\n\n\nEfford, M. G. (2011). Estimation of population density by spatially explicit capture-recapture analysis of data from area searches. Ecology, 92, 2202–2207.\n\n\nKendeigh, S. C. (1944). Measurement of bird populations. Ecological Monographs, 14, 67–106. https://doi.org/10.2307/1961632\n\n\nMarques, T. A., Thomas, L., & Royle, J. A. (2011). A hierarchical model for spatial capture-recapture data: comment. Ecology, 92, 526–528.\n\n\nMilleret, C., Dupont, P., Brøseth, H., Kindberg, J., Royle, J. A., & Bischof, R. (2018). Using partial aggregation in spatial capture recapture. Methods in Ecology and Evolution, 9(8), 1896–1907. https://doi.org/10.1111/2041-210x.13030\n\n\nPaterson, J. T., Proffitt, K., Jimenez, B., Rotella, J., & Garrott, R. (2019). Simulation-based validation of spatial capture-recapture models: A case study using mountain lions. PLOS ONE, 14(4), e0215458. https://doi.org/10.1371/journal.pone.0215458\n\n\nQiu, Y., Balan, S., Beall, M., Sauder, M., Okazaki, N., & Hahn, T. (2023). RcppNumerical: ’Rcpp’ integration for numerical computing libraries. https://CRAN.R-project.org/package=RcppNumerical\n\n\nRoyle, J. A., & Young, K. V. (2008). A hierarchical model for spatial capture-recapture data. Ecology, 89, 2281–2289.\n\n\nRussell, R. E., Royle, J. A., Desimone, R., Schwartz, M. K., Edwards, V. L., Pilgrim, K. P., & Mckelvey, K. S. (2012). Estimating abundance of mountain lions from unstructured spatial sampling. The Journal of Wildlife Management, 76(8), 1551–1561. https://doi.org/10.1002/jwmg.412",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Area and transect search</span>"
    ]
  },
  {
    "objectID": "A04-polygondetectors.html#footnotes",
    "href": "A04-polygondetectors.html#footnotes",
    "title": "Appendix D — Area and transect search",
    "section": "",
    "text": "The logic here is that hazards are additive whereas probabilities are not.↩︎\nFor constraints on the shape of polygon detectors see Polygon shape↩︎\nAlternatively, one can construct a mask with make.mask and provide that in the ‘mask’ argument of secr.fit. Note that make.mask defaults to type = 'rectangular'; see Transect search for an example in which points are dropped if they are within the rectangle but far from detectors (the default in secr.fit)↩︎",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Area and transect search</span>"
    ]
  },
  {
    "objectID": "A05-mark-resight.html",
    "href": "A05-mark-resight.html",
    "title": "Appendix E — Mark-resight models",
    "section": "",
    "text": "E.1 How is mark-resight different?\nIn capture–recapture, all newly detected unmarked animals become marked and are distinguishable in future. Some field protocols also involve ‘resighting’ occasions on which previously marked animals are identified but newly detected animals are not marked. We call these ‘mark–resight’ data. McClintock & White (2012) provide an excellent summary of non-spatial mark–resight methods. The extension of spatially explicit capture–recapture models for mark–resight data was begun by Sollmann et al. (2013), and spatial mark–resight models were applied by Rich et al. (2014) and Rutledge et al. (2015). Whittington et al. (2018) describe a major Bayesian application.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Mark-resight models</span>"
    ]
  },
  {
    "objectID": "A05-mark-resight.html#distribution-of-marked-animals",
    "href": "A05-mark-resight.html#distribution-of-marked-animals",
    "title": "Appendix E — Mark-resight models",
    "section": "\nE.2 Distribution of marked animals",
    "text": "E.2 Distribution of marked animals\nThe historical development of mark–resight models has stressed the resighting component of mark–resight data; the sampling process leading to marking is not modelled. Modelling of the sighting data then requires some assumption about the distribution of the marked animals. Most commonly, marked animals are assumed to be a random sample from a defined population, and that may be achievable in some situations. An important case is when some animals carry natural marks and others don’t, as in the puma study of Rich et al. (2014).\nHowever, when a spatially distributed population is sampled with localized detectors such as traps, animals living near the traps have a higher chance of becoming marked, and the marked animals are not representative of a well-defined population. The difficulty may be overcome by spatially modelling both the marking and resighting processes. This requires that data on detection locations were collected during the marking phase, and that the spatial distribution of marking effort was recorded. Matechou et al. (2013) analysed non-spatial capture–recapture–resighting data jointly with counts of unmarked individuals, and the core capture–mark–resight models in secr are their spatial equivalent (see also Whittington et al. (2018)).\nRather than modelling the marking process we may take the traditional path, i.e. assume a certain spatial distribution for the marked individuals. Typically, the distribution is assumed to be uniform over a known area (e.g., Sollmann et al. (2013), Rich et al. (2014)). The number of marked individuals remaining at the time of resighting may be known or unknown. We use ‘capture–mark–resight’ for models that include the marking process and ‘sighting-only’ for models in which the spatial distribution of marks is assumed.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Mark-resight models</span>"
    ]
  },
  {
    "objectID": "A05-mark-resight.html#overview-of-implementation-in-secr",
    "href": "A05-mark-resight.html#overview-of-implementation-in-secr",
    "title": "Appendix E — Mark-resight models",
    "section": "\nE.3 Overview of implementation in secr\n",
    "text": "E.3 Overview of implementation in secr\n\nThe implementation in secr follows Efford & Hunter (2018). The models discard some spatial information in the unmarked sightings – information that is used in the models of Chandler & Royle (2013) and Sollmann et al. (2013). This results in some (probably small) loss of precision, and requires an adjustment for overdispersion to ensure confidence intervals have good coverage properties.\nFollowing Efford & Hunter (2018),\n\nthe population is assumed to follow a (possibly inhomogeneous) Poisson distribution in space\ndetection histories of marked animals are modelled as usual in SECR\ncounts of unmarked sightings are modelled with a Poisson distribution\noverdispersion in the counts (relative to a Poisson distribution) is estimated by simulation and used in a pseudo-likelihood addressed; this has minimal effect on the estimates themselves, but improves coverage of confidence intervals\n\nUnidentified sightings of marked animals are treated as independent of identified sightings. This is an approximation, but its effect on estimates is believed to be negligible.\nThe core models are listed in Table E.1. These may be customized in various ways, particularly by specifying the assumed distribution of marked animals for sighting-only models. Each model may be fitted with or without a parameter for incomplete identification of marked animals when they are resighted (pID).\n\n\nTable E.1: Classification of mark–resight models available in secr. ‘Capture–mark–resight’ models include the marking process; ‘sighting-only’ models rest on prior knowledge or assumptions regarding where animals have been marked. For sighting-only data some detection histories may be all-zero.\n\n\n\n\n\n\n\n\nData\nPre-marking1\n\nCode\n\n\n\nCapture–mark–resight\nnone\nNA\n\n\nSighting-only\nKnown n_0\n\ndetails = list(knownmarks = TRUE)\n\n\n\nUnknown n_0\n\ndetails = list(knownmarks = FALSE)\n\n\n\n\n\n\n\n\nn_0 is the number of marked individuals in the study area at the time of the sighting surveys.\n\nMark–resight data are represented in secr by a capthist data object, with minor extensions (Table E.2). We describe the structure for a single-session capthist object, which records detections of each marked animal over S occasions at any of K detectors. Detectors on sighting occasions may be of the usual binary proximity (‘proximity’) or count proximity (‘count’) types1. Detectors on marking occasions may be any combination of these types and multi-catch traps. Area search (‘polygon’) and transect search (‘transect’) types are not yet supported for either phase when there are sighting occasions: discretization is recommended.\n\n\n\n\n\n\nWarning\n\n\n\nNot all detector types and options in secr will work with mark–resight data. See Limitations for a list of known constraints (some will be mentioned along the way). See the step-by-step guide for a quick introduction.\n\n\n\n\nTable E.2: Special attributes of data objects for mark–resight analysis in secr.\n\n\n\nObject\nAttribute\nDescription\n\n\n\ntraps\nmarkocc\nOccasions may be marking (1), sighting (0) or unresolved (-1)\n\n\ncapthist\nTu\nCounts of unmarked animals\n\n\ncapthist\nTm\nCounts of marked animals not identified\n\n\ncapthist\nTn\nCounts of sightings with unknown mark status (not modelled)\n\n\nmask\n\nmarking1\n\nDistribution of pre-marked animals (sighting-only data; optional)\n\n\n\n\n\n\n\n\nmarking is the name of a special mask covariate, accessed like other covariates.\n\n\nE.3.1 Marking and sighting occasions\nSampling occasions (intervals) are either marking occasions or sighting occasions. On marking occasions any recaptured animals are also recorded2. On sighting occasions newly caught animals are released unmarked. The body of the capthist object has one row for each individual detected on at least one marking occasion, and one column for each occasion, whether marking or sighting.\nThe ‘traps’ object, a required attribute of any capthist object, has its own optional attribute markocc to distinguish marking occasions (1) from sighting occasions (0) (e.g., c(1,0,0,0,0) for marking on one occasion followed by four resighting occasions). A capthist object is recognised as a mark–resight dataset if its traps has a markocc attribute with at least one sighting occasion (i.e. !all(markocc)). Marking occasions and sighting occasions may be interspersed.\n\nE.3.2 Sighting-only data\nA sighting-only dataset has a markocc attribute with no marking occasions. Two scenarios are possible: either the capthist object includes a sighting history for each marked animal (including all-zero histories for any not re-sighted), or it includes sighting histories only for the re-sighted animals. In the latter case the number of marked animals at the time of sampling is unknown, and the fitted model must take account of this (see Number of marks unknown).\n\nE.3.3 Sightings of unmarked animals\nIn addition to the marking and sighting events of known individuals recorded in the body of the capthist object, there will usually be sightings of unmarked animals. These data take the form of a matrix with the number of detections of unmarked animals at each detector (row) on each sampling occasion (column). A single searched polygon or transect is one detector and hence contributes a single row. Columns corresponding to marking occasions are all-zero. This matrix is stored as attribute Tu of the capthist object.\nInstead of providing Tu as a matrix, the counts may be summed over occasions (Tu is a vector of detector-specific counts), or provided as the grand total (Tu is a single integer). These alternatives have definite limitations:\n\noccasion-specific or detector-specific models cannot be fitted\nplot, summary and verify are less informative\ncannot subset by occasion or detector\nAIC should not be used compare models differing in summarisation\ncannot mix markocc -1 and 0\n\nE.3.4 Unidentified sightings of marked animals\nAn observer may be able to to determine that an animal is marked, but be unable to positively identify it as a particular individual. The number of sightings of marked but unidentified individuals is stored as capthist attribute Tm, which uses the same formats as Tu.\n\nE.3.5 Sightings of unknown status\nSightings for which the mark status could not be determined are a further category of sighting on sighting occasions. For example, the identifying mark may not be visible in a photograph because of the orientation the animal. The number of sightings with unknown mark status is generally not used in the models (but see unresolved sightings). However, it is good practice to account for these observations. They may be stored as capthist attribute Tn and will appear in summaries.\n\nE.3.6 Unresolved sightings\nSometimes the sighting method does not allow marked animals to be distinguished from unmarked animals. Sighting occasions of this type are coded with ‘-1’ in the markocc vector. On these occasions the counts of all sightings (‘unresolved sightings’) stored in the appropriate column of Tn, the corresponding columns of Tu and Tm are all-zero, and no sightings of marked animals appear in the capthist object. Models comprising one or more marking occasions and only unresolved sightings (e.g., markocc &lt;- c(1, -1, -1, -1, -1)) may be fitted in secr, but there are constraints (see Sighting without attention to marking).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Mark-resight models</span>"
    ]
  },
  {
    "objectID": "A05-mark-resight.html#data-preparation",
    "href": "A05-mark-resight.html#data-preparation",
    "title": "Appendix E — Mark-resight models",
    "section": "\nE.4 Data preparation",
    "text": "E.4 Data preparation\nData on marked animals may be formatted and read as usual with read.capthist (secr-datainput.pdf). Each identified sighting of a marked individuals appears as a row in the ‘capture’ file, like any other detection. The markocc attribute may be set as an argument of read.capthist or assigned later.\nSighting data for unmarked animals (Tu) are provided separately as a K \\times S matrix, where K is the number of detectors and S is the total number of occasions (including marking occasions). Elements in the matrix are either binary (0/1) for proximity detectors, or whole numbers (0, 1, 2,…) for count, polygon and transect detectors. Sightings of marked animals that are not identified to individual (Tm) are optionally provided in a separate K \\times S matrix. Usually you will read these data from separate text files or spreadsheets. Sightings with unknown mark status (Tn) may also be provided, but will be ignored in analyses except for occasions with code -1 in markocc.\nThe function addSightings may be used to merge these matrices of sighting data with an existing mark–resight capthist object, i.e. to set its attributes Tu and Tm. There are also custom extraction and replacement functions for sighting-related attributes (markocc, Tu and Tm). addSightings also allows input from text files in which the first column is a session identifier (see Step-by-step guide for an example).\nLet’s assume you have prepared the files MRCHcapt.txt, MRCHcapt.txt, Tu.txt and Tm.txt. Simulated examples can be found in the extdata folder of the secr distribution (version 4.4.3 and above). This code can be used to build a mark–resight capthist object:\n\nlibrary(secr)\nolddir &lt;- setwd(system.file(\"extdata\", package = \"secr\"))\nMRCH &lt;- read.capthist(\"MRCHcapt.txt\", \"MRCHtrap.txt\", detector = \n    c(\"multi\", rep(\"proximity\",4)), markocc = c(1,0,0,0,0))\nMRCH &lt;- addSightings(MRCH, \"Tu.txt\", \"Tm.txt\")\nsession(MRCH) &lt;- \"Simulated mark-resight data\"\nsetwd(olddir)\n\n\nE.4.1 Data summary\nThe summary method for capthist objects recognises mark–resight data and provides a summary of the markocc, Tu, and Tm attributes.\n\nsummary(MRCH)\n\nObject class       capthist \nDetector type      multi, proximity (4) \nDetector number    36 \nAverage spacing    20 m \nx-range            0 100 m \ny-range            0 100 m \n\nMarking occasions\n 1 2 3 4 5\n 1 0 0 0 0\n\nCounts by occasion \n                   1  2  3  4  5 Total\nn                 70 15 16 19 17   137\nu                 70  0  0  0  0    70\nf                 28 24 13  3  2    70\nM(t+1)            70 70 70 70 70    70\nlosses             0  0  0  0  0     0\ndetections        70 17 18 28 23   156\ndetectors visited 31 14 15 18 13    91\ndetectors used    36 36 36 36 36   180\n\nSightings by occasion \n          1  2  3  4  5 Total\nID        0 17 18 28 23    86\nNot ID    0  8  5 11  7    31\nUnmarked  0  7  9  6  9    31\nUncertain 0  0  0  0  0     0\nTotal     0 32 32 45 39   148\n\n\nThe main table of counts is calculated differently for sighting-only and capture–mark–resight data. When markocc includes marking occasions the counts refer to marking, recaptures and sightings of marked animals. This means that u (number of new individuals) is always zero on sighting occasions, and M(t+1) increases only on marking occasions. When markocc records that the data are sighting-only, the main table of counts summarises identified sightings of previously marked animals, accumulating u and M(t+1) as if all occasions were marking occasions.\nOn sighting occasions the row ‘detections’ in Counts by occasion corresponds to the number of sightings of marked and identified individuals (‘ID’) in Sightings by occasion. Other detections in the sightings table are additional to the main table.\nThe default plot of a mark–resight capthist object shows only marked and identified individuals. To make a separate plot of the sightings of unmarked individuals, either use the same function with type = \"sightings\" or use sightingPlot:\n\npar(mar = c(1,3,3,3), mfrow = c(1,2), pty = 's')\n# petal plot with key\nplot(MRCH, type = \"sightings\", border = 20)\noccasionKey(MRCH, cex = 0.7, rad = 5, px = 0.96, py = 0.88, \n            xpd = TRUE)\n# bubble plot\nsightingPlot(MRCH, \"Tu\", mean = FALSE, border = 20, fill = TRUE, \n    col = \"blue\", legend = c(1,2,3), px = 1.0, py = c(0.85,0.72),\n    xpd = TRUE)\n\n\n\n\n\n\nFigure E.1: Two ways to plot sightings of unmarked animals. Petals in the lefthand plot indicate the number of sightings, except that marking occcasions are shown as a solid dot. mean = FALSE causes sightingPlot to use the total over all occasions rather than the mean. Other settings are used here only to customise the layout and colour of the plots.\n\n\n\n\n\nE.4.2 Data checks\nThe verify method for capthist objects checks that mark–resight data satisfy\n\nthe length of markocc matches the number of sampling occasions S\n\nanimals are not marked on sighting occasions, or sighted on marking occasions\nthe dimensions of matrices with counts of sighted animals (Tu, Tm) match the rest of the capthist object\nsightings are made only when detector usage is &gt; 0\ncounts are whole numbers.\n\nChecks are performed by default when a model is fitted with secr.fit, or when verify is called independently.\n\nE.4.3 Different layouts for marking and sighting\nDetector layouts are likely to differ between marking and sighting occasions. This can be accommodated within a single, merged detector layout using detector- and occasion-specific usage codes. See here for a function to create a single-session mark–resight traps object from two components.\n\nE.4.4 Simulating mark–resight data\nMark–resight data may be generated with function sim.resight. In this example, one grid of detectors is used for both marking and for resighting, but the detection rate is higher on the initial (marking) occasion.\n\ngrid &lt;- make.grid(detector = c(\"multi\", rep(\"proximity\",4)))\nmarkocc(grid) &lt;- c(1, 0, 0, 0, 0)\ng0mr &lt;- c(0.3, 0.1, 0.1, 0.1, 0.1)\nsimMRCH &lt;- sim.resight(grid, detectpar=list(g0 = g0mr, sigma = 25),  \n    popn = list(D = 30), pID = 0.7)\n\nIf all elements of markocc are zero then a separate mechanism is used to pre-mark a fraction pmark of individuals selected at random from the population. The resulting sighting-only data may include all-zero sighting histories for marked animals. Use the argument markingmask to simulate marking of animals with centres in a certain subregion.\n\nE.4.5 Input of sighting-only data\nFor sighting-only data we postulate that the marked animals are a random sample of individuals from a known spatial distribution, possibly a specified region. The body of the capthist object then comprises sightings of known individuals, with other sightings in attributes Tu and Tm. Marked animals that were known to be present but not resighted are included as ‘all-zero’ detection histories in the body of the capthist object. These detection histories are input by including a record in the detection file for each marked-but-unsighted individual, distinguished by 0 (zero) in the ‘occasion’ field, and with an arbitrary (but not missing) detector ID or coordinates in the next field(s).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Mark-resight models</span>"
    ]
  },
  {
    "objectID": "A05-mark-resight.html#model-fitting",
    "href": "A05-mark-resight.html#model-fitting",
    "title": "Appendix E — Mark-resight models",
    "section": "\nE.5 Model fitting",
    "text": "E.5 Model fitting\nProperly prepared, a capthist object includes all the data for fitting a mark–resight model3. It is only necessary to call secr.fit. Here is an example in which we use a coarse mask (nx = 32 instead of the default nx = 64) to speed things up. The predictor ‘ts’ and the new ‘real’ parameter pID are explained in following sections.\n\nmask &lt;- make.mask(traps(MRCH), nx = 32, buffer = 100)\nfit0 &lt;- secr.fit(MRCH, model = list(g0 ~ ts), mask = mask, \n    trace = FALSE)\npredict(fit0, newdata = data.frame(ts = factor(c(\"M\",\"S\"))))\n\n$`ts = M`\n       link estimate SE.estimate      lcl      ucl\nD       log 25.87863    3.410206 20.01037 33.46783\ng0    logit  0.37635    0.053074  0.27923  0.48454\nsigma   log 25.30921    1.972903 21.72832 29.48024\npID   logit  0.67424    0.054842  0.55923  0.77150\n\n$`ts = S`\n       link  estimate SE.estimate      lcl      ucl\nD       log 25.878634    3.410206 20.01037 33.46783\ng0    logit  0.093003    0.012526  0.07119  0.12063\nsigma   log 25.309209    1.972903 21.72832 29.48024\npID   logit  0.674239    0.054842  0.55923  0.77150\n\n\nThis example relies on the built-in autoini function to provide starting parameter values for the numerical maximization. However, the automatic values may not work with complex mark–resight models. It is not worth continuing if the first likelihood evaluation fails, as indicated by LogLik NaN when trace = TRUE. Then you should experiment with the start argument until you find values that work.\n\nE.5.1 Adjustment for overdispersion\nThe default confidence intervals have poor coverage when the data include counts in Tu or Tm. This is due to overdispersion of the summed counts, caused by a quirk of the model specification: covariation is ignored in the multivariate Poisson model used to describe the counts. A simple protocol fixes the problem:\n\nFit a model assuming no overdispersion,\nEstimate the overdispersion by simulating at the initial estimates, and\nRe-fit the model (or at least re-estimate the variance-covariance matrix) using an overdispersion-adjusted pseudo-likelihood.\n\nSteps (2) and (3) may be performed together in one call to secr.fit. In the example below, the first line repeats the model specification, while the second line provides the previously fitted model as a starting point and calls for nsim simulations to estimate overdispersion (c-hat in MARK parlance).\n\nfit1 &lt;- secr.fit(MRCH, model = list(g0~ts), mask = mask, \n    trace = FALSE, start = fit0, details = list(nsim = 10000))\n\nsims completed\n\npredict(fit1, newdata = data.frame(ts = factor(c(\"M\",\"S\"))))\n\n$`ts = M`\n       link estimate SE.estimate      lcl      ucl\nD       log 24.13195    3.392929 18.34423 31.74573\ng0    logit  0.44590    0.077217  0.30368  0.59755\nsigma   log 25.55800    2.037723 21.86596 29.87343\npID   logit  0.66675    0.064273  0.53160  0.77910\n\n$`ts = S`\n       link  estimate SE.estimate       lcl      ucl\nD       log 24.131951    3.392929 18.344231 31.74573\ng0    logit  0.094439    0.013448  0.071174  0.12429\nsigma   log 25.557999    2.037723 21.865962 29.87343\npID   logit  0.666747    0.064273  0.531598  0.77910\n\n\nFor this dataset the adjustment had a small effect on the confidence intervals (0% increase in interval length). The estimates of overdispersion are saved in the details$chat component of the fitted model, and we can see that they are close to 1.0. Note that the adjustment is strictly for overdispersion in the unmarked sightings, and the effect of that overdispersion is ‘diluted’ by other components of the likelihood.\n\nfit1$details$chat[1:2]\n\n    Tu     Tm \n1.9651 1.6944 \n\n\nA shortcut is to specify method = \"none\" at Step 3 to block re-maximization of the pseudolikelihood: then only the variance-covariance matrix, likelihood and related parameters will be re-computed. Pre-determined values of \\hat c may be provided as input in the chat component of details; if nsim = 0 the input chat will be used without further simulation (example in Step-by-step guide Step 8).\nStochasticity in the estimate of overdispersion causes parameter estimates to vary. The problem is minimized by running many simulations (say, nsim = 10000), which has relatively little effect on total execution time.\nCAVEAT: the multi-threaded simulation code does not allow for potential non-independence of random number streams across threads. If this is a concern then set ncores = 1 to block multi-threading. See also ?secrRNG.\n\nE.5.2 Different parameter values on marking and sighting occasions\nMarking and sighting occasions share parameters by default. This may make sense for the spatial scale parameter (sigma ~ 1), but it is unlikely to hold for the baseline rate (intercept) g0 or lambda0. Density estimates can be very biased if the difference is ignored4. A new canned predictor ‘ts’ is introduced to distinguish marking and sighting occasions. For example g0 ~ ts will fit two levels of g0, one for marking occasions and one for sighting occasions. The same can be achieved with g0 ~ tcov, timecov = factor(2-markocc).\nTo see the values associated with each level of ‘ts’ (i.e., marking and sighting occasions) we specify the newdata argument of predict.secr, as in Adjustment for overdispersion.\n\nE.5.3 Proportion identified\nIf some sightings are of marked animals that cannot be identified to individual then a further ‘real’ parameter is required for the proportion identified. This is called pID in secr. pID is estimated by default, whether or not the capthist object has attribute Tm, because failure to identify marked animals affects the re-detection probability of marked individuals. For data with a single initial marking occasion, a model estimating pID is exactly equivalent to a learned response model (g0 ~ b) in the absence of Tm (same density estimate, same likelihood).\npID may be fixed at an arbitrary value, just like other real parameters. For example, a call to secr.fit with fixed = list(pID = 1.0) implies that every sighted individual that was already marked could be identified.\nOnly a subset of the usual predictors is appropriate for pID. Use of predictors other than session, Session, or the names of any session or trap covariate will raise a warning.\n\n\n\n\n\n\n\n\n\n\n\n\nE.5.4 Specifying distribution of pre-marked animals (sighting-only models)\nBy default, animals in a sighting-only dataset are assumed to have been marked throughout the habitat mask. However, uniform marking across a subregion of the habitat mask, or any other known distribution, may be specified by including a mask covariate named ‘marking’. For example,\n\nmask &lt;- make.mask(grid)\nd &lt;- distancetotrap(mask, grid)\ncovariates(mask) &lt;- data.frame(marking = d &lt; 60)\n\nIf the mask is used in secr.fit with a sighting-only dataset, the fitted model will ‘understand’ that animals centred within 60 m of the grid had a uniform probability of marking, and no animals from beyond this limit were marked. Note that the area is supposed to contain the centres of all marked animals; this is not the same as a searched polygon. If the covariate is missing then all animals in the mask are assumed to have had an equal chance of becoming marked. The ‘marking’ covariate is actually more general, in that it can represent any known or assumed probability distribution for the locations of marked animals within the mask: values are automatically normalized by dividing by their sum. Be warned that assumptions about the extent of marking directly affect the estimated density.\nIt is possible in principle that the marked animals are distributed over an area larger than the habitat mask. The software does not directly allow for this. An easy solution is to increase the size of the habitat mask, but take care to control its coarseness (spacing), and be aware of the effect on estimates of N, and on the sampling variance if distribution = \"binomial\".\n\nE.5.5 Number of marks unknown (sighting-only models)\nSighting-only datasets may entail the further complication that the number of marked animals in the population at the time of resighting is unknown (Model 3). This can happen if enough time has passed for some marked animals to have died or emigrated. It also arises if individuation relies on unique natural marks, but these are missing from some individuals (Rich et al. 2014).\nThe resighting data do not include any all-zero detection histories, as the only way to know for certain that a marked animal is still present is to sight it at least once. The theory nevertheless allows a spatial model to be fitted5.\nData preparation is exactly as for sighting-only models with known number of marks, except that there are no all-zero histories. The unknown-marks model is fitted by setting the secr.fit argument details = list(knownmarks = FALSE), (possibly along with other details components).\n\nE.5.6 Covariates, groups and finite mixtures\nConditional-likelihood models are generally not useful for mark-resight analyses. Individual covariates therefore should not be used in mark-resight models in secr. Other covariates (at the level of session, occasion, or detector) should work in any of the models.\nThe mark–resight models in secr do not currently allow groups (g). Finite mixtures, including ‘hcov’ models, are only partly implemented.\n\nE.5.7 Discarding unmarked sightings\nSighting data can be highly problematic because of difficulty in reading marks and determining when consecutive sightings are independent. A conservative approach is to discard the counts of unmarked sightings (Tu) and unidentified marked animals (Tm), while retaining sighting data on marked animals. This works for capture–mark–resight data (marking occasions included), but not for all-sighting data (which rely on unmarked sightings to estimate detection parameters). The parameter pID is confounded with the sighting-phase g0 so we fix it at 1.0. The likelihood does not require adjustment for overdispersion of unmarked sightings.\n\nTu(MRCH) &lt;- NULL\nTm(MRCH) &lt;- NULL\nmask &lt;- make.mask(traps(MRCH), nx = 32, buffer = 100)\nfit2 &lt;- secr.fit(MRCH, model = list(g0~ts), fixed = list(pID = 1.0), \n    mask = mask, trace = FALSE)\npredict(fit2, newdata = data.frame(ts = factor(c(\"M\",\"S\"))))\n\n$`ts = M`\n       link estimate SE.estimate       lcl      ucl\nD       log  18.1896     2.96621 13.241198 24.98738\ng0    logit   0.9446     0.15068  0.056934  0.99979\nsigma   log  26.8962     2.32809 22.706517 31.85902\n\n$`ts = S`\n       link  estimate SE.estimate       lcl       ucl\nD       log 18.189635   2.9662052 13.241198 24.987378\ng0    logit  0.063549   0.0088513  0.048255  0.083266\nsigma   log 26.896234   2.3280869 22.706517 31.859020\n\n\nConfidence interval length has increased by -12% over the full estimates (we did throw out a lot of data), but we expect the result to be more robust.\n\nE.5.8 Comparing models\nFitted models may be compared by AIC or AICc. Models can be compared when they describe the same data. In practice this means\n\nDo not compare models with and without either sighting attribute (Tu, Tm), and\nDo not compare sighting-only models with different pre-marking assumptions (the marking covariate of the mask).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Mark-resight models</span>"
    ]
  },
  {
    "objectID": "A05-mark-resight.html#sec-MRlimitations",
    "href": "A05-mark-resight.html#sec-MRlimitations",
    "title": "Appendix E — Mark-resight models",
    "section": "\nE.6 Limitations",
    "text": "E.6 Limitations\nThese limitations apply when fitting mark–resight models in secr:\n\nonly ‘multi’, ‘proximity’ and ‘count’ detector types are allowed\ngroups (g) are not allowed\n\nhybrid mixtures with known class membership (‘hcov’) have not been implemented for mark-resight data\nsome secr functions do not yet handle mark–resight data or models. These are mostly flagged with a Warning on their documentation page.\n\nIf the sighting phase is an area or transect search then it may be possible to analyse the data by rasterizing the areas or transects (function discretize).\n\nE.6.1 Warning\nThe implementation of spatially explicit mark–resight models is still somewhat limited. It is intended that mark–resight models work across other capabilities of secr, particularly\n\ndata may span multiple sessions (multi-session capthist objects can contain mark–resight data)\nmark–resight may be augmented with telemetry data (addTelemetry), but joint telemetry models have not been tested and should be used with care\nfxiContour() and related functions (probability density plots of centres of detected animals)\nfinite mixture (h2) models are allowed for detection parameters, including pID\n\nmodelAverage() and collate()\nlinear habitat masks are allowed (as in the package secrlinear)\n\nHowever, these uses have not been tested much, where they have been tested at all..\nIt is unclear what sample size should be used for sample-size-adjusted AIC (AICc). This is one more reason to use AIC(..., criterion = \"AIC\"). Currently secr uses the number of detection histories of marked individuals, as for other models.\n\nE.6.2 Pitfalls\nThe likelihood for the sighting-only model with unknown number of marks (Model 3) has a boundary corresponding to the density of detected marks in the marking mask (true density cannot be less than this). This is ordinarily not a problem (estimated density will usually be larger). However, for multi-session data with a common density parameter it is quite possible for the number of marks in a particular session to exceed the threshold. The log-likelihood function then returns ‘NA’; although maximization may proceed, variance estimation is likely to fail. A possible (but slow) workaround is to combine the multiple sessions in a single-session capthist (you’re on your own here - there is no function for this).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Mark-resight models</span>"
    ]
  },
  {
    "objectID": "A05-mark-resight.html#sec-appendix1",
    "href": "A05-mark-resight.html#sec-appendix1",
    "title": "Appendix E — Mark-resight models",
    "section": "\nE.7 Step-by-step guide",
    "text": "E.7 Step-by-step guide\nThis guide should help you get started on a simple mark–resight analysis.\n\nDecide where your study fits in Table 1 and determine the markocc vector. Do you have data for the marking process (capture–mark–resight data), or will this be assumed (sighting-only data)? For capture–mark–resight data, markocc will have ‘1’ for modelled marking occasions and ‘0’ for sighting occasions (e.g., markocc(traps) &lt;- c(1,0,0,0,0) for one marking occasion and four sighting occasions). For sighting-only data, markocc is ‘0’ for every occasion (omit the quotation marks). If you have sighting-only data, is the number of marked animals known or unknown?\nPrepare a capture file and detector layout file as usual (secr-datainput.pdf). Sightings of marked animals are included as if they were recaptures. If your data are sighting-only and the number of marked animals is known then include a capture record with occasion = 0 for any marked animal that was never resighted. If your detector type is ‘count’ (sampling with replacement; repeat observations possible at each detector) then repeat data rows in the capture file as necessary.\n\nPrepare separate text files for sightings of unmarked animals (Tu) and unidentified sightings of marked animals (Tm). Each row has a session identifier followed by the number of sightings for one detector on each occasion (always zero in columns corresponding to marking occasions). The session identifier is used to split the file when the data span multiple sessions; it should be constant for a single-session capthist. The files will be read with read.table, so if necessary consult the help for that function. The start of a file for 1 marking occasion and 4 sighting occasions might look like this:\nS1 0 0 0 0 0  \nS1 0 1 0 2 0  \nS1 0 0 1 1 1  \nS1 0 0 0 1 0  \nS1 0 1 0 0 1  \nS1 0 0 2 0 0  \n...  \n\nInput data\n\n\nlibrary(secr)\ndatadir &lt;- system.file(\"extdata\", package = \"secr\")  # or choose your own\nolddir &lt;- setwd(datadir)\nCH &lt;- read.capthist(\"MRCHcapt.txt\", \"MRCHtrap.txt\", detector = \n    c(\"multi\", rep(\"proximity\",4)), markocc = c(1,0,0,0,0), \n    verify = FALSE)\nCH &lt;- addSightings(CH, \"Tu.txt\", \"Tm.txt\")\n\nNo errors found :-)\n\nsetwd(olddir)   # return to original folder\n\n\nReview data\n\n\nsummary(CH)\n\nObject class       capthist \nDetector type      multi, proximity (4) \nDetector number    36 \nAverage spacing    20 m \nx-range            0 100 m \ny-range            0 100 m \n\nMarking occasions\n 1 2 3 4 5\n 1 0 0 0 0\n\nCounts by occasion \n                   1  2  3  4  5 Total\nn                 70 15 16 19 17   137\nu                 70  0  0  0  0    70\nf                 28 24 13  3  2    70\nM(t+1)            70 70 70 70 70    70\nlosses             0  0  0  0  0     0\ndetections        70 17 18 28 23   156\ndetectors visited 31 14 15 18 13    91\ndetectors used    36 36 36 36 36   180\n\nSightings by occasion \n          1  2  3  4  5 Total\nID        0 17 18 28 23    86\nNot ID    0  8  5 11  7    31\nUnmarked  0  7  9  6  9    31\nUncertain 0  0  0  0  0     0\nTotal     0 32 32 45 39   148\n\n\nRPSV with CC = TRUE gives a crude estimate of the spatial scale parameter sigma.\n\nRPSV(CH, CC = TRUE)   \n\n[1] 20.234\n\n\n\nDesign an appropriate habitat mask, representing the region from which animals are potentially detected (sighted). Most simply this is got by buffering around the detectors, for example applying a 100-m buffer:\n\n\nmask &lt;- make.mask(traps(CH), nx = 32, buffer = 100)\n\n\nFit a null model and adjust for overdispersion\n\n\nfit0 &lt;- secr.fit(CH, mask = mask, trace = FALSE)\nfit0 &lt;- secr.fit(CH, mask = mask, trace = FALSE, start = fit0, \n                 details = list(nsim = 10000))\n\nsims completed\n\npredict(fit0)\n\n       link estimate SE.estimate      lcl      ucl\nD       log 17.17114    2.637593 12.72948 23.16260\ng0    logit  0.22142    0.034569  0.16105  0.29643\nsigma   log 24.51217    1.811371 21.21125 28.32678\npID   logit  0.28221    0.043641  0.20492  0.37489\n\n\n\nConsider other detection models\n\n\nmodel with different marking and sighting rates\n\n\nfitts &lt;- secr.fit(CH, model = g0 ~ ts, mask = mask, trace = FALSE,\n    details = list(chat = fit0$details$chat))\npredict(fitts, newdata = data.frame(ts = c(\"M\",\"S\")))\n\n$`ts = M`\n       link estimate SE.estimate      lcl      ucl\nD       log 22.18120    3.374696 16.49003 29.83654\ng0    logit  0.55702    0.123799  0.31989  0.77073\nsigma   log 25.90649    2.117242 22.07795 30.39894\npID   logit  0.65075    0.084782  0.47283  0.79470\n\n$`ts = S`\n       link  estimate SE.estimate       lcl      ucl\nD       log 22.181199    3.374696 16.490032 29.83654\ng0    logit  0.097687    0.016005  0.070498  0.13385\nsigma   log 25.906491    2.117242 22.077951 30.39894\npID   logit  0.650752    0.084782  0.472827  0.79470\n\n\n\nAIC(fit0, fitts)[,-c(2,6)]\n\n                        model npar  logLik    AIC   dAIC AICwt\nfitts D~1 g0~ts sigma~1 pID~1    5 -520.35 1050.7  0.000     1\nfit0   D~1 g0~1 sigma~1 pID~1    4 -551.03 1110.1 59.344     0\n\n\nBetter model fit was achieved by estimating different detection rates on marking and sighting occasions, and the estimates from the null model should be discarded. We now reveal that the data were simulated with D = 30 animals/ha, g0(marking) = 0.3, g0(sighting) = 0.1, sigma = 25 m and pID = 0.7: confidence intervals for the estimates from model ‘fitts’ comfortably cover these values.\nWe have used the initial null-model estimates of \\hat c to adjust the later model. It may be better to use the \\hat c of the larger (more general) model. The AIC values compared are strictly quasi-AIC values as they use the pseudolikelihood.\n\nAnalyses for other data types\n\n\nmarked animals all identified on resighting\n\nIn this case we would want to block estimation of the parameter pID by fixing it at 1.0:\n\nfit &lt;- secr.fit(CH, mask = mask, fixed = list(pID = 1), \n    trace = FALSE)\nfit &lt;- secr.fit(CH, mask = mask, fixed = list(pID = 1), \n    trace = FALSE, start = fit, details = list(nsim = 10000))\npredict(fit)\n\n\nsighting-only data with unknown number of marks (assume previous marking uniform throughout mask)\n\n\nfit &lt;- secr.fit(CH, mask = mask, trace = FALSE, \n                details = list(knownmarks = FALSE))\nfit &lt;- secr.fit(CH, mask = mask, trace = FALSE, \n    details = list(knownmarks = FALSE, nsim = 10000))\npredict(fit)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Mark-resight models</span>"
    ]
  },
  {
    "objectID": "A05-mark-resight.html#miscellaneous",
    "href": "A05-mark-resight.html#miscellaneous",
    "title": "Appendix E — Mark-resight models",
    "section": "\nE.8 Miscellaneous",
    "text": "E.8 Miscellaneous\n\nE.8.1 Sighting without attention to marking\nWhen sighting occasions never distinguish between marked and unmarked animals, the parameter pID is redundant and cannot be estimated. If no action is taken it will appear in the fitted model as the starting value (default 0.7) with zero variance. It is preferable to suppress this by fixing it to some arbitrary value (e.g., fixed = list(pID = 1.0)).\nThe contribution of unresolved sightings to the estimates is likely to be small.\n\nEstimation of the spatial scale parameter (sigma) rests entirely on recaptures of marked animals within and between any marking occasions. The unresolved counts carry a little spatial information (Chandler and Royle 2013), but this is discarded in the secr implementation and cannot contribute to estimating sigma (see also Adjustment for overdispersion).\nIt will usually be necessary to fit a distinct level of the intercept parameter lambda0 on the sighting occasions (perhaps using the predictor ts).\n\nNevertheless, it may be useful to model unresolved counts in a joint analysis of a larger dataset.\n\nE.8.2 Combining marking and sighting detector layouts\nHere is a function to create a single-session mark–resight traps object from two components. ‘trapsM’ and ‘trapsR’ are secr traps objects for marking and resighting occasions respectively. ‘markocc’ is an integer vector distinguishing marking (1) and sighting (-1, 0) occasions. The value returned is a combined traps object with usage attribute based on markocc. Detectors shared between the marking and sighting layouts are duplicated in the result; this may slow down model fitting, but it is otherwise not a problem.\n\ntrapmerge &lt;- function (trapsM, trapsR, markocc){\n    if (!is.null(usage(trapsM)) | !is.null(usage(trapsR)))\n        warning(\"discarding existing usage data\")\n    KM &lt;- nrow(trapsM)\n    KR &lt;- nrow(trapsR)\n    S &lt;- length(markocc)\n    notmarking &lt;- markocc&lt;1\n    newdetector &lt;- c(detector(trapsM)[1], \n                     detector(trapsR)[1])[notmarking+1]\n    detector(trapsM) &lt;- detector(trapsR)[1]\n    usage(trapsM) &lt;- matrix(as.integer(!notmarking), byrow = TRUE, \n                            nrow = KM, ncol = S)\n    usage(trapsR) &lt;- matrix(as.integer(notmarking), byrow = TRUE, \n                            nrow = KR, ncol = S)\n    trps &lt;- rbind(trapsM, trapsR)\n    # Note: nrow(trps) == KM + KR\n    detector(trps) &lt;- newdetector\n    markocc(trps) &lt;- markocc\n    trps\n}\n\nThis demonstration uses only 9 marking points and 25 re-sighting points.\n\ngridM &lt;- make.grid(nx = 3, ny = 3, detector = \"multi\")\ngridR &lt;- make.grid(nx = 5, ny = 5, detector = \"proximity\")\ncombined &lt;- trapmerge(gridM, gridR, c(1,1,0,0,0))\ndetector(combined)\n\n[1] \"multi\"     \"multi\"     \"proximity\" \"proximity\" \"proximity\"\n\n# show usage for first 16 detectors of 34 in the combined layout\nusage(combined)[1:16,]\n\n   1 2 3 4 5\n1  1 1 0 0 0\n2  1 1 0 0 0\n3  1 1 0 0 0\n4  1 1 0 0 0\n5  1 1 0 0 0\n6  1 1 0 0 0\n7  1 1 0 0 0\n8  1 1 0 0 0\n9  1 1 0 0 0\n10 0 0 1 1 1\n11 0 0 1 1 1\n12 0 0 1 1 1\n13 0 0 1 1 1\n14 0 0 1 1 1\n15 0 0 1 1 1\n16 0 0 1 1 1\n\n\n\nE.8.3 Simulation code\nThis code was used to simulate data for the initial demonstration.\n\nlibrary(secr)\ngrid &lt;- make.grid(detector = c(\"multi\", rep(\"proximity\",4)))\nmarkocc(grid) &lt;- c(1, 0, 0, 0, 0)\ng0mr &lt;- c(0.3, 0.1, 0.1, 0.1, 0.1)\nMRCH &lt;- sim.resight(grid, detectpar=list(g0 = g0mr, sigma = 25),  \n                    popn = list(D = 30), pID = 0.7, seed = 123)\n# write to files in working directory\nTu.char &lt;- paste (\"S1\", apply (Tu(MRCH), 1, paste, collapse = \" \"))\nTm.char &lt;- paste (\"S1\", apply (Tm(MRCH), 1, paste, collapse = \" \"))\nwrite.capthist(MRCH, filestem = \"MRCH\")\nwrite.table(Tu.char, file = \"Tu.txt\", col.names = F, row.names = F, \n    quote = FALSE)\nwrite.table(Tm.char, file = \"Tm.txt\", col.names = F, row.names = F, \n    quote = FALSE)\n\n\nE.8.4 Sighting-only example\n\nlibrary(secr)\ngrid &lt;- make.grid(detector = 'proximity')\nmarkocc(grid) &lt;- c(0, 0, 0, 0, 0)\nMRCH5 &lt;- sim.resight(grid, detectpar=list(g0 = 0.3, sigma = 25), \n    unsighted = TRUE, popn = list(D = 30), pID = 1.0, seed = 123)\nTm(MRCH5) &lt;- NULL\nsummary(MRCH5)\n\nObject class       capthist \nDetector type      proximity \nDetector number    36 \nAverage spacing    20 m \nx-range            0 100 m \ny-range            0 100 m \n\nMarking occasions\n 1 2 3 4 5\n 0 0 0 0 0\n\nCounts by occasion \n                   1  2  3  4  5 Total\nn                 26 27 25 23 31   132\nu                 26  6  4  4  3    43\nf                  8 11  5  8 11    43\nM(t+1)            26 32 36 40 43    43\nlosses             0  0  0  0  0     0\ndetections        48 57 49 52 62   268\ndetectors visited 25 28 27 25 34   139\ndetectors used    36 36 36 36 36   180\n\nEmpty histories :  75 \n\nSightings by occasion \n           1  2  3  4  5 Total\nID        48 57 49 52 62   268\nNot ID     0  0  0  0  0     0\nUnmarked  30 30 28 29 34   151\nUncertain  0  0  0  0  0     0\nTotal     78 87 77 81 96   419\n\nfit0 &lt;- secr.fit(MRCH5, fixed = list(pID = 1.0), trace = FALSE,\n                 details = list(knownmarks = TRUE))\nfit1 &lt;- secr.fit(MRCH5, fixed = list(pID = 1.0), start = fit0, \n                 details = list(nsim = 2000, knownmarks = TRUE),\n                 trace = FALSE)\n\nsims completed\n\n\n\nMRCH6 &lt;- sim.resight(grid, detectpar=list(g0 = 0.3, sigma = 25), \n    unsighted = FALSE, popn = list(D = 30), pID = 1.0, seed = 123)\nTm(MRCH6) &lt;- NULL\nsummary(MRCH6)\n\nObject class       capthist \nDetector type      proximity \nDetector number    36 \nAverage spacing    20 m \nx-range            0 100 m \ny-range            0 100 m \n\nMarking occasions\n 1 2 3 4 5\n 0 0 0 0 0\n\nCounts by occasion \n                   1  2  3  4  5 Total\nn                 26 27 25 23 31   132\nu                 26  6  4  4  3    43\nf                  8 11  5  8 11    43\nM(t+1)            26 32 36 40 43    43\nlosses             0  0  0  0  0     0\ndetections        48 57 49 52 62   268\ndetectors visited 25 28 27 25 34   139\ndetectors used    36 36 36 36 36   180\n\nSightings by occasion \n           1  2  3  4  5 Total\nID        48 57 49 52 62   268\nNot ID     0  0  0  0  0     0\nUnmarked  30 30 28 29 34   151\nUncertain  0  0  0  0  0     0\nTotal     78 87 77 81 96   419\n\nfit2 &lt;- secr.fit(MRCH6, fixed = list(pID = 1.0), trace = FALSE,\n                 details = list(knownmarks = FALSE))\n\n\n\n\nFigure E.1: Two ways to plot sightings of unmarked animals. Petals in the lefthand plot indicate the number of sightings, except that marking occcasions are shown as a solid dot. mean = FALSE causes sightingPlot to use the total over all occasions rather than the mean. Other settings are used here only to customise the layout and colour of the plots.\n\n\n\nChandler, R. B., & Royle, J. A. (2013). Spatially explicit models for inference about density in unmarked or partially marked populations. Annals of Applied Statistics, 7, 936–954.\n\n\nEfford, M. G., & Hunter, C. M. (2018). Spatial capture-mark-resight estimation of animal population density. Biometrics, 74, 411–420. https://doi.org/10.1111/biom.12766\n\n\nKendeigh, S. C. (1944). Measurement of bird populations. Ecological Monographs, 14, 67–106. https://doi.org/10.2307/1961632\n\n\nMatechou, E., Morgan, B. J. T., Pledger, S., Collazo, J. A., & Lyons, J. E. (2013). Integrated analysis of capture-recapture-resighting data and counts of unmarked birds at stopover sites. Journal of Agricultural, Biological and Environmental Statistics, 18, 120–135.\n\n\nMcClintock, B. T., & White, G. C. (2012). From NOREMARK to MARK: Software for estimating demographic parameters using mark-resight methodology. Journal of Ornithology, Supplement 2, 152, S641–S650.\n\n\nRich, L. N., Kelly, M. J., Sollmann, R., Noss, A. J., Maffei, L., Arispe, R. L., Paviolo, A., De Angelo, C. D., Blanco, D., E., Y., & Di Bitetti, M. S. (2014). Comparing capture-recapture, mark-resight, and spatial mark-resight models for estimating puma densities via camera traps. Journal of Mammalogy, 95, 382–391.\n\n\nRutledge, M. E., Sollmann, R., Washburn, B. E., Moorman, C. E., & DePerno, C. S. (2015). Using novel spatial mark-resight techniques to monitor resident canada geese in a suburban environment. Wildlife Research, 41, 447–453.\n\n\nSollmann, R., Gardner, B., Parsons, A. W., Stocking, J. J., McClintock, B. T., Simons, T. R., Pollock, K. H., & O’Connell, A. F. (2013). A spatial mark-resight model augmented with telemetry data. Ecology, 94, 553–559.\n\n\nWhittington, J., Hebblewhite, M., & Chandler, R. (2018). Generalized spatial mark-resight models with an application to grizzly bears. Journal of Applied Ecology, 55, 157–168.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Mark-resight models</span>"
    ]
  },
  {
    "objectID": "A05-mark-resight.html#footnotes",
    "href": "A05-mark-resight.html#footnotes",
    "title": "Appendix E — Mark-resight models",
    "section": "",
    "text": "‘Count’ detectors most closely approximate sampling with replacement (McClintock and White 2012); sampling strictly without replacement implies detection at no more than one site per occasion (detector type ‘multi’) that has yet to be implemented for mark–resight data, and may prove difficult.↩︎\nThere may also be marking occasions on which recaptures are ignored, but this possibility has yet to be modelled.↩︎\nA slight exception is the optional marking mask covariate used for sighting-only data.↩︎\nThis is not an issue for sighting-only models as the marked fraction is not modelled.↩︎\nThe number of marked individuals may be estimated as a derived parameter if required.↩︎",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Mark-resight models</span>"
    ]
  },
  {
    "objectID": "A06-noneuclidean.html",
    "href": "A06-noneuclidean.html",
    "title": "Appendix F — Non-Euclidean distances",
    "section": "",
    "text": "F.1 Basics\nThe Euclidean distance between points (x_1,y_1) and (x_2,y_2) is given by d = \\sqrt{(x_1-x_2)^2 + (y_1 - y_2)^2}. Non-Euclidean distances are defined in secr by setting the ‘userdist’ component of the ‘details’ argument of secr.fit. The options are (i) to provide a static (pre-computed) K \\times M matrix containing the distances between the K detectors and each of the M mask points, or (ii) to provide a function that computes the distances dynamically. A static distance matrix can allow for barriers to movement. Providing a function is more flexible and allows the estimation of a parameter for the distance model, but evaluating the function for each likelihood slows down model fitting.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Non-Euclidean distances</span>"
    ]
  },
  {
    "objectID": "A06-noneuclidean.html#static-userdist",
    "href": "A06-noneuclidean.html#static-userdist",
    "title": "Appendix F — Non-Euclidean distances",
    "section": "\nF.2 Static userdist",
    "text": "F.2 Static userdist\n\nA pre-computed non-Euclidean distance matrix may incorporate constraints on movement, particularly mapped barriers to movement, and this is the most obvious reason to employ a static userdist. The function nedist builds a suitable matrix.\nAs an example, take the 1996 DNA survey of the grizzly bear population in the Central Selkirk mountains of British Columbia by Mowat & Strobeck (2000). Their study area was partly bounded by lakes and reservoirs that we assume are rarely crossed by bears. To treat the lakes as barriers in a SECR model we need a matrix of hair snag – mask point distances for the terrestrial (non-Euclidean) distance between each pair of points.\nWe start with the hair snag locations CStraps and a SpatialPolygonsDataFrame object BLKS_BC representing the large lakes (Fig. F.1). Buffering 30 km around the detectors gives a naive mask (Fig. F.1 b); we use a 2-km pixel size and reject points centred in a lake. The shortest dry path from many points on the naive mask to the nearest detector is much longer than the straight line distance.\n\nCSmask2000 &lt;- make.mask (CStraps, buffer = 30000, \n    type = \"trapbuffer\", spacing = 2000, poly = BLKS_BC, \n    poly.habitat = FALSE, keep.poly = FALSE)\n\n\ncode to plot Central Selkirk naive maskpar(mfrow = c(1,2), mar = c(1,4,1,4), cex = 1.6, col = \"black\")\nplot(CStraps, gridl = FALSE, border = 30000)\nselectedlakes &lt;- c(1730, 2513, 2514, 2769, 2686, 2749)\nplot(BLKS_BC[selectedlakes,], col = \"blue\", add = TRUE)\ntext(1546000, 510000, \"a.\", xpd = TRUE, col = \"black\")\n\nplot(CStraps, gridl = FALSE, border = 30000, hidetr = TRUE)\nplot(CSmask2000, dots = FALSE, add = TRUE)\nplot(CStraps, add = TRUE)\npar(col = \"black\", cex = 1.6)\ntext(1537000, 510000, \"b.\", xpd = TRUE)\n\n\n\n\n\n\nFigure F.1: (a) Central Selkirk grizzly bear hair snag locations, (b) Mask using naive 30-km buffer around hair snags.\n\n\n\n\nWe next calculate the matrix of all dry detector–mask distances by calling nedist that in turn uses functions from the R package gdistance (van Etten, 2023). Missing pixels in the mask represent barriers to movement when their combined width exceeds a threshold determined by the adjacency rule in gdistance.\nWhat do we mean by an adjacency rule? gdistance finds least-cost distances through a graph formed by joining ‘adjacent’ pixels. Adjacent pixels are defined by the argument ‘directions’, which may be 4 (rook’s case), 8 (queen’s case) or 16 (knight and one-cell queen moves) as in the raster function adjacent. The default in nedist is ‘directions = 16’ because that gives the best approximation to Euclidean distances when there are no barriers. The knight’s moves are \\sqrt 5 \\approx 2.24 \\times cell width (‘spacing’), so the width of a polygon intended to map a barrier should be at least 2.24 \\times cell width.\nSome of the BC lakes are narrow and less than 4.48 km wide. To ensure these act as barriers we could simply reduce the spacing of our mask, but that would slow down model fitting. The alternative is to retain the 2-km mask for model fitting and to define a finer (0.5-km) mask purely for the purpose of computing distances1:\n\nCSmask500 &lt;- make.mask (CStraps, buffer = 30000, type = \n    \"trapbuffer\", spacing = 500, poly = BLKS_BC, poly.habitat =\n     FALSE, keep.poly = FALSE)\nuserd &lt;- nedist(CStraps, CSmask2000, CSmask500)\n\nThe first argument of nedist provides the rows of the distance matrix and the second argument the columns; the third (if present) defines an alternative mask on which to base the calculations. To verify the computation, map the distance from a chosen detector i to every point in a mask. Here is a short function to do that (example in Fig. F.2 a).\n\n# map non-Euclidean distance from detector i \n# or arbitrary point on existing plot (i = NA)\ndmap &lt;- function (traps, mask, userd, i = 1, ...) {\n    if (is.na(i)) i &lt;- nearesttrap(unlist(locator(1)), traps)\n    covariates(mask) &lt;- data.frame(d = userd[i,])\n    covariates(mask)$d[!is.finite(covariates(mask)$d)] &lt;- NA  \n    plot(mask, covariate = \"d\", ...)  \n    points(traps[i,], pch = 3, col = \"red\")\n}\n\nAt this point we could simply use ‘userd’ as our userdist matrix. However, CSmask2000 now includes a lot of points that are further than 30 km from any detector. It is better to drop these points and the associated columns of ‘userd’ (Fig. F.2 b):\n\nOK &lt;- apply(userd, 2, min) &lt; 30000\nCSmask2000b &lt;- subset(CSmask2000, OK)\nuserdx &lt;- userd[,OK]\n\n\ncode to plot Central Selkirk non-Euclidean distances and maskpar(mfrow = c(1,2), mar = c(1,4,1,4), cex = 1.1, col = \"black\")\n\ndmap(CStraps, CSmask2000, userd, dots = FALSE, scale = 0.001, \n     title = \"km\")\npar(cex = 1.6)\ntext(1537000, 510000, \"a.\", xpd=T, col=\"black\")\n\nplot(CStraps, gridl = FALSE, border = 30000, hidetr = TRUE)\nplot(CSmask2000b, dots = FALSE, add = TRUE)\nplot(CStraps, add = TRUE); par(col = \"black\", cex = 1.6)\ntext(1537000, 510000, \"b.\", xpd = TRUE, col = \"black\")\n\n\n\n\n\n\nFigure F.2: (a) Central Selkirk dry-path distances from an arbitrary point (+), and (b) Efficient mask rejecting dry-path distances &gt;30km.\n\n\n\n\nFinally, we can fit a model using the non-Euclidean distance matrix:\n\nCSa &lt;- secr.fit(CS_sexcov_all, mask = CSmask2000b, \n    details = list(userdist = userdx))\npredict(CSa)                \n\nFor completeness, note that Euclidean distances may also be pre-calculated, using the function edist. By default, secr.fit uses that function internally, and there is usually little speed improvement when the calculation is done separately.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Non-Euclidean distances</span>"
    ]
  },
  {
    "objectID": "A06-noneuclidean.html#dynamic-userdist",
    "href": "A06-noneuclidean.html#dynamic-userdist",
    "title": "Appendix F — Non-Euclidean distances",
    "section": "\nF.3 Dynamic userdist",
    "text": "F.3 Dynamic userdist\n\nFor dynamic non-Euclidean distances a function is passed to secr.fit in the ‘details’ argument ‘userdist’, rather than a matrix. The userdist function should take three arguments. The first two are simply 2-column matrices with the coordinates of the detectors and animal locations (mask points) respectively. The third is a habitat mask (this may be the same as xy2). The function has this form:\n\nmydistfn &lt;- function (xy1, xy2, mask) {\n  if (missing(xy1)) return(charactervector)  \n  ...\n  distmat  # return nrow(xy1) x nrow(xy2) matrix\n}\n\nComputation of the distances is entirely under the control of the user – here we indicate that by ‘…’. The calculations may use cell-specific values of two ‘real’ parameters ‘D’ and ‘noneuc’ that as needed are passed by secr.fit as covariates of the mask. ‘D’ is the usual cell-specific expected density in animals per hectare. ‘noneuc’ is a special cell-specific ‘real’ parameter used only here: it means whatever the user wants it to mean.\nWhether ‘noneuc’, ‘D’ or other mask covariates are needed by mydistfn is indicated by the character vector returned by mydistfn when it is called with no arguments. Thus, charactervector may be either a zero-length character vector or a vector of one or more parameter names (“noneuc”, “D”, c(“noneuc”, “D”)).\n‘noneuc’ has its own link scale (default ‘log’) on which it may be modelled as a linear function of any of the predictors available for density (x, y, x2, y2, xy, session, Session, g, or any mask covariate – see Chapter 11. It may also, in principle, be modelled using regression splines (Borchers & Kidney, 2014), but this is untested. When the model is fitted by secr.fit, the beta parameters for the ‘noneuc’ sub-model are estimated along with all the others. To make noneuc available to userdist, ensure that it appears in the ‘model’ argument. Use the formula noneuc ~ 1 if noneuc is constant.\nThe function may compute least-cost paths via intervening mask cells using the powerful igraph package (Csardi & Nepusz, 2006). This is most easily accessed with package gdistance, which in turn uses the RasterLayer S4 object class from the package raster. To facilitate this, secr includes code to treat the ‘mask’ S3 class as a virtual S4 class, and provides a method for the function ‘raster’ to convert a mask to a RasterLayer.\nIf the function generates any bad distances (negative, infinite or missing) these will be replaced by 1e10, with a warning.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Non-Euclidean distances</span>"
    ]
  },
  {
    "objectID": "A06-noneuclidean.html#examples",
    "href": "A06-noneuclidean.html#examples",
    "title": "Appendix F — Non-Euclidean distances",
    "section": "\nF.4 Examples",
    "text": "F.4 Examples\nWe use annotated examples to show how the userdist function may be used to define different models. For illustration we use the Orongorongo Valley brushtail possum dataset from February 1996 (OVpossumCH). The data are captures of possums over 5 nights in single-catch traps at 30-m spacing. We start by extracting the data, defining a habitat mask, and fitting a null model:\n\ncode to construct ovposs and ovmaskdatadir &lt;- system.file(\"extdata\", package = \"secr\")\novforest &lt;- sf::st_read (paste0(datadir, \"/OVforest.shp\"), \n    quiet = TRUE)\novposs &lt;- OVpossumCH[[1]]  # select February 1996\novmask &lt;- make.mask(traps(ovposs), buffer = 120, type = \"trapbuffer\", \n    poly = ovforest[1:2,], spacing = 7.5, keep.poly = FALSE)\n# for plotting only\nleftbank &lt;- read.table(paste0(datadir,\"/leftbank.txt\"))[21:195,]\n\n\n\nfit0 &lt;- secr.fit(ovposs, mask = ovmask, detectfn = \"HHN\", trace = FALSE)\n\nWarning: multi-catch likelihood used for single-catch traps\n\n\nThe warning is routine: we will suppress it in later examples. The distance functions below are not specific to a particular study: each may be applied to other datasets.\n\nF.4.1 Scale of movement \\sigma depends on location of home-range centre\nIn this simple case we use the non-Euclidean distance function to model continuous spatial variation in \\sigma. This cannot be done directly in secr because sigma is treated as part of the detection model, which does not allow for continuous spatial variation in its parameters. Instead we model spatial variation in ‘noneuc’ as a stand-in for ‘sigma’\n\nfn1 &lt;- function (xy1, xy2, mask) {\n  if (missing(xy1)) return(\"noneuc\")\n  sig &lt;- covariates(mask)$noneuc   # sigma(x,y) at mask points\n  sig &lt;- matrix(sig, byrow = TRUE, nrow = nrow(xy1), ncol = nrow(xy2))\n  euc &lt;- edist(xy1, xy2) \n  euc / sig\n}\nfit1 &lt;- secr.fit(ovposs, mask = ovmask, detectfn = \"HHN\", \n    details = list(userdist = fn1), model = noneuc ~ x + y + \n     x2 + y2 + xy, fixed = list(sigma = 1), trace = FALSE)\n\n\npredict(fit1)\n\n        link estimate SE.estimate       lcl      ucl\nD        log 14.68433   1.0914749 12.696150 16.98385\nlambda0  log  0.10852   0.0096263  0.091235  0.12908\nnoneuc   log 25.92426   1.3019339 23.495538 28.60404\n\n\nWe can take the values of noneuc directly from the mask covariates because we know xy2 and mask are the same points. We may sometimes want to use fn1 in context where this does not hold, e.g., when simulating data.\n\nfn1a &lt;- function (xy1, xy2, mask) {\n  if(missing(xy1)) return(\"noneuc\")\n  xy1 &lt;- addCovariates(xy1, mask)\n  sig &lt;- covariates(xy1)$noneuc   # sigma(x,y) at detectors\n  sig &lt;- matrix(sig, nrow = nrow(xy1), ncol = nrow(xy2))\n  euc &lt;- edist(xy1, xy2) \n  euc / sig\n}\nfit1a &lt;- secr.fit(ovposs, mask = ovmask, detectfn = \"HHN\", trace = \n    FALSE, details = list(userdist = fn1a), model = noneuc ~ \n        x + y + x2 + y2 + xy, fixed = list(sigma = 1))\n\n\npredict(fit1a)\n\n        link estimate SE.estimate       lcl      ucl\nD        log 14.46194   1.0296891 12.580493 16.62476\nlambda0  log  0.10758   0.0095253  0.090471  0.12792\nnoneuc   log 26.12539   1.4198769 23.487423 29.05964\n\n\nWe can verify the use of ‘noneuc’ in fn1 by using it to re-fit the null model:\n\nfit0a &lt;- secr.fit(ovposs, mask = ovmask, detectfn = \"HHN\", \n    details = list(userdist = fn1), model = noneuc ~ 1, \n    fixed = list(sigma = 1), trace = FALSE)\n\n\npredict(fit0)\n\n        link estimate SE.estimate       lcl      ucl\nD        log 14.38020   1.0030935 12.544728 16.48423\nlambda0  log  0.10148   0.0089476  0.085402  0.12058\nsigma    log 27.37653   0.9729740 25.535004 29.35087\n\npredict(fit0a)\n\n        link estimate SE.estimate       lcl      ucl\nD        log 14.38019   1.0030937 12.544720 16.48422\nlambda0  log  0.10148   0.0089477  0.085403  0.12058\nnoneuc   log 27.37645   0.9729683 25.534936 29.35078\n\n\nHere, fitting noneuc as a constant while holding sigma fixed is exactly the same as fitting sigma alone.\n\nF.4.2 Scale of movement \\sigma depends on locations of both home-range centre and detector\nHypothetically, detections at xy1 of an animal centred at xy2 may depend on both locations (this may also be seen as a approximation to the following case of continuous variation along the path between xy1 and xy2). To model this we need to retrieve the value of noneuc for both locations. Within fn2 we use addCovariates to extract the covariates of the mask (and hence noneuc) for each point in xy1 and xy2. The call to secr.fit is identical except that it uses fn2 instead of fn1:\n\nfn2 &lt;- function (xy1, xy2, mask) {\n  if (missing(xy1)) return(\"noneuc\")\n  xy1 &lt;- addCovariates(xy1, mask)\n  xy2 &lt;- addCovariates(xy2, mask)\n  sig1 &lt;- as.numeric(covariates(xy1)$noneuc) # sigma at detector\n  sig2 &lt;- as.numeric(covariates(xy2)$noneuc) # sigma at mask pt\n  euc &lt;- edist(xy1, xy2) \n  sig &lt;- outer (sig1, sig2, FUN = function(s1, s2) (s1 + s2)/2)\n  euc / sig\n}\nfit2 &lt;- secr.fit(ovposs, mask = ovmask, detectfn = \"HHN\", \n    details = list(userdist = fn2), model = noneuc ~ x + y + \n     x2 + y2 + xy, fixed = list(sigma = 1), trace = FALSE)\n\n\npredict(fit2)\n\n        link estimate SE.estimate       lcl      ucl\nD        log 14.54654   1.0580294 12.616245 16.77218\nlambda0  log  0.10781   0.0095493  0.090662  0.12821\nnoneuc   log 26.02329   1.3510114 23.507232 28.80866\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe value of noneuc reported by predict.secr is the predicted value at the centroid of the mask, because the model uses standardised mask coordinates.\n\n\n\nF.4.3 Continuously varying \\sigma using gdistance\nA more elegant but slower approach is to find the least-cost path across the network of cells between xy1 and xy2, using noneuc (i.e. sigma ) as the cell-specific cost weighting (large cell-specific sigma equates with greater ‘conductance’, the inverse of friction or cost). For this we use functions from the package gdistance, which in turn uses igraph.\n\nfn3 &lt;- function (xy1, xy2, mask) {\n  if (missing(xy1)) return(\"noneuc\")\n  # warp distances to be \\propto \\int_along path sigma(x,y) dp\n  # where p is path distance  \n  if (!require(gdistance))\n    stop (\"install package gdistance to use this function\")\n  # make raster from mask\n  Sraster &lt;- raster(mask, \"noneuc\")\n  # Assume animals can traverse gaps: bridge gaps using mean\n  Sraster[is.na(Sraster[])] &lt;- mean(Sraster[], na.rm = TRUE)\n  # TransitionLayer\n  tr &lt;- transition(Sraster, transitionFunction = mean, \n                   directions = 16)\n  tr &lt;- geoCorrection(tr, type = \"c\", multpl = FALSE)\n  # costDistance\n  costDistance(tr, as.matrix(xy1), as.matrix(xy2))\n}\nfit3 &lt;- secr.fit(ovposs, mask = ovmask, detectfn = \"HHN\",\n    details = list(userdist = fn3), model = noneuc ~ x + y + \n     x2 + y2 + xy, fixed = list(sigma = 1), trace = FALSE)\n\n\npredict(fit3)\n\n        link estimate SE.estimate       lcl     ucl\nD        log  14.4063   1.0500177 12.490935 16.6154\nlambda0  log   0.1076   0.0095041  0.090529  0.1279\nnoneuc   log  26.4338   1.3645616 23.891813 29.2463\n\n\nThe gdistance function costDistance uses a TransitionLayer object that essentially describes the connections between cells in a RasterLayer. In transition adjacent cells are assigned a positive value for ‘conductance’ and all other cells a zero value. Adjacency is defined by the directions argument as 4 (rook’s case), 8 (queen’s case), 16 (knight and one-cell queen moves) and possibly other values (see ?adjacent in gdistance). Values &lt; 16 can considerably distort distances even if conductance is homogeneous. geoCorrection is needed to allow for the greater separation (\\times \\sqrt 2) of cell centres measured along diagonals.\nIn ovmask there are two forest blocks separated by a shingle stream bed and low scrub that is easily crossed by possums but does not count as ‘habitat’. Habitat gaps are assumed in secr to be traversible. The opposite is assumed by gdistance. To coerce gdistance to behave like secr we here temporarily fill in the gaps.\nThe argument ‘transitionFunction’ determines how the conductance values of adjacent cells are combined to weight travel between them. Here we simply average them, but any other single-valued function of 2 inputs can be used.\nIntegrating along the path (fn3) takes about 3.6 times as long as the approximation (fn2) and gives quite similar results.\n\nF.4.4 Density-dependent \\sigma\n\nA more interesting variation makes sigma a function of the cell-specific density, which may vary independently across space (Efford et al., 2016). Specifically, \\sigma(x,y) = k / \\sqrt{D(x,y)}, where k is the fitted parameter (noneuc).\n\nfn4 &lt;- function (xy1, xy2, mask) {\n  if(missing(xy1)) return(c(\"D\", \"noneuc\"))\n  if (!require(gdistance))\n    stop (\"install package gdistance to use this function\")\n  # make raster from mask\n  D &lt;- covariates(mask)$D\n  k &lt;- covariates(mask)$noneuc  \n  Sraster &lt;- raster(mask, values = k / D^0.5)\n  # Assume animals can traverse gaps: bridge gaps using mean\n  Sraster[is.na(Sraster[])] &lt;- mean(Sraster[], na.rm = TRUE)\n  # TransitionLayer\n  tr &lt;- transition(Sraster, transitionFunction = mean, \n                   directions = 16)\n  tr &lt;- geoCorrection(tr, type = \"c\", multpl = FALSE)\n  # costDistance\n  costDistance(tr, as.matrix(xy1), as.matrix(xy2))\n}\nfit4 &lt;- secr.fit(ovposs, mask = ovmask, detectfn = \"HHN\", trace = \n    FALSE, details = list(userdist = fn4), fixed = list(sigma = 1),\n    model = list(noneuc ~ 1, D ~ x + y + x2 + y2 + xy))\n\n\npredict(fit4)\n\n        link  estimate SE.estimate       lcl       ucl\nD        log  15.45490   1.6387186 12.562138  19.01380\nlambda0  log   0.10669   0.0094078  0.089788   0.12678\nnoneuc   log 103.22821   5.0334083 93.824969 113.57385\n\n\n\n# or using regression splines with same df\nfit4a &lt;- secr.fit(ovposs, mask = ovmask, detectfn = \"HHN\",\n    details = list(userdist = fn4), fixed = list(sigma = 1),\n    model = list(noneuc~1, D ~ s(x,y, k = 6)), trace = FALSE)\n\n\npredict(fit4a)\n\n        link  estimate SE.estimate       lcl       ucl\nD        log  15.76414   1.7672732 12.663161  19.62448\nlambda0  log   0.10681   0.0094166  0.089887   0.12691\nnoneuc   log 103.08276   5.0090010 93.723571 113.37656\n\n\n\ncode to plot OV surface 4apar(mar = c(1,4,1,6), cex = 1.4)\nplot(predictDsurface(fit4a))\nplot(traps(ovposs), add = TRUE)\nlines(leftbank)\n\n\n\n\n\n\nFigure F.3: Surface 4a.\n\n\n\n\n\nF.4.5 Habitat model for connectivity\nYet another possibility, in the spirit of Royle et al. (2013), is to model conductance as a function of habitat covariates. As usual in secr these are stored as one or more mask covariates. It is easy to add a covariate for forest type (Nothofagus-dominant ‘beech’ vs ‘nonbeech’) to our mask:\n\novmask &lt;- addCovariates(ovmask, ovforest[1:2,])\nfit5 &lt;- secr.fit(ovposs, mask = ovmask, detectfn = \"HHN\",\n    details = list(userdist = fn2), model = list(D ~ forest, \n     noneuc ~ forest), fixed = list(sigma = 1), trace = FALSE)\n\n\npredict(fit5, newdata = \n    data.frame(forest = c(\"beech\", \"nonbeech\")))\n\n$`forest = beech`\n        link estimate SE.estimate      lcl      ucl\nD        log  9.42514   2.6531406  5.48569 16.19364\nlambda0  log  0.10111   0.0089337  0.08506  0.12019\nnoneuc   log 29.55771   3.4060705 23.59969 37.01991\n\n$`forest = nonbeech`\n        link estimate SE.estimate      lcl      ucl\nD        log 15.67451   1.2679355 13.37985 18.36271\nlambda0  log  0.10111   0.0089337  0.08506  0.12019\nnoneuc   log 27.23476   1.0319163 25.28619 29.33349\n\n\nNote that we have re-used the userdist function fn2, and allowed both density and noneuc (sigma) to vary by forest type. Strictly, we should have identified “forest” as a required covariate in the (re)definition of fn2, but this is obviously not critical.\nA full analysis should also consider models with variation in lambda0. There is no simple way in secr to model continuous spatial variation in lambda0 as a function of home-range location (cf sigma in Example 1 above). However, variation in lambda0 at the point of detection may be modelled with detector-level covariates (Section 10.2.1).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Non-Euclidean distances</span>"
    ]
  },
  {
    "objectID": "A06-noneuclidean.html#and-the-winner-is",
    "href": "A06-noneuclidean.html#and-the-winner-is",
    "title": "Appendix F — Non-Euclidean distances",
    "section": "\nF.5 And the winner is…",
    "text": "F.5 And the winner is…\nNow that we have a bunch of fitted models, let’s see which does the best:\n\nfits &lt;- secrlist(fit0, fit0a, fit1, fit1a, fit2, fit3, fit4, \n                 fit4a, fit5)\nAIC(fits)[, -c(2,4,6)]\n\n                                          model npar    AIC   dAIC  AICwt\nfit4a       D~s(x, y, k = 6) lambda0~1 noneuc~1    8 3098.1  0.000 0.4549\nfit4  D~x + y + x2 + y2 + xy lambda0~1 noneuc~1    8 3098.5  0.382 0.3758\nfit1  D~1 lambda0~1 noneuc~x + y + x2 + y2 + xy    8 3101.8  3.704 0.0714\nfit3  D~1 lambda0~1 noneuc~x + y + x2 + y2 + xy    8 3102.4  4.304 0.0529\nfit2  D~1 lambda0~1 noneuc~x + y + x2 + y2 + xy    8 3103.5  5.411 0.0304\nfit1a D~1 lambda0~1 noneuc~x + y + x2 + y2 + xy    8 3105.0  6.862 0.0147\nfit0                      D~1 lambda0~1 sigma~1    3 3118.1 19.979 0.0000\nfit0a                    D~1 lambda0~1 noneuc~1    3 3118.1 19.979 0.0000\nfit5           D~forest lambda0~1 noneuc~forest    5 3118.4 20.280 0.0000\n\n\n…the model with a quadratic or spline trend in density and density-dependent sigma.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Non-Euclidean distances</span>"
    ]
  },
  {
    "objectID": "A06-noneuclidean.html#notes",
    "href": "A06-noneuclidean.html#notes",
    "title": "Appendix F — Non-Euclidean distances",
    "section": "\nF.6 Notes",
    "text": "F.6 Notes\nThe ‘real’ parameter for spatial scale (\\sigma) is lurking in the background as part of the detection model. User-defined non-Euclidean distances are used in the detection function just like ordinary Euclidean distances. This means in practice that they are (almost) always divided by \\sigma. Formally: the distance d_{ij} between an animal i and a detector j appears in all commonly used detection functions as the ratio r_{ij} = d_{ij}/\\sigma (e.g., halfnormal \\lambda = \\lambda_0 \\exp(-0.5r_{ij}^2) and exponential \\lambda = \\lambda_0 \\exp(-r_{ij})).\nWhat if we want non-Euclidean distances, but do not want to estimate noneuc? This is a perfectly reasonable request if sigma is constant across space and the distance computation is determined entirely by the habitat geometry, with no need for an additional parameter. If ‘noneuc’ is not included in the character vector returned by your userdist function when it is called with no arguments then noneuc is not modelled at all. This is the default in secrlinear.\nProviding a suitable initial value for ‘noneuc’ can be a problem. The argument ‘start’ of secr.fit may be a named, and possibly incomplete, list of real parameter values, so a call such as this is valid:\n\nsecr.fit (captdata, model = noneuc~1, details = list(userdist = fn2), \n    trace = FALSE, start = list(noneuc = 25), fixed = list(sigma = 1))\n\n\nsecr.fit(capthist = captdata, model = noneuc ~ 1, start = list(noneuc = 25), \n    fixed = list(sigma = 1), details = list(userdist = fn2), \n    trace = FALSE)\nsecr 5.2.0, 20:24:43 13 Feb 2025\n\nDetector type      single \nDetector number    100 \nAverage spacing    30 m \nx-range            365 635 m \ny-range            365 635 m \n\nN animals       :  76  \nN detections    :  235 \nN occasions     :  5 \nMask area       :  21.227 ha \n\nModel           :  D~1 g0~1 noneuc~1 \nUser distances  :  dynamic (function)\nFixed (real)    :  sigma = 1 \nDetection fn    :  halfnormal\nDistribution    :  poisson \nN parameters    :  3 \nLog likelihood  :  -759.03 \nAIC             :  1524.1 \nAICc            :  1524.4 \n\nBeta parameters (coefficients) \n           beta  SE.beta     lcl      ucl\nD       1.70107 0.117615  1.4705  1.93159\ng0     -0.97849 0.136239 -1.2455 -0.71147\nnoneuc  3.37983 0.044415  3.2928  3.46688\n\nVariance-covariance matrix of beta parameters \n                 D          g0      noneuc\nD       0.01383322  0.00015578 -0.00099075\ng0      0.00015578  0.01856108 -0.00334338\nnoneuc -0.00099075 -0.00334338  0.00197273\n\nFitted (real) parameters evaluated at base levels of covariates \n        link estimate SE.estimate      lcl      ucl\nD        log  5.47980    0.646740  4.35162  6.90047\ng0     logit  0.27319    0.027051  0.22348  0.32927\nnoneuc   log 29.36583    1.304938 26.91757 32.03677\n\n\nWe have ignored the parameter \\lambda_0. This is almost certainly a mistake, as large variation in \\sigma without compensatory or normalising variation in \\lambda_0 is biologically implausible and can lead to improbable results Efford (2014).\nIt is intended that non-Euclidean distances should work with all relevant functions in secr.\nYou may be tempted to model ‘noneuc’ as a function of group - after all, D~g is permitted, right? Unfortunately, this will not work. There is only one pre-computed distance matrix, not one matrix per group.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Non-Euclidean distances</span>"
    ]
  },
  {
    "objectID": "A06-noneuclidean.html#simulation-after-sutherland-et-al.-2015",
    "href": "A06-noneuclidean.html#simulation-after-sutherland-et-al.-2015",
    "title": "Appendix F — Non-Euclidean distances",
    "section": "\nF.7 Simulation after Sutherland et al. (2015)",
    "text": "F.7 Simulation after Sutherland et al. (2015)\n\nSutherland et al. (2015) simulated SECR data from a population of animals whose movement was channeled to varying extents along a dendritic network (river system). Their model treated the habitat as 2-dimensional and shrank distances for pixels close to water and expanded them for pixels further away. The authors kindly provided data for the network map and detector layout which we use here to emulate their simulations in secr. We assume an existing SpatialLinesDataFrame sample.water for the network, and a matrix of x-y coordinates for detector locations gridTrapsXY. rivers is a version of sample.water clipped to the habitat mask and used only for plotting.\n\n# use package secrlinear to create a discretised version of the \n# network as a handy way to get distance to water \n# loading secrlinear also loads secr\nlibrary(secrlinear)\nlibrary(gdistance)\n\n\nswlinearmask &lt;- read.linearmask(data = sample.water, spacing = 100)\n\n\n# generate secr traps object from detector locations\ntr &lt;- data.frame(gridTrapsXY*1000)  # convert to metres\nnames(tr) &lt;- c(\"x\",\"y\")             \ntr &lt;- read.traps(data=tr, detector = \"count\")\n\n# generate 2-D habitat mask\nsw2Dmask &lt;- make.mask(tr, buffer = 3950, spacing = 100)\nd2w &lt;- distancetotrap(sw2Dmask, swlinearmask)\ncovariates(sw2Dmask) &lt;- data.frame(d2w = d2w/1000) # km to water\n\n\n\nWarning: attribute variables are assumed to be spatially constant throughout all\ngeometries\n\n\n\npar(mar = c(1,6,1,6))\nplot(sw2Dmask, covariate = \"d2w\", dots = FALSE)\nplot(tr, add = TRUE)\nplot(rivers, add = TRUE, col = \"blue\")\n\n\n\n\n\n\n\n\nFigure F.4: Shaded plot of distance to water (d2w in km) with detector sites (red crosses) and rivers superimposed. Detector spacing 1.5 km N-S.\n\n\n\n\nThe distance function requires a value of the friction parameter ‘noneuc’ for each mask pixel. Distances are approximated using gdistance functions as before, except that we interpret the distance-to-water scale as ‘friction’ and invert that for gdistance.\n\ndfn &lt;- function (xy1, xy2, mask) {\n    if (missing(xy1)) return(\"noneuc\")\n    require(gdistance)\n    Sraster &lt;- raster(mask, \"noneuc\")\n    # conductance is inverse of friction\n    trans &lt;- transition(Sraster, transitionFunction = \n        function(x) 1/mean(x), directions = 16)\n    trans &lt;- geoCorrection(trans)\n    costDistance(trans, as.matrix(xy1), as.matrix(xy2))\n}    \n\nThe Royle et al. (2013) and Sutherland et al. (2015) models use an (\\alpha_0, \\alpha_1) parameterisation instead of (\\lambda_0, \\sigma). Their \\alpha_2 translates directly to a coefficient in the secr model, as we’ll see. We consider just one realisation of one scenario (the package secrdesign Efford (2023) manages replicated simulations of multiple scenarios).\n\n# parameter values from Sutherland et al. 2014\nalpha0 &lt;- -1   # implies lambda0 = invlogit(-1) = 0.2689414\nsigma &lt;- 1400\nalpha1 &lt;- 1 / (2 * sigma^2)\nalpha2 &lt;- 5    # just one scenario from the range 0..10\nK  &lt;- 10       # sampling over 10 occasions, collapsed to 1 occ\n\nNow we are ready to build a simulated dataset.\n\n# simulate fixed population of 200 animals in masked area\npop &lt;- sim.popn (D = 200/nrow(sw2Dmask), core = tr, \n                 buffer = 3950, Ndist = \"fixed\")  \n# to simulate non-Euclidean detection we attach a mask with \n# the pixel-specific friction to the simulated popn object\ncovariates(sw2Dmask)$noneuc &lt;- exp(alpha2 * \n                                    covariates(sw2Dmask)$d2w)\nattr(pop, \"mask\") &lt;- sw2Dmask\n# simulate detections, specifying non-Euclidean distance function\nCH &lt;- sim.capthist(tr, pop = pop, userdist = dfn, \n    noccasions = 1, binomN = K, detectpar = list(lambda0 = \n    invlogit(alpha0), sigma = sigma), detectfn = \"HHN\")\n\n\nsummary(CH, moves = TRUE)\n\nObject class       capthist \nDetector type      count \nDetector number    64 \nAverage spacing    1385.7 m \nx-range            1698699 1708399 m \ny-range            2387891 2398391 m \n\nCounts by occasion \n                    1 Total\nn                  37    37\nu                  37    37\nf                  37    37\nM(t+1)             37    37\nlosses              0     0\ndetections        109   109\ndetectors visited  33    33\ndetectors used     64    64\n\nNumber of movements per animal\n 0  1  2  3 \n16 13  7  1 \n\nDistance moved, excluding zero (m)\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1386    1386    1443    1552    1500    3151 \n\nIndividual covariates\n sex   \n F:21  \n M:16  \n\n\nModel fitting is simple, but the default starting value for noneuc is not suitable and is overridden:\n\nfitne1 &lt;- secr.fit (CH, mask = sw2Dmask, detectfn = \"HHN\", \n    binomN = 10, model = noneuc ~ d2w -1, details = list(\n    userdist = dfn), start = list(D = 0.005, lambda0 = 0.3,\n    sigma = 1000, noneuc = 100), trace = FALSE)\n\nThe warning from nlm indicates a potential problem, but the standard errors and confidence limits below look plausible (they could be checked by running again with method = “none”). Fitting is slow (4 minutes on an aging PC). This is partly because the mask is large (32384 pixels) in order to maintain resolution in relation to the stream network.\n\ncoef(fitne1)\n\n              beta  SE.beta     lcl      ucl\nD          -5.3042 0.181714 -5.6603 -4.94800\nlambda0    -1.1386 0.169830 -1.4715 -0.80577\nsigma       7.1491 0.091395  6.9699  7.32821\nnoneuc.d2w  4.5961 0.503034  3.6102  5.58205\n\npredict(fitne1)\n\n        link   estimate SE.estimate        lcl        ucl\nD        log 4.9709e-03  9.1079e-04 3.4814e-03 7.0976e-03\nlambda0  log 3.2026e-01  5.4784e-02 2.2958e-01 4.4674e-01\nsigma    log 1.2729e+03  1.1658e+02 1.0642e+03 1.5226e+03\nnoneuc   log 7.5395e+00  1.6876e+00 4.8881e+00 1.1629e+01\n\nregion.N(fitne1)\n\n    estimate SE.estimate    lcl    ucl  n\nE.N   160.98      29.495 112.74 229.85 37\nR.N   160.98      26.627 118.77 224.97 37\n\n\nThe coefficient noneuc.d2w corresponds to alpha2. Estimates of predicted (‘real’) parameters D and lambda0, and the coefficient noneuc.d2w, and are comfortably close to the true values, and all true values are covered by the 95% CI.\nWe fit the ‘noneuc’ (friction) parameter through the origin (zero intercept; -1 in formula). The predicted value of ‘noneuc’ relates to the covariate value for the first pixel in the mask (d2w = 1.133 km), but in this zero-intercept model the meaning of ‘noneuc’ itself is obscure. In effect, the parameter alpha1 (or sigma) serves as the intercept; the same model may be fitted by fixing sigma (fixed = list(sigma = 1)) and estimating an intercept for noneuc (model = noneuc ~ d2w). In this case, ‘noneuc’ may be interpreted as the site-specific sigma (see also examples in the main text).\nIt is interesting to plot the predicted detection probability under the simulated model. For plotting we add the pdot value as an extra covariate of the mask. Note that pdot here uses the ‘noneuc’ value previously added as a covariate to sw2Dmask.\n\ncovariates(sw2Dmask)$predicted.pdot &lt;- pdot(sw2Dmask, tr, \n    noccasions = 1, binomN = 10, detectfn = \"HHN\", detectpar = \n    list(lambda0 = invlogit(-1), sigma = sigma), userdist = dfn)\n\n\npar(mar = c(1,6,1,6))\nplot(sw2Dmask, covariate = \"predicted.pdot\", dots = FALSE)\nplot(tr, add = TRUE)\nplot(rivers, add = TRUE, col = \"blue\")\n\n\n\n\n\n\n\n\nFigure F.5: Shaded plot of probability animal is detected at least once. Animals living within the detector array and away from a river (about half the population within the array) stand very little chance of being detected because the model confines them to a small home range and lambda0 is constant.\n\n\n\n\n\n\n\nFigure F.1: (a) Central Selkirk grizzly bear hair snag locations, (b) Mask using naive 30-km buffer around hair snags.\nFigure F.2: (a) Central Selkirk dry-path distances from an arbitrary point (+), and (b) Efficient mask rejecting dry-path distances &gt;30km.\nFigure F.3: Surface 4a.\nFigure F.4: Shaded plot of distance to water (d2w in km) with detector sites (red crosses) and rivers superimposed. Detector spacing 1.5 km N-S.\nFigure F.5: Shaded plot of probability animal is detected at least once. Animals living within the detector array and away from a river (about half the population within the array) stand very little chance of being detected because the model confines them to a small home range and lambda0 is constant.\n\n\n\nBorchers, D. L., & Kidney, D. (2014). Flexible density surface estimation for spatially explicit capture–recapture surveys. University of St Andrews. https://research-repository.st-andrews.ac.uk/bitstream/handle/10023/9147/secrgam_techrep.pdf?sequence=1\n\n\nCsardi, G., & Nepusz, T. (2006). The igraph software package for complex network research. InterJournal, 1695. http://igraph.org\n\n\nEfford, M. G. (2014). Bias from heterogeneous usage of space in spatially explicit capture-recapture analyses. Methods in Ecology and Evolution, 5, 599–602.\n\n\nEfford, M. G. (2023). secrdesign: Sampling Design for Spatially Explicit Capture-Recapture. https://CRAN.R-project.org/package=secrdesign\n\n\nEfford, M. G., Dawson, D. K., Jhala, Y. V., & Qureshi, Q. (2016). Density-dependent home-range size revealed by spatially explicit capture-recapture. Ecography, 39, 676–688.\n\n\nEfford, M. G., & Mowat, G. (2014). Compensatory heterogeneity in spatially explicit capture-recapture data. Ecology, 95, 1341–1348.\n\n\nKendeigh, S. C. (1944). Measurement of bird populations. Ecological Monographs, 14, 67–106. https://doi.org/10.2307/1961632\n\n\nMowat, G., & Strobeck, C. (2000). Estimating population size of grizzly bears using hair capture, DNA profiling, and mark-recapture analysis. Journal of Wildlife Management, 64, 183–193.\n\n\nRoyle, J. A., Chandler, R. B., Gazenski, K. D., & Graves, T. A. (2013). Spatial capture-recapture models for jointly estimating population density and landscape connectivity. Ecology, 94, 287–294.\n\n\nSutherland, C., Fuller, A. K., & Royle, J. A. (2015). Modelling non-euclidean movement and landscape connectivity in highly structured ecological networks. Methods in Ecology and Evolution, 6, 169–177.\n\n\nvan Etten, J. (2023). Gdistance: Distances and routes on geographical grids. https://CRAN.R-project.org/package=gdistance",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Non-Euclidean distances</span>"
    ]
  },
  {
    "objectID": "A06-noneuclidean.html#footnotes",
    "href": "A06-noneuclidean.html#footnotes",
    "title": "Appendix F — Non-Euclidean distances",
    "section": "",
    "text": "Mixing 2-km and 0.5-km cells carries a slight penalty: the centres of a few 2-km cells (&lt;1%) do not lie in valid 0.5-km cells; these become inaccessible (infinite distance from all detectors) and are silently dropped in a later step.↩︎",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Non-Euclidean distances</span>"
    ]
  },
  {
    "objectID": "A07-telemetry.html",
    "href": "A07-telemetry.html",
    "title": "Appendix G — Telemetry",
    "section": "",
    "text": "G.1 Example\nWe start with a concrete example based on a simulated dataset.\nShow simulation code# Code to simulate capthist objects (trCH, teCH)\n\n# detectors\nte &lt;- make.telemetry()\ntr &lt;- make.grid(detector = \"multi\", nx = 8, ny = 8)\n\n# spatial population\nset.seed(123)\npop4 &lt;- sim.popn(tr, D = 5, buffer = 100, seed = 567)\n\n# select 12 telemetered individuals from larger population\npop4C &lt;- subset(pop4, sample.int(nrow(pop4), 12))\n\n# renumber = FALSE (keep original animalID) needed for matching\ntrCH &lt;- sim.capthist(tr,  popn = pop4, renumber = FALSE, \n    detectfn = \"HHN\", detectpar = list(lambda0 = 0.1, \n    sigma = 25), seed = 123)\nsession(trCH) &lt;- 'Trapping'\nteCH &lt;- sim.capthist(te, popn = pop4C, renumber = FALSE, \n    detectfn = \"HHN\", detectpar = list(lambda0 = 1, sigma = 25),\n    noccasions = 10, seed = 345)\nsession(teCH) &lt;- 'Telemetry'\nThe first step is to combine the capthist objects trCH (trapping data) and teCH (telemetry fixes).\ncombinedCH &lt;- addTelemetry(trCH, teCH)\nGenerating plots is straightforward. By default, plot.capthist displays the captures and ignores telemetry fixes. The plot type “telemetry” displays the fixes and distinguishes those of animals that also appear in the (unplotted) capture data of the combined object.\npar(mfrow = c(1,2), mar = c(2,2,3,2))\nplot(traps(trCH), border = 150, bty = 'o') # base plot\nplot(combinedCH, title = 'Trapping', tracks = TRUE, rad = 4, add = TRUE)\nplot(traps(trCH), border = 150, bty = 'o') # base plot\nplot(combinedCH, title = 'Telemetry', type = 'telemetry', \n     tracks = TRUE, add = TRUE)\n\n\n\n\n\n\nFigure G.1: Simulated trapping and telemetry data. The 5-day trapping study (left) yielded 29 recaptures. Telemetry data were obtained on 10 occasions for each of 12 animals (right). Points for animals that were both trapped and telemetered are ringed in black. Colours distinguish individuals.\nNext we fit trapping-only and joint trapping-and-telemetry models:\nmask &lt;- make.mask(traps(trCH), buffer = 100, type = 'trapbuffer', \n    nx = 32)\nargs &lt;- list(mask = mask, detectfn = 'HHN', trace = FALSE)\nfits &lt;- list.secr.fit(list(trCH, combinedCH), constant = args, \n    names = c('tr','combined'))\ncollate(fits)[1,,,]\n\n, , D\n\n         estimate SE.estimate    lcl    ucl\ntr         5.9057      1.3312 3.8175 9.1363\ncombined   5.7070      1.2801 3.6967 8.8104\n\n, , lambda0\n\n         estimate SE.estimate      lcl     ucl\ntr       0.118276    0.029911 0.072604 0.19268\ncombined 0.073111    0.015752 0.048157 0.11100\n\n, , sigma\n\n         estimate SE.estimate    lcl    ucl\ntr         22.027      2.4031 17.798 27.262\ncombined   26.498      1.1295 24.375 28.806\nHere, telemetry data greatly improves the precision of the estimated scale of movement \\sigma, but the effect on estimates of the other two parameters is small.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Telemetry</span>"
    ]
  },
  {
    "objectID": "A07-telemetry.html#standalone-telemetry-data",
    "href": "A07-telemetry.html#standalone-telemetry-data",
    "title": "Appendix G — Telemetry",
    "section": "\nG.2 Standalone telemetry data",
    "text": "G.2 Standalone telemetry data\nTelemetry data are stored in modified secr capthist objects. A capthist object may comprise telemetry fixes only (‘standalone telemetry’) or telemetry fixes in association with capture–recapture data (composite telemetry and capture–recapture). In this section I describe standalone telemetry data. A composite capthist is formed by combining a telemetry-only object and a standard capthist object with addTelemetry, as described in Section G.3.\n\nG.2.1 The ‘traps’ object for telemetry data\nTelemetry data differ from all other SECR data in that the detection process does not constrain where animals are detected (telemetry provides a spatially unbiased sample of each animal’s activity). That is clearly not the case for area-search and point detectors, which inevitably constrain where animals are detected. Nevertheless, for compatibility with the rest of secr, we associate telemetry data with a notional detector located at a point. The ‘traps’ object for telemetry data comprises the coordinates of this point plus the usual attributes of a ‘traps’ object in secr (detector type, usage, etc.). Remember that the point is only a ‘notional’ detector - it is never visited.\nThe function make.telemetry generates a suitable object:\n\nte &lt;- make.telemetry()\nsummary(te)\n\nObject class       traps \nDetector type      telemetry \nTelemetry type     independent \n\nstr(te)\n\nClasses 'traps' and 'data.frame':   1 obs. of  2 variables:\n $ x: num 0\n $ y: num 0\n - attr(*, \"detector\")= chr \"telemetry\"\n - attr(*, \"telemetrytype\")= chr \"independent\"\n\n\nThe attribute telemetrytype is always ‘independent’ for standalone telemetry data; other possible values for composite data are described later.\n\nG.2.2 Detector type\nEvery ‘traps’ object has an associated detector type (attribute detector, commonly ‘multi’ or ‘proximity’). This may be a vector with a different value for each occasion. The detector type for telemetry data is ‘telemetry’. In a standalone telemetry capthist, all elements of detector are ‘telemetry’.\n\nG.2.3 But where are the data?\nAs for any other detector type, the body of a telemetry capthist is a 3-D array whose elements are the number of detections for each combination of animal, occasion and detector. Coordinates are stored separately in the ‘telemetryxy’ attribute. Use one of these functions to reveal the telemetry component of a capthist object CH:\n\nstr(CH)\nsummary(CH)\nplot(CH, type = 'telemetry')\ntelemetryxy(CH)\n\nThe attribute ‘telemetryxy’ is a list with one component for each animal. The fixes of each animal are sorted in chronological order.\n\nG.2.4 Data input\nData for a telemetry-only object should be read with function read.telemetry, a simplified version of read.capthist. The input format for telemetry fixes follows the ‘XY’ format for captures, with one line per fix (see secr-datainput.pdf).\nThe first few lines of a text file containing telemetry data collected on 5 occasions might look like this –\n1  10 1  -83.3  -20.04\n1  10 2  -57.91  -4.77\n1  10 3 -112.96  -7.51\n1  10 4  -77.71 -75.79\n1  10 5  -85.81 -42.45\n1 101 1  143.06 170.48\n1 101 2   99.22 145.49\netc.\nThe first column is a session code, the next an animal identifier (‘10’, ‘101’), the third an occasion number (1..noccasions) and the last two are the x and y coordinates. GPS coordinates should be projected (i.e. not latitude and longitude), and in metres if possible.\nA file named ‘telemetrydemo.txt’ may be read with\n\nCHt &lt;- read.telemetry(file = \"data/telemetrydemo.txt\")\n\nNo errors found :-)\n\nhead(CHt)\n\nSession =  1 \n, , 1\n\n   1 2 3 4 5\n4  1 1 1 1 1\n9  1 1 1 1 1\n10 1 1 1 1 1\n11 1 1 1 1 1\n12 1 1 1 1 1\n14 1 1 1 1 1\n\ntelemetryxy(CHt)[['10']]  # coordinates of first animal\n\n        x      y\n1  -83.30 -20.04\n2  -57.91  -4.77\n3 -112.96  -7.51\n4  -77.71 -75.79\n5  -85.81 -42.45\n\n\nInput may be from a text file (named in argument ‘file’) or dataframe (argument ‘data’). The body of the resulting capthist object merely tallies the number of detections per animal per session and occasion. The fixes for one session are stored separately in an attribute that is a list of dataframes, one per animal. Use telemetryxy(CHt) to retrieve this list.\nThe summary of a telemetry-only capthist is quirky:\n\nsummary(CHt)\n\nObject class       capthist \nDetector type      telemetry (5) \nTelemetry type     independent \n\nCounts by occasion \n                   1  2  3  4  5 Total\nn                 79 79 79 79 79   395\nu                 79  0  0  0  0    79\nf                  0  0  0  0 79    79\nM(t+1)            79 79 79 79 79    79\nlosses             0  0  0  0  0     0\ndetections        79 79 79 79 79   395\ndetectors visited  0  0  0  0  0     0\ndetectors used     0  0  0  0  0     0\n\nEmpty histories :  79 \n79 telemetered animals, 0 detected\n5-5 locations per animal, mean =  5, sd = 0 \n\nIndividual covariates\n       V6      \n Min.   :1.00  \n 1st Qu.:1.00  \n Median :2.00  \n Mean   :1.52  \n 3rd Qu.:2.00  \n Max.   :2.00  \n\n\nEven though each fix is counted as a ‘detection’ in the body of the final capthist object, none of the telemetered animals is considered to have been ‘detected’ in a conventional SECR sense. The telemetry-only capthist object includes a trivial traps object with a single point. The telemetry type of the traps for a telemetry-only capthist defaults to ‘independent’.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Telemetry</span>"
    ]
  },
  {
    "objectID": "A07-telemetry.html#sec-composite",
    "href": "A07-telemetry.html#sec-composite",
    "title": "Appendix G — Telemetry",
    "section": "\nG.3 Combining telemetry and capture–recapture",
    "text": "G.3 Combining telemetry and capture–recapture\nFor the purpose of density estimation and modelling, standalone telemetry data are added to an existing spatial capture–recapture (capthist) data object with the function addTelemetry. The relationship between the telemetry and capture–recapture samples is determined by the type argument (default ‘concurrent’). We first explain the possible telemetry types.\n\nG.3.1 Types of telemetry data\nsecr distinguishes three types of telemetry data – independent, dependent and concurrent – that differ in how they relate to other SECR samples (capture–recapture data). Each type corresponds to a particular probability model. \n\n\n\n\n\n\n\nFigure G.2: Schematic relationship of capture–recapture data to three types of telemetry data. Vertical overlap indicates individuals that appear in both datasets.\n\n\n\n\n\nIndependent telemetry\n\nIndependent telemetry data have no particular relationship to spatial capture–recapture data except that they may be modelled using a shared value of the spatial-scale parameter \\sigma, and possibly other spatial parameters. Telemetered animals do not have detection histories.\n\nDependent telemetry\n\nDependent telemetry data relate to a sample of animals detected during the capture–recapture study: an animal must be caught in that study to become telemetered, and no animal is telemetered and not otherwise detected (i.e. no detection history is all-zero).\n\nConcurrent telemetry\n\nConcurrent telemetry data are obtained for a sample of animals from the same regional population as the capture–recapture study. Telemetered animals appear stochastically in the capture–recapture sample with probability related to their location. Detection histories of some animals may be all-zero, and these are modelled. Whether the capture–recapture phase precedes or follows telemetry is not material.\n\nG.3.2 What exactly does addTelemetry do?\nThe addTelemetry function forms a composite capthist object. Its usage follows -\naddTelemetry (detectionCH, telemetryCH, type = c(\"concurrent\", \"dependent\", \n    \"independent\"), collapsetelemetry = TRUE, verify = TRUE) \nCapture–recapture data in the argument ‘detectionCH’ form the basis for addTelemetry. The base capthist is modified in these ways –\n\nFor all telemetry types addTelemetry extends the capture–recapture ‘traps’ object by adding a single (notional) detector location (duplicating the first).\nBy default, the ‘detector’ attribute is extended by a single sampling occasion with type ‘telemetry’; all telemetry data are associated with this occasion, regardless of how many occasions there were in the telemetry input. If collapsetelemetry = FALSE distinct telemetry occasions are retained.\nThe ‘usage’ attribute is set to zero for the notional telemetry detector on capture occasions and for capture detectors on telemetry occasions. Other usage data from ‘detectionCH’ is retained.\nAll-zero detection histories are generated for the ‘concurrent’ data type.\n\nThe coordinates of telemetry fixes are transferred from telemetryCH as the attribute ‘telemetryxy’ of the output.\nIf the data are independent then the labels of telemetered animals are prefixed by ‘T’ to reduce the chance of identity conflicts with animals in ‘detectionCH’.\nBy default, addtelemetry calls verify.capthist to check its output.\n\nG.3.3 Composite data, different sessions\nWe use addTelemetry to combine telemetry data and capture–recapture data from the same session, or possibly for each of several sessions when the detectionCH and telemetryCH are parallel (equal-length) multi-session objects. It is also feasible to concatenate telemetry and capture–recapture data as separate sessions of a multi-session object with MS.capthist. The effect is similar to a single-session composite capthist with telemetrytype ‘independent’, because secr treats sessions as independent (i.e. individual histories do not span session boundaries). See the next section for an example.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Telemetry</span>"
    ]
  },
  {
    "objectID": "A07-telemetry.html#model-fitting",
    "href": "A07-telemetry.html#model-fitting",
    "title": "Appendix G — Telemetry",
    "section": "\nG.4 Model fitting",
    "text": "G.4 Model fitting\n\nG.4.1 Standalone telemetry data\nWe can estimate \\sigma for a half-normal circular home-range model directly:\n\nRPSV (CHt, CC = TRUE)\n\n[1] 24.868\n\n\nNote the CC argument (named for Calhoun & Casby (1958)) that is required to scale the result correctly.\nMore laboriously:\n\nfit0 &lt;- secr.fit(CHt, buffer = 300, detectfn = 'HHN', \n                 trace = FALSE)\npredict(fit0)\n\n      link estimate SE.estimate    lcl    ucl\nsigma  log   24.867     0.69957 23.533 26.277\n\n\nThe detection function (argument detectfn) must be either hazard half-normal (14, ‘HHN’) or hazard exponential (16, ‘HEX’)1. The default detection function for a dataset with any telemetry component is ‘HHN’. For telemetry-only data the likelihood is conditional on the number of observations, so the argument CL is set internally to TRUE. A large buffer value here brings \\hat \\sigma from secr.fit closer to \\hat \\sigma from RPSV. See below for more on the buffer argument.\nSee Technical notes for potential numerical problems.\n\nG.4.2 Composite telemetry and capture–recapture data\nFitting a model to composite data should raise no further problems: secr.fit receives all the information it requires in the composite capthist input. The likelihood is a straightforward extension of the usual SECR likelihood, with some subtle differences in the case of dependent or concurrent telemetry. \nThe use of detection functions expressed in terms of the hazard provides a more natural link between the model for the activity distribution and the model for detection probability. When a hazard function is used secr.fit automatically flips the default model for the first detection parameter from ‘g0 ~ 1’ to ‘lambda0 ~ 1’.\nOur introductory example fitted a model to single-session composite data. We can compare the results when the telemetry and trapping data are in separate sessions:\n\nmsCH &lt;- MS.capthist(trCH, teCH)\nfit.ms &lt;- secr.fit(msCH, mask=mask, detectfn = 'HHN', \n                   trace = FALSE)\npredict(fit.ms)\n\n$`session = trCH`\n        link estimate SE.estimate       lcl     ucl\nD        log  5.68802    1.251718  3.714104  8.7110\nlambda0  log  0.10628    0.021189  0.072182  0.1565\nsigma    log 23.63780    1.043956 21.678653 25.7740\n\n$`session = teCH`\n      link estimate SE.estimate    lcl    ucl\nsigma  log   23.638       1.044 21.679 25.774\n\n\nNote: If the order of teCH and trCH had been reversed in msCH we would need to use details=list(autoini=2) to base parameter starting values on the trapping data, or provide start values manually.\n\nG.4.3 Habitat mask for telemetry data\nThe centres of both detected and telemetry-only animals are assumed to lie on the habitat mask. Ensure the mask is large enough to encompass telemetry-only animals. A conservative approach is to buffer around the individual telemetry centroids. Using teCH from before:\n\ncentroids &lt;- data.frame(t(sapply(telemetryxy(teCH), \n    apply, 2, mean)))\nmask1 &lt;- make.mask(centroids, buffer = 100, type = 'trapbuffer')\n\nFor composite telemetry and capture–recapture, buffering should include the detector sites:\n\ntmpxy &lt;- rbind(centroids, data.frame(traps(trCH))) \nmask2 &lt;- make.mask(tmpxy, buffer = 100, type = 'trapbuffer')\n\n\npar(mfrow = c(1,2))\nplot(mask1)\nplot(teCH, add = TRUE, title = 'Telemetry only')\nplot(mask2)\nplot(traps(trCH), add = TRUE)\nplot(teCH, add = TRUE, title = 'Telemetry and detector sites')\n\n\n\n\n\n\nFigure G.3: Habitat masks prepared by buffering around telemetry sites (left) or both telemetry and detector sites (right).\n\n\n\n\nThe mask generated automatically by secr.fit buffers around both detector sites and telemetry fixes, as shown here. The ‘buffer’ argument in make.mask can be problematic when used with a standalone telemetry traps object because the notional detector location is an arbitrary point - it is better to use the centroid coordinates as input.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Telemetry</span>"
    ]
  },
  {
    "objectID": "A07-telemetry.html#simulation",
    "href": "A07-telemetry.html#simulation",
    "title": "Appendix G — Telemetry",
    "section": "\nG.5 Simulation",
    "text": "G.5 Simulation\nSimulation of joint capture–recapture and telemetry data is a 2-step operation in secr, with the steps depending on the type of telemetry sampling. Here is an example of each type.\nWe choose to fix the number of observations per animal at 25 using the exactN argument of sim.capthist. The same effect can be achieved by increasing the number of occasions to 25 and setting exactN = 1.\n\nG.5.1 Independent telemetry\nFor independent data there is no specified connection between the populations sampled, so we separately generate telemetry and capture–recapture datasets and stick them together.\n\n# detectors\nte &lt;- make.telemetry()\ntr &lt;- make.grid(nx = 8, ny = 8, detector = \"proximity\")\npop1 &lt;- sim.popn(tr, D = 10, buffer = 200)\npop2 &lt;- sim.popn(core = tr, buffer = 200, Nbuffer = 20, \n                 Ndist = 'fixed')\ntrCH &lt;- sim.capthist(tr,  popn = pop1, detectfn = \"HHN\", \n    detectpar = list(lambda0 = 0.1, sigma = 25))\nteCH &lt;- sim.capthist(te,  popn = pop2, detectfn = \"HHN\", \n    detectpar = list(sigma = 25), noccasions = 1, exactN = 25)\nCHI &lt;- addTelemetry(trCH, teCH, type = 'independent')\n\nNo errors found :-)\n\nsession(CHI) &lt;- 'Independent'\nsummary(CHI)\n\nObject class       capthist \nDetector type      proximity (5), telemetry \nTelemetry type     independent \nDetector number    64 \nAverage spacing    20 m \nx-range            0 140 m \ny-range            0 140 m \n\nUsage range by occasion\n    1 2 3 4 5 6\nmin 0 0 0 0 0 0\nmax 1 1 1 1 1 1\n\nCounts by occasion \n                   1  2  3  4  5   6 Total\nn                 24 16 19 22 17  20   118\nu                 24  9  7  5  1  20    66\nf                 35 14 13  4  0   0    66\nM(t+1)            24 33 40 45 46  66    66\nlosses             0  0  0  0  0   0     0\ndetections        29 19 30 26 19 500   623\ndetectors visited 23 16 23 23 18   0   103\ndetectors used    64 64 64 64 64   0   320\n\nEmpty histories :  20 \n20 telemetered animals, 0 detected\n25-25 locations per animal, mean =  25, sd = 0 \n\nIndividual covariates\n sex   \n F:34  \n M:32  \n\n\n\nG.5.2 Dependent telemetry\nFor dependent data the telemetry sample is drawn from animals caught during the capture–recapture phase. This example uses the previously constructed ‘traps’ objects (tr and te). The original numbering of animals must be conserved (renumber = FALSE).\n\npop3 &lt;- sim.popn(tr, D = 10, buffer = 200)\ntrCH &lt;- sim.capthist(tr,  popn = pop3, detectfn = \"HHN\", \n    detectpar = list(lambda0 = 0.1, sigma = 25), renumber = \n    FALSE, savepopn = TRUE)\n\n## select trapped animals from saved popn\npop3D &lt;- subset(attr(trCH, 'popn'), rownames(trCH))\n## sample 12 detected animals for telemetry\npop3Dt &lt;- subset(pop3D, sample.int(nrow(pop3D), 12))  \n## simulate telemetry\nteCHD &lt;- sim.capthist(te, popn = pop3Dt, renumber = FALSE, \n    detectfn = \"HHN\", detectpar = list(sigma = 25), \n    noccasions = 1, exactN = 25)\nCHD &lt;- addTelemetry(trCH, teCHD, type = 'dependent')\n\nNo errors found :-)\n\nsession(CHD) &lt;- 'Dependent'\nsummary(CHD)\n\nObject class       capthist \nDetector type      proximity (5), telemetry \nTelemetry type     dependent \nDetector number    64 \nAverage spacing    20 m \nx-range            0 140 m \ny-range            0 140 m \n\nUsage range by occasion\n    1 2 3 4 5 6\nmin 0 0 0 0 0 0\nmax 1 1 1 1 1 1\n\nCounts by occasion \n                   1  2  3  4  5   6 Total\nn                 20 22 22 10 26  12   112\nu                 20 12  8  1  4   0    45\nf                 12 14  9  5  5   0    45\nM(t+1)            20 32 40 41 45  45    45\nlosses             0  0  0  0  0   0     0\ndetections        33 31 29 11 36 300   440\ndetectors visited 25 25 26 10 26   0   112\ndetectors used    64 64 64 64 64   0   320\n12 telemetered animals, 12 detected\n25-25 locations per animal, mean =  25, sd = 0 \n\nIndividual covariates\n sex   \n F:20  \n M:25  \n\n\n\nG.5.3 Concurrent telemetry\nFor concurrent telemetry a sample of animals is taken from the regional population without reference to whether or not each animal was detected in the capture–recapture phase. The original numbering of animals must be conserved (renumber = FALSE), as for dependent telemetry.\n\nset.seed(567) \npop4 &lt;- sim.popn(tr, D = 10, buffer = 200)\n# select 15 individuals at random from larger population\npop4C &lt;- subset(pop4, sample.int(nrow(pop4), 15))\n# original animalID (renumber = FALSE) are needed for matching\ntrCH &lt;- sim.capthist(tr,  popn = pop4, renumber = FALSE, \n    detectfn = \"HHN\", detectpar = list(lambda0 = 0.1, \n    sigma = 25))\nteCHC &lt;- sim.capthist(te, popn = pop4C, renumber = FALSE, \n    detectfn = \"HHN\", detectpar = list(sigma = 25), noccasions =\n    1, exactN = 25)\nCHC &lt;- addTelemetry(trCH, teCHC, type = 'concurrent')\n\nNo errors found :-)\n\nsession(CHC) &lt;- 'Concurrent'\nsummary(CHC)\n\nObject class       capthist \nDetector type      proximity (5), telemetry \nTelemetry type     concurrent \nDetector number    64 \nAverage spacing    20 m \nx-range            0 140 m \ny-range            0 140 m \n\nUsage range by occasion\n    1 2 3 4 5 6\nmin 0 0 0 0 0 0\nmax 1 1 1 1 1 1\n\nCounts by occasion \n                   1  2  3  4  5   6 Total\nn                 16 17 21 17 16  15   102\nu                 16 12  7  5  3  11    54\nf                 28 11  9  5  1   0    54\nM(t+1)            16 28 35 40 43  54    54\nlosses             0  0  0  0  0   0     0\ndetections        25 26 27 26 24 375   503\ndetectors visited 23 20 22 22 19   0   106\ndetectors used    64 64 64 64 64   0   320\n\nEmpty histories :  11 \n15 telemetered animals, 4 detected\n25-25 locations per animal, mean =  25, sd = 0 \n\nIndividual covariates\n sex   \n F:24  \n M:30  \n\n\n\nG.5.4 Plotting to compare simulated data\n\npar(mfrow = c(1,3), mar = c(2,2,4,2), xpd = TRUE)\nplot(traps(CHI), border = 200, gridlines = FALSE, bty = 'o')\nplot(CHI, type = 'telemetry', tracks = TRUE, add = TRUE)\n\nplot(traps(CHD), border = 200, gridlines = FALSE, bty = 'o')\nplot(CHD, type = 'telemetry', tracks = TRUE, add = TRUE)\n\nplot(traps(CHC), border = 200, gridlines = FALSE, bty = 'o')\nplot(CHC, type = 'telemetry', tracks = TRUE, add = TRUE)\n\n\n\n\n\n\nFigure G.4: Simulated telemetry data in relation to a capture–recapture grid (red crosses). Independently telemetered individuals are not recognised if they are caught on the grid. Dependent telemetry is restricted to animals caught on the grid. Individuals telemetered concurrently may or may not be caught, but are recognised when they are.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Telemetry</span>"
    ]
  },
  {
    "objectID": "A07-telemetry.html#sec-telemetrytechnotes",
    "href": "A07-telemetry.html#sec-telemetrytechnotes",
    "title": "Appendix G — Telemetry",
    "section": "\nG.6 Technical notes",
    "text": "G.6 Technical notes\n\nG.6.1 Assumption of common \\sigma\n\nJoint analysis of telemetry and capture–recapture data usually relies on the assumption that the same value of the parameter \\sigma applies in both sampling processes. This does not hold when\n\ntelemetry fixes have large measurement error that inflates \\sigma, or\nthe tendency of an animal to interact with a detector after encountering it varies systematically with distance from the home-range centre, or\nactivity is not stationary and the telemetry and capture–recapture data relate to different time intervals.\n\nThe assumption may be avoided altogether by modelling distinct values of \\sigma on trapping and telemetry occasions. This is readily achieved using the automatic predictor tt in the formula for sigma, as in secr.fit(CH, detectfn = 'HHN', model = sigma~tt, ...). The model then has one level of sigma for non-telemetry occasions (tt = ‘nontelemetry’) and another for telemetry occasions (tt = ‘telemetry’). However, this sacrifices much of the benefit from a joint analysis when the telemetry data are dependent or concurrent, and all benefit for independent telemetry data.\n\nG.6.2 Numerical problems\nFitting joint telemetry and SECR models can be difficult - the usual computations in secr may fail to return a likelihood. The problem is often due to a near-zero value in a component of the telemetry likelihood. This occurs particularly in large datasets. The problem may be fixed by scaling the offending values by an arbitrary large number given in the details argument ‘telemetryscale’. The required magnitude for ‘telemetryscale’ may be found by experimentation (try 1e3, 1e6, 1e9, 1e12 etc.). This ad hoc solution must be applied consistently if models are to be compared by AIC.\n\nte &lt;- make.telemetry()\nteCH2 &lt;- sim.capthist(te, popn = list(D = 2, buffer = 200), \n    detectfn = \"HHN\", exactN = 100, detectpar = list(sigma = 25),\n    noccasions = 1)\nmask &lt;- make.mask(traps(teCH2), buffer = 300, type = \n    'trapbuffer')\n# fails\nfit1 &lt;- secr.fit(teCH2, mask = mask, detectfn = 'HHN', CL = TRUE, \n    trace = FALSE, details = list(telemetryscale = 1)) \npredict(fit1)\n\n      link estimate SE.estimate lcl ucl\nsigma  log       NA          NA  NA  NA\n\n# succeeds\nfit1000 &lt;- secr.fit(teCH2, mask = mask, detectfn = 'HHN', \n    CL = TRUE, trace = FALSE, details = list(telemetryscale = \n    1e3)) \npredict(fit1000)\n\n      link estimate SE.estimate    lcl    ucl\nsigma  log   25.044     0.20636 24.643 25.452\n\n\nNumerical problems may also be caused by inappropriate starting values, poor model specification, or an unknown bug. It may help to use the longer-tailed detection function ‘HEX’ instead of ‘HHN’.\n\nG.6.3 Learned response\nLearned responses (b, bk) are not expected in telemetry data. However, they may make sense for the ‘SECR’ occasions of a composite dataset (combined stationary detectors and telemetry). There is no way to avoid a global learned response (b) from propagating to the telemetry occasions (i.e. modelling different telemetry sigmas for animals detected or not detected in the pre-telemetry phase). A site-specific learned response, however, cannot propagate to the telemetry phase if there is a single telemetry ‘occasion’ because (i) no animal is detected at the notional telemetry detector in the pre-telemetry phase, and (ii) there is no opportunity for learning within the telemetry phase if all detections are on one occasion.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Telemetry</span>"
    ]
  },
  {
    "objectID": "A07-telemetry.html#telemetrylimitations",
    "href": "A07-telemetry.html#telemetrylimitations",
    "title": "Appendix G — Telemetry",
    "section": "\nG.7 Limitations",
    "text": "G.7 Limitations\nSome important functions of secr have yet to be updated to work with telemetry data. These are listed in Section G.8. Other limitations are described here.\n\nG.7.1 Incompatible with area search\nTelemetry data may not be combined with area-search (polygon) data except as independent data in distinct sessions. This is because the polygon data types presently implemented in secr must be constant across a session.\nIf the ‘independent data, distinct-session’ solution is inadequate you might try rasterizing the search area (function discretize).\n\nG.7.2 Incompatible with mark–resight\nTelemetry data may not be combined with mark-resight except possibly in distinct sessions (this has not been tested).\n\nG.7.3 Incompatible with hybrid heterogeneity model\nThe secr.fit code for hybrid heterogeneity models (hcov) has yet to be updated.\n\nG.7.4 Non-Euclidean distance\nNon-Euclidean distance methods cannot be used with telemetry data at present (a very large distance matrix would be required).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Telemetry</span>"
    ]
  },
  {
    "objectID": "A07-telemetry.html#sec-telemetryappendix2",
    "href": "A07-telemetry.html#sec-telemetryappendix2",
    "title": "Appendix G — Telemetry",
    "section": "\nG.8 Telemetry-related functions.",
    "text": "G.8 Telemetry-related functions.\n\n\nTable G.1: Functions specifically for telemetry data.\n\n\n\n\n\n\n\nFunction\nPurpose\n\n\n\naddTelemetry\ncombine capture-recapture and telemetry data in new capthist\n\n\nmake.telemetry\nbuild a traps object for standalone telemetry data\n\n\nread.telemetry\ninput telemetry fixes from text file or dataframe\n\n\ntelemetered\ndetermine which animals in a capthist object have telemetry data\n\n\ntelemetrytype\nextract or replace the ‘telemetrytype’ attribute of a traps object\n\n\ntelemetryxy\nextract or replace telemetry coordinates from capthist\n\n\nxy2CH\nmake a standalone telemetry capthist from a composite capthist\n\n\n\n\n\n\n\n\nTable G.2: Telemetry-ready general functions. Functions marked with an asterisk (*) use the telemetry coordinates if the capthist is telemetry-only, otherwise the detection sites.\n\n\n\nFunction\nPurpose\n\n\n\nderived\nHorvitz-Thompson-like density estimate\n\n\njoin\ncombine sessions of multi-session capthist object\n\n\nmake.capthist\nbuild capthist object\n\n\nmoves\nsequential movements*\n\n\nMS.capthist\nform multi-session capthist from separate sessions\n\n\nplot.capthist\nplotting (type = ‘telemetry’)\n\n\nrbind.capthist\nconcatenate rows of capthist\n\n\n\nRPSV, MMDM, ARL, dbar\n\nindices of home-range size*\n\n\nsecr.fit\nmodel fitting\n\n\nsim.capthist\ngenerate capthist data\n\n\nsubset.capthist\nselect subset of animals, occasions or detectors\n\n\nsummary.capthist\nsummary\n\n\nverify.capthist\nperform integrity checks\n\n\nverify.traps\nperform integrity checks\n\n\n\n\n\n\n\n\nTable G.3: General functions not ready for telemetry.\n\n\n\nFunction\nPurpose\n\n\n\nreduce.capthist\nchange detector type or collapse occasions\n\n\nsim.secr\nparametric bootstrap fitted model\n\n\nsimulate\nsimulate from fitted model\n\n\nsecr.test\nanother parametric boostrap\n\n\nfxTotal\n\n\n\nfxiContour\n\n\n\n\n\n\n\n\n\n\nFigure G.1: Simulated trapping and telemetry data. The 5-day trapping study (left) yielded 29 recaptures. Telemetry data were obtained on 10 occasions for each of 12 animals (right). Points for animals that were both trapped and telemetered are ringed in black. Colours distinguish individuals.\nFigure G.2: Schematic relationship of capture–recapture data to three types of telemetry data. Vertical overlap indicates individuals that appear in both datasets.\nFigure G.3: Habitat masks prepared by buffering around telemetry sites (left) or both telemetry and detector sites (right).\nFigure G.4: Simulated telemetry data in relation to a capture–recapture grid (red crosses). Independently telemetered individuals are not recognised if they are caught on the grid. Dependent telemetry is restricted to animals caught on the grid. Individuals telemetered concurrently may or may not be caught, but are recognised when they are.\n\n\n\nCalhoun, J. B., & Casby, J. U. (1958). Calculation of home range and density of small mammals. Public Health Monograph, 55.\n\n\nChandler, R. B., Crawford, D. A., Garrison, E. P., Miller, K. V., & Cherry, M. J. (2021). Modeling abundance, distribution, movement and space use with camera and telemetry data. Ecology, 103(10). https://doi.org/10.1002/ecy.3583\n\n\nGopalaswamy, A. M., Royle, J. A., Delampady, M., Nichols, J. D., Karanth, K. U., & Macdonald, D. W. (2012). Density estimation in tiger populations: Combining information for strong inference. Ecology, 93(7), 1741–1751. https://doi.org/10.1890/11-2110.1\n\n\nKendeigh, S. C. (1944). Measurement of bird populations. Ecological Monographs, 14, 67–106. https://doi.org/10.2307/1961632\n\n\nLinden, D. W., Sirén, A. P. K., & Pekins, P. J. (2018). Integrating telemetry data into spatial capture–recapture modifies inferences on multi‐scale resource selection. Ecosphere, 9(4). https://doi.org/10.1002/ecs2.2203\n\n\nMargenau, L. L. S., Cherry, M. J., Miller, K. V., Garrison, E. P., & Chandler, R. B. (2022). Monitoring partially marked populations using camera and telemetry data. Ecological Applications, 32(4). https://doi.org/10.1002/eap.2553\n\n\nRuprecht, J. S., Eriksson, C. E., Forrester, T. D., Clark, D. A., Wisdom, M. J., Rowland, M. M., Johnson, B. K., & Levi, T. (2021). Evaluating and integrating spatial capture–recapture models with data of variable individual identifiability. Ecological Applications, 31(7). https://doi.org/10.1002/eap.2405\n\n\nSollmann, R., Gardner, B., Parsons, A. W., Stocking, J. J., McClintock, B. T., Simons, T. R., Pollock, K. H., & O’Connell, A. F. (2013). A spatial mark-resight model augmented with telemetry data. Ecology, 94, 553–559.\n\n\nWhittington, J., Hebblewhite, M., & Chandler, R. (2018). Generalized spatial mark-resight models with an application to grizzly bears. Journal of Applied Ecology, 55, 157–168.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Telemetry</span>"
    ]
  },
  {
    "objectID": "A07-telemetry.html#footnotes",
    "href": "A07-telemetry.html#footnotes",
    "title": "Appendix G — Telemetry",
    "section": "",
    "text": "This constraint arises from the need internally to normalise the probability density function for each telemetry fix. The normalising constant for these functions is 1/(2\\pi \\sigma^2), whereas for most other possible values of detectfn it is hard to compute or the function does not correspond to a probability density.↩︎",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Telemetry</span>"
    ]
  },
  {
    "objectID": "A08-parameterizations.html",
    "href": "A08-parameterizations.html",
    "title": "Appendix H — Parameterizations",
    "section": "",
    "text": "H.1 Theory\nThe idea is to replace a primary detection parameter with a function of other parameter(s) in the SECR model. This may allow constraints to be applied more meaningfully. Specifically, it may make sense to consider a function of the parameters to be constant, even when one of the primary parameters varies. The new parameter also may be modelled as a function of covariates etc.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Parameterizations</span>"
    ]
  },
  {
    "objectID": "A08-parameterizations.html#theory",
    "href": "A08-parameterizations.html#theory",
    "title": "Appendix H — Parameterizations",
    "section": "",
    "text": "H.1.1 \\lambda_0 and \\sigma\n\n\nOne published example concerns compensatory heterogeneity of detection parameters (Efford and Mowat 2014). Combinations of \\lambda_0 and \\sigma yield the same effective sampling area a when the cumulative hazard of detection (\\lambda(d))1 is a linear function of home-range utilisation. Variation in home-range size then has no effect on estimates of density. It would be useful to allow \\sigma to vary while holding a constant, but this has some fishhooks because computation of \\lambda_0 from a and \\sigma is not straightforward. A simple alternative is to substitute a_0 = 2 \\pi \\lambda_0 \\sigma^2; Efford & Mowat (2014) called a_0 the ‘single-detector sampling area’. If the sampling regime is constant, holding a_0 constant is almost equivalent to holding a constant (but see Limitations). Fig. H.1 illustrates the relationship for 3 levels of a_0.\n\n\n\n\n\n\n\nFigure H.1: Structural relationship between parameters \\lambda_0 and \\sigma expressed by holding a_0 constant in \\lambda_0 = a_0 / (2 \\pi \\sigma^2).\n\n\n\n\n\nH.1.2 \\sigma and D\n\n Another biologically interesting structural relationship is that between population density and home-range size (Efford et al., 2016). If home ranges have a definite edge and partition all available space then an inverse-square relationship is expected D = (k / r)^2 or r = k / \\sqrt D, where r is a linear measure of home-range size (e.g., grid cell width) and k is a constant of proportionality. In reality, the home-range model that underlies SECR detection functions does not require a hard edge, so the language of ‘partitioning’ and Huxley (1934) [’s] ‘elastic discs’ does not quite fit. However, the inverse-square relationship is empirically useful, and we conjecture that it may also arise from simple models for constant overlap of home ranges when density varies – a topic for future research. For use in SECR we equate r with the spatial scale of detection \\sigma, and predict concave-up relationships as in Fig. H.2.\nThe relationship may be modified by adding a constant c to represent the lower asymptote of sigma as density increases ( \\sigma = k / \\sqrt D + c; by default c = 0 in secr).\nIt is possible, intuitively, that once a population becomes very sparse there is no further effect of density on home-range size. Alternatively, very low density may reflect sparseness of resources, requiring the few individuals present to exploit very large home ranges even if they seldom meet. If density is no longer related to \\sigma at low density, even indirectly, then the steep increase in \\sigma modelled on the left of Fig. H.3 will ‘level off’ at some value of \\sigma. We don’t know of any empirical example of this hypothetical phenomenon, and do not provide a model.\n\n\n\n\n\n\n\nFigure H.2: Structural relationship between parameters D and \\sigma expressed by holding k constant in \\sigma = 100 k / \\sqrt D. The factor of 100 adjusts for the inconsistent default units of \\sigma and D in secr (metres and hectares).\n\n\n\n\nWe use ‘primary parameter’ to mean one of (D, \\lambda_0, \\sigma)2 For each relationship there is a primary parameter considered the ‘driver’ that varies for external reasons (e.g., between times, sexes etc.), and a dependent parameter that varies in response to the driver, moderated by a ‘surrogate’ parameter that may be constant or under external control. The surrogate parameter appears in the model in place of the dependent parameter. Using the surrogate parameterization is exactly equivalent to the default parameterization if the driver parameter(s) (\\sigma and \\lambda_0 for a_03, D for k4) are constant.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Parameterizations</span>"
    ]
  },
  {
    "objectID": "A08-parameterizations.html#implementation",
    "href": "A08-parameterizations.html#implementation",
    "title": "Appendix H — Parameterizations",
    "section": "\nH.2 Implementation",
    "text": "H.2 Implementation\nParameterizations in secr are indicated by an integer code (Table H.1). The internal implementation of the parameterizations (3)–(5) is straightforward. At each evaluation of the likelihood function:\n\nThe values of the driver and surrogate parameters are determined\nEach dependent parameter is computed from the relevant driver and surrogate parameters\nValues of the now-complete set of primary parameters are passed to the standard code for evaluating the likelihood.\n\n\n\nTable H.1: Parameterization codes.\n\n\n\n\n\n\n\n\n\n\nCode\nDescription\nDriver\nSurrogate(s)\nDependent\n\n\n\n0\nDefault\n–\n–\n–\n\n\n3\nSingle-detector sampling area\n\\sigma\na_0\n\\lambda_0 = a_0/(2\\pi\\sigma^2)\n\n\n4\nDensity-dependent home range\nD\n\nk, c\n\n\\sigma = 100 k / \\sqrt D + c\n\n\n5\n3 & 4 combined\n\nD, \\sigma\n\n\nk, c, a_0\n\n\n\\sigma = 100 k / \\sqrt D + c, \\lambda_0 = a_0/(2\\pi\\sigma^2)\n\n\n\n\n\n\n\nThe transformation is performed independently for each level of the surrogate parameters that appears in the model. For example, if the model includes a learned response a0 ~ b, there are two levels of a0 (for naive and experienced animals) that translate to two levels of lambda0. For parameterization (4), \\sigma = 100 k / \\sqrt D. The factor of 100 is an adjustment for differing units (areas are expressed in hectares in secr, and 1 hectare = 10 000 \\mathrm{m}^2). For parameterization (5), \\sigma is first computed from D, and then \\lambda_0 is computed from \\sigma.\n\nH.2.1 Interface\nUsers choose between parameterizations either\n\nexplicitly, by setting the ‘param’ component of the secr.fit argument ‘details’, or\nimplicitly, by including a parameterization-specific parameter name in the secr.fit model.\n\nImplicit selection causes the value of details$param to be set automatically (with a warning).\nThe main parameterization options are listed in Table 1 (other specialised options are listed in Section H.5).\nThe constant c in the relationship \\sigma = k / \\sqrt{D} + c is set to zero and not estimated unless ‘c’ appears explicitly in the model. For example, model = list(sigmak ~ 1) fixes c = 0, whereas model = list(sigmak ~ 1, c ~ 1) causes c to be estimated. The usefulness of this model has yet to be proven! By default an identity link is used for ‘c’, which permits negative values; negative ‘c’ implies that for some densities (most likely densities outside the range of the data) a negative sigma is predicted. If you’re uncomfortable with this and require ‘c’ to be positive then set link = list(c = 'log') in secr.fit and specify a positive starting value for it in start (using the vector form for that argument of secr.fit).\nInitial values may be a problem as the scales for a0 and sigmak are not intuitive. Assuming automatic initial values can be computed for a half-normal detection function with parameters g_0 and \\sigma, the default initial value for a_0 is 2 \\pi g_0 \\sigma^2 /10000, and for k, \\sigma \\sqrt{D}. If the usual automatic procedure (see ?autoini) fails then ad hoc and less reliable starting values are used. In case of trouble, it is suggested that you first fit a very simple (or null) model using the desired parameterization, and then use this to provide starting values for a more complex model. Here is an example (actually a trivial one for which the default starting values would have been OK; some warnings are suppressed):\n\nfit0  &lt;- secr.fit(captdata, model = a0~1, detectfn = 'HHN',\n                  trace = FALSE)\nfitbk &lt;- secr.fit(captdata, model = a0~bk, detectfn = 'HHN', \n                  trace = FALSE, start = fit0)\n\n\nH.2.2 Models for surrogate parameters a0 and sigmak\nThe surrogate parameters a0 and sigmak are treated as if they are full ‘real’ parameters, so they appear in the output from predict.secr, and may be modelled like any other ‘real’ parameter. For example, model = sigmak ~ h2 is valid.\nDo not confuse this with the modelling of primary ‘real’ parameters as functions of covariates, or built-in effects such as a learned response.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Parameterizations</span>"
    ]
  },
  {
    "objectID": "A08-parameterizations.html#example",
    "href": "A08-parameterizations.html#example",
    "title": "Appendix H — Parameterizations",
    "section": "\nH.3 Example",
    "text": "H.3 Example\nAmong the datasets included with secr, only ovenCH provides a useful temporal sequence - 5 years of data from mistnetting of ovenbirds (Seiurus aurocapilla) at Patuxent Research Refuge, Maryland. A full model for annually varying density and detection parameters may be fitted with\n\nmsk &lt;- make.mask(traps(ovenCH[[1]]), buffer = 300, nx = 25)\noven0509b &lt;- secr.fit(ovenCH, model = list(D ~ session, \n    sigma ~ session, lambda0 ~ session + bk), mask = msk, \n    detectfn = 'HHN', trace = FALSE)\n\nThis has 16 parameters and takes some time to fit.\nWe hypothesize that home-range (territory) size varied inversely with density, and model this by fixing the parameter k. Efford & Mowat (2014) reported for this dataset that \\lambda_0 did not compensate for within-year, between-individual variation in \\sigma, but it is nevertheless possible that variation between years was compensatory, and we model this by fixing a_0. For good measure, we also allow for site-specific net shyness by modelling a_0 with the builtin effect ‘bk’:\n\noven0509bs &lt;- secr.fit(ovenCH, model = list(D ~ session, sigmak ~ 1,\n    a0 ~ bk), mask = msk, detectfn = 'HHN', trace = FALSE)\n\nWarning: Using parameterization details$param = 5\n\n\nThe effect of including both sigmak and a0 in the model is to force parameterization (5). The model estimates a different density in each year, as in the previous model. Annual variation in D drives annual variation in \\sigma through the relation \\sigma_y = k / \\sqrt{D_y} where k (= sigmak/100) is a parameter to be estimated and the subscript y indicates year. The detection function ‘HHN’ is the hazard-half-normal which has parameters \\sigma and \\lambda_0. We already have year-specific \\sigma_y, and this drives annual variation in \\lambda_0: \\lambda_{0y} = a_{0X} / (2 \\pi \\sigma_y^2) where a_{0X} takes one of two different values depending on whether the bird in question has been caught previously in this net.\nThis is a behaviourally plausible and fairly complex model, but it uses just 8 parameters compared to 16 in a full annual model with net shyness. It may be compared by AIC with the full model (the model structure differs but the data are the same). Although the new model has somewhat higher deviance (1858.5 vs 1851.6), the reduced number of parameters results in a substantially lower AIC (\\DeltaAIC = 9.1).\nIn Fig. H.3 we illustrate the results by overplotting the fitted curve for \\sigma_y on a scatter plot of the separate annual estimates from the full model. A longer run of years was analysed by Efford et al. (2016).\n\n\n\n\n\n\n\nFigure H.3: Fitted structural relationship between parameters D and \\sigma (curve; \\hat k = r round(predict(oven0509bs)[[1]]['sigmak','estimate']/100,3)) and separate annual estimates (ovenbirds mistnetted on Patuxent Research Refuge 2005–2009).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Parameterizations</span>"
    ]
  },
  {
    "objectID": "A08-parameterizations.html#sec-paramlimitations",
    "href": "A08-parameterizations.html#sec-paramlimitations",
    "title": "Appendix H — Parameterizations",
    "section": "\nH.4 Limitations",
    "text": "H.4 Limitations\nUsing a_0 as a surrogate for a is unreliable if the distribution or intensity of sampling varies. This is because a_0 depends only on the parameter values, whereas a depends also on the detector layout and duration of sampling. For example, if a different size of trapping grid is used in each session, a will vary even if the detection parameters, and hence a_0, stay the same. This is also true (a varies, a_0 constant) if the same trapping grid is operated for differing number of occasions in each session. It is a that really matters, and constant a_0 is not a sensible null model in these scenarios.\nparameterizations (4) and (5) make sense only if density D is in the model; an attempt to use these when maximizing only the conditional likelihood (CL = TRUE) will cause an error.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Parameterizations</span>"
    ]
  },
  {
    "objectID": "A08-parameterizations.html#sec-othernotes",
    "href": "A08-parameterizations.html#sec-othernotes",
    "title": "Appendix H — Parameterizations",
    "section": "\nH.5 Other notes",
    "text": "H.5 Other notes\nDetection functions 0–3 and 5–8 (‘HN’,‘HR’,‘EX’, ‘CHN’, ‘WEX’, ‘ANN’, ‘CLN’, ‘CG’) describe the probability of detection g(d) and use g_0 as the intercept instead of \\lambda_0. Can parameterizations (3) and (5) also be used with these detection functions? Yes, but the user must take responsibility for the interpretation, which is less clear than for detection functions based on the cumulative hazard (14–19, or ‘HHN’, ‘HHR’, ‘HEX’, ‘HAN’, ‘HCG’, ‘HVP’). The primary parameter is computed as g_0 = 1 - \\exp(-a_0 / (2\\pi \\sigma^2)).\nIn a sense, the choice between detection functions ‘HN’ and ‘HHN’, ‘EX’ and ‘HEX’ etc. is between two parameterizations, one with half-normal hazard \\lambda(d) and one with half-normal probability g(d), always with the relationship g(d) = 1 - \\exp(-\\lambda(d)) (using d for the distance between home-range centre and detector). It may have been clearer if this had been programmed originally as a switch between ‘hazard’ and ‘probability’ parameterizations, but this would now require significant changes to the code and is not a priority.\nIf a detection function is specified that requires a third parameter (e.g., z in the case of the hazard-rate function ‘HR’) then this is carried along untouched.\nIt is possible that home range size, and hence \\sigma, varies in a spatially continuous way with density. The sigmak parameterization does not work when density varies spatially within one population because of the way models of state variables (D) and detection variables (g_0, \\lambda_0, \\sigma) are separated within secr. Non-Euclidean distance methods allow a workaround as described in Appendix F and Efford et al. (2016).\nSome parameterization options (Table H.2) were not included in Table H.1 because they are not intended for general use and their implementation may be incomplete (e.g., not allowing covariates). Although parameterizations (2) and (6) promise a ‘pure’ implementation in terms of the effective sampling area a rather than the surrogate a_0, this option has not been implemented and tested as extensively as that for a_0 (parameterization 3). The transformation to determine \\lambda_0 or g_0 requires numerical root finding, which is slow. Also, assuming constant a does not make sense when either the detector array or the number of sampling occasions varies, as both of these must affect a. Use at your own risk!\n\n\nTable H.2: Additional parameterizations.\n\n\n\n\n\n\n\n\n\n\nCode\nDescription\nDriver\nSurrogate(s)\nDependent\n\n\n\n2\nEffective sampling area\n\\sigma\na\n\n\\lambda_0 such that a = \\int p_\\cdot(\\mathbf x|\\sigma, \\lambda_0) d\\mathbf x\n\n\n\n6\n2 & 4 combined\nD\n\nk, c, a\n\n\n\\sigma = 100 k / \\sqrt D + c, \\lambda_0 as above",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Parameterizations</span>"
    ]
  },
  {
    "objectID": "A08-parameterizations.html#abundance",
    "href": "A08-parameterizations.html#abundance",
    "title": "Appendix H — Parameterizations",
    "section": "\nH.6 Abundance",
    "text": "H.6 Abundance\nWe use density D as the primary parameter for abundance; the number of activity centres N(A) is secondary as it is contingent on delineation of an area A.\nAppendix J considers an alternative parameterization of multi-session density in terms of the initial density D_1 and session-to-session trend \\lambda_t.\n\n\n\nFigure H.1: Structural relationship between parameters \\lambda_0 and \\sigma expressed by holding a_0 constant in \\lambda_0 = a_0 / (2 \\pi \\sigma^2).\nFigure H.2: Structural relationship between parameters D and \\sigma expressed by holding k constant in \\sigma = 100 k / \\sqrt D. The factor of 100 adjusts for the inconsistent default units of \\sigma and D in secr (metres and hectares).\nFigure H.3: Fitted structural relationship between parameters D and \\sigma (curve; \\hat k = r round(predict(oven0509bs)[[1]]['sigmak','estimate']/100,3)) and separate annual estimates (ovenbirds mistnetted on Patuxent Research Refuge 2005–2009).\n\n\n\nEfford, M. G., Dawson, D. K., Jhala, Y. V., & Qureshi, Q. (2016). Density-dependent home-range size revealed by spatially explicit capture-recapture. Ecography, 39, 676–688.\n\n\nEfford, M. G., & Mowat, G. (2014). Compensatory heterogeneity in spatially explicit capture-recapture data. Ecology, 95, 1341–1348.\n\n\nHuxley, J. S. (1934). A natural experiment on the territorial instinct. British Birds, 27, 270–277.\n\n\nKendeigh, S. C. (1944). Measurement of bird populations. Ecological Monographs, 14, 67–106. https://doi.org/10.2307/1961632",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Parameterizations</span>"
    ]
  },
  {
    "objectID": "A08-parameterizations.html#footnotes",
    "href": "A08-parameterizations.html#footnotes",
    "title": "Appendix H — Parameterizations",
    "section": "",
    "text": "The cumulative hazard \\lambda(d) and probability g(d) formulations are largely interchangeable because g(d) = 1 - \\exp(-\\lambda(d)).↩︎\nsecr names D, lambda0 or sigma.↩︎\nsecr name a0↩︎\nsecr uses sigmak = 100k↩︎",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Parameterizations</span>"
    ]
  },
  {
    "objectID": "A09-densityappendix.html",
    "href": "A09-densityappendix.html",
    "title": "Appendix I — Density revisited",
    "section": "",
    "text": "I.1 User-provided model functions\nSome density models cannot be coded in the generalized linear model form of the model argument. To alleviate this problem, a model may be specified as an R function that is passed to secr.fit, specifically as the component ‘userDfn’ of the list argument ‘details’. We document this feature here, although you may never use it.\nThe userDfn function must follow some rules.\nThe coefficients form the density part of the full vector of beta coefficients used by the likelihood maximization function (nlm or optim). Ideally, the first one should correspond to an intercept or overall density, as this is what appears in the output of predict.secr. If transformation of density to the `link’ scale is required then it should be hard-coded in userDfn.\nCovariates are available to user-provided functions, but within the function they must be extracted ‘manually’ (e.g., covariates(mask)$habclass rather than just ‘habclass’). To pass other arguments (e.g., a basis for splines), add attribute(s) to the mask.\nIt will usually be necessary to specify starting values for optimisation manually with the start argument of secr.fit.\nIf the parameter values in Dbeta are invalid the function should return an array of all zero values.\nHere is a ‘null’ userDfn that emulates D \\sim 1 with log link\nuserDfn0 &lt;- function (Dbeta, mask, ngroup, nsession) {\n    if (Dbeta[1] == \"name\") return (\"0\")\n    if (Dbeta[1] == \"parameters\") return (\"intercept\")\n    D &lt;- exp(Dbeta[1])   # constant for all points\n    tempD &lt;- array(D, dim = c(nrow(mask), ngroup, nsession))\n    return(tempD)\n}\nWe can compare the result using userDfn0 to a fit of the same model using the ‘model’ argument. Note how the model description combines ‘user.’ and the name ‘0’.\nmodel.0 &lt;- secr.fit(captdata, model = D ~ 1, trace = FALSE)\nuserDfn.0 &lt;- secr.fit(captdata, details = list(userDfn = userDfn0), trace = FALSE)\nAIC(model.0, userDfn.0)[,-c(2,5,6)]\n\n                           model npar  logLik dAIC AICwt\nmodel.0         D~1 g0~1 sigma~1    3 -759.03    0   0.5\nuserDfn.0 D~userD.0 g0~1 sigma~1    3 -759.03    0   0.5\n\npredict(model.0)\n\n       link estimate SE.estimate      lcl      ucl\nD       log  5.47980    0.646741  4.35162  6.90047\ng0    logit  0.27319    0.027051  0.22348  0.32927\nsigma   log 29.36583    1.304938 26.91757 32.03677\n\npredict(userDfn.0)\n\n       link estimate SE.estimate      lcl      ucl\nD       log  5.47981    0.646741  4.35162  6.90047\ng0    logit  0.27319    0.027051  0.22348  0.32927\nsigma   log 29.36583    1.304938 26.91757 32.03677\nNot very exciting, maybe, but reassuring!",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Density revisited</span>"
    ]
  },
  {
    "objectID": "A09-densityappendix.html#sec-userDfn",
    "href": "A09-densityappendix.html#sec-userDfn",
    "title": "Appendix I — Density revisited",
    "section": "",
    "text": "It should accept four arguments, the first a vector of parameter values or a character value (below), and the second a ‘mask’ object, a data frame of x and y coordinates for points at which density must be predicted.\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\nDbeta\ncoefficients of density model, or one of c(‘name’, ‘parameters’)\n\n\nmask\nhabitat mask object\n\n\nngroup\nnumber of groups\n\n\nnsession\nnumber of sessions\n\n\n\n\nWhen called with Dbeta = \"name\", the function should return a character string to identify the density model in output. (This should not depend on the values of other arguments).\nWhen called with Dbeta = \"parameters\", the function should return a character vector naming each parameter. (When used this way, the call always includes the mask argument, so information regarding the model may be retrieved from any attributes of mask that have been set by the user).\nOtherwise, the function should return a numeric array with dim = c(nmask, ngroup, nsession) where nmask is the number of points (rows in mask). Each element in the array is the predicted density (natural scale, in animals / hectare) for each point, group and session. This is simpler than it sounds, as usually there will be a single session and single group.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Density revisited</span>"
    ]
  },
  {
    "objectID": "A09-densityappendix.html#sigmoid-density-function",
    "href": "A09-densityappendix.html#sigmoid-density-function",
    "title": "Appendix I — Density revisited",
    "section": "\nI.2 Sigmoid density function",
    "text": "I.2 Sigmoid density function\nNow let’s try a more complex example. First create a test dataset with an east-west density step (this could be done more precisely with sim.popn + sim.capthist):\n\nset.seed(123)\nch &lt;- subset(captdata, centroids(captdata)[,1]&gt;500 | runif(76) &gt; 0.75)\nplot(ch)\n# also make a mask and assign the x coordinate to covariate 'X'\nmsk &lt;- make.mask(traps(ch), buffer = 100, type = 'trapbuffer')\ncovariates(msk)$X &lt;- msk$x\n\n\n\n\n\n\nFigure I.1: Test data.\n\n\n\n\nNow define a sigmoid function of covariate X:\n\nsigmoidfn &lt;- function (Dbeta, mask, ngroup, nsession) {\n    scale &lt;- 7.5   # arbitrary 'width' of step\n    if (Dbeta[1] == \"name\") return (\"sig\")\n    if (Dbeta[1] == \"parameters\") return (c(\"D1\", \"threshold\", \"D2\"))\n    X2 &lt;- (covariates(mask)$X - Dbeta[2]) / scale\n    D &lt;- Dbeta[1] + 1 / (1+exp(-X2)) * (Dbeta[3] - Dbeta[1])\n    tempD &lt;- array(D, dim = c(nrow(mask), ngroup, nsession))\n    return(tempD)\n}\n\nFit null model and sigmoid model:\n\nfit.0 &lt;- secr.fit(ch, mask = msk, link = list(D = \"identity\"), trace = FALSE)\nfit.sigmoid &lt;- secr.fit(ch, mask = msk, details = list(userDfn = sigmoidfn), \n                        start = c(2.7, 500, 5.8, -1.2117, 3.4260), \n                        link = list(D = \"identity\"), trace = FALSE)\ncoef(fit.0)\n\n         beta  SE.beta     lcl      ucl\nD      3.6137 0.524474  2.5857  4.64164\ng0    -1.0875 0.161739 -1.4045 -0.77051\nsigma  3.3953 0.055649  3.2863  3.50439\n\ncoef(fit.sigmoid)\n\n                beta   SE.beta       lcl       ucl\nD.D1          1.6839  0.513850   0.67678   2.69104\nD.threshold 514.2646 16.652893 481.62551 546.90365\nD.D2          5.8810  1.047466   3.82802   7.93401\ng0           -1.0860  0.161699  -1.40297  -0.76912\nsigma         3.3931  0.055487   3.28439   3.50190\n\nAIC(fit.0, fit.sigmoid)[,-c(2,5,6)]\n\n                               model npar  logLik   dAIC AICwt\nfit.sigmoid D~userD.sig g0~1 sigma~1    5 -520.64  0.000     1\nfit.0               D~1 g0~1 sigma~1    3 -528.11 10.941     0\n\n\nThe sigmoid model has improved fit, but there is a lot of uncertainty in the two density levels. The average of the fitted levels D1 and D2 (3.7825) is not far from the fitted homogeneous level (3.6137).\n\nPlot codebeta &lt;- coef(fit.sigmoid)[1:3,'beta']\nX2 &lt;- (300:700 - beta[2]) / 15\nD &lt;- beta[1] + 1 / (1+exp(-X2)) * (beta[3] - beta[1])\nplot (300:700, D, type = 'l', xlab = 'X', ylab = 'Density', \n      ylim = c(0,7))\nabline(v = beta[2], lty = 2)\nabline(h = coef(fit.0)[1,1], lty = 1, col = 'blue')\nrug(unique(traps(ch)$x), col = 'red')\ntext(400, 2.2, 'D1')\ntext(620, 6.4, 'D2')\n\n\n\n\n\n\nFigure I.2: Sigmoid function fitted to test data.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Density revisited</span>"
    ]
  },
  {
    "objectID": "A09-densityappendix.html#sec-linkappendix",
    "href": "A09-densityappendix.html#sec-linkappendix",
    "title": "Appendix I — Density revisited",
    "section": "\nI.3 More on link functions",
    "text": "I.3 More on link functions\nThe density model for V covariates with an identity link is defined as\n\nD(\\mathbf x |  \\phi) = \\mathrm{max} [\\beta_0 + \\sum_{v=1}^V \\beta_v c_v(\\mathbf x), 0].\n\nFrom secr 4.5.0 there is a scaled identity link ‘i1000’ that multiplies each real parameter value by 1000. Then secr.fit(..., link = list(D = 'i1000')) is a fast alternative to specifying typsize for low absolute density.\nGoing further, you can even define your own ad hoc link function. To do this, provide the following functions in your workspace (your name ‘xxx’ combined with standard prefixes) and use your name to specify the link:\n\n\n\n\n\n\n\nName\nPurpose\nExample\n\n\n\nxxx\ntransform to link scale\ni100 &lt;- function(x) x * 100\n\n\ninvxxx\ntransform from link scale\ninvi100 &lt;- function(x) x / 100\n\n\nse.invxxx\ntransform SE from link scale\nse.invi100 &lt;- function (beta, sebeta) sebeta / 100\n\n\nse.xxx\ntransform SE to link scale\nse.i100 &lt;- function (real, sereal) sereal * 100\n\n\n\nFollowing this example, you would call secr.fit(..., link = list(D = 'i100')). To see the internal transformations for the standard link functions, type secr:::transform, secr:::untransform, secr:::se.untransform or secr:::se.transform.\n\n\n\nFigure I.1: Test data.\nFigure I.2: Sigmoid function fitted to test data.\n\n\n\nKendeigh, S. C. (1944). Measurement of bird populations. Ecological Monographs, 14, 67–106. https://doi.org/10.2307/1961632",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Density revisited</span>"
    ]
  },
  {
    "objectID": "A10-trend.html",
    "href": "A10-trend.html",
    "title": "Appendix J — Trend revisited",
    "section": "",
    "text": "J.1 ‘Dlambda’ parameterization\nWe parameterize the density model in terms of the initial density D_1 and the finite rates of increase \\lambda_t for the remaining sessions (\\lambda_1 refers to the density increase between Session 1 and Session 2, etc.). Reparameterization of the density model is achieved internally in secr.fit by manipulating the density design matrix to provide a new array of mask-cell- and session-specific densities at each evaluation of the full likelihood. This happens when the details argument ‘Dlambda’ is set to TRUE. The density model (D~) and the fitted coefficients take on a new meaning determined by the internal function Dfn2. More explanation is given in Section J.4.\nNow, fitting the ovenbird model with D~1 results in two density parameters (density in session 1, constant finite rate of increase across remaining sessions):\nmsk &lt;- make.mask(traps(ovenCH[[1]]), buffer = 300, nx = 32)\n fit1  &lt;- secr.fit(ovenCH, model = D~1, mask = msk, trace = \n     FALSE, details = list(Dlambda = TRUE))\n coef(fit1)\n\n           beta  SE.beta      lcl       ucl\nD.D1   0.032027 0.191324 -0.34296  0.407016\nD.D2  -0.063858 0.070151 -0.20135  0.073636\ng0    -3.561920 0.150653 -3.85719 -3.266645\nsigma  4.363968 0.081052  4.20511  4.522827\nDensity-relevant beta parameters have names starting with ‘D.’1. The first is the log initial density; others relate to the \\lambda parameters.\nTo make the most of the reparameterization we need the special prediction function predictDlambda to extract the lambda estimates (the simple predict method does not work).\npredictDlambda (fit1)\n\n        estimate SE.estimate     lcl    ucl\nD1       1.03254    0.199373 0.70966 1.5023\nlambda1  0.93814    0.065893 0.81762 1.0764\nlambda2  0.93814    0.065893 0.81762 1.0764\nlambda3  0.93814    0.065893 0.81762 1.0764\nlambda4  0.93814    0.065893 0.81762 1.0764\nThis is an advance on the earlier approach using sdif contrasts, as we have constrained \\lambda to a constant.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>Trend revisited</span>"
    ]
  },
  {
    "objectID": "A10-trend.html#covariate-and-other-trend-models",
    "href": "A10-trend.html#covariate-and-other-trend-models",
    "title": "Appendix J — Trend revisited",
    "section": "\nJ.2 Covariate and other trend models",
    "text": "J.2 Covariate and other trend models\nThe method allows many covariate models for \\lambda. We can fit a time trend in \\lambda using:\n\n fit2  &lt;- secr.fit(ovenCH, model = D~Session, mask = msk, \n     trace = FALSE, details = list(Dlambda = TRUE))\n predictDlambda (fit2)\n\n        estimate SE.estimate     lcl    ucl\nD1       0.90132    0.212018 0.57192 1.4204\nlambda1  1.14737    0.221235 0.78899 1.6685\nlambda2  0.99977    0.092474 0.83433 1.1980\nlambda3  0.87115    0.085988 0.71826 1.0566\nlambda4  0.75908    0.153416 0.51283 1.1236\n\n\nSession-specific \\lambda (lower-case ‘session’) provide a direct comparison with the original analysis:\n\n fit3  &lt;- secr.fit(ovenCH, model = D~session, mask = msk, \n     trace = FALSE, details = list(Dlambda = TRUE))\n predictDlambda (fit3)\n\n        estimate SE.estimate     lcl    ucl\nD1       0.92027     0.22763 0.57080 1.4837\nlambda1  1.04689     0.33132 0.57137 1.9182\nlambda2  1.18182     0.34965 0.66986 2.0851\nlambda3  0.73077     0.22567 0.40447 1.3203\nlambda4  0.84210     0.29413 0.43308 1.6374\n\n\nThe ovenbird population appeared to increase in density for two years and then decline for two years, but the effects are far from significant.\nModel selection procedures apply as usual:\n\n AIC(fit1, fit2, fit3)[,-c(2,5,6)]\n\n                      model npar  logLik  dAIC  AICwt\nfit1       D~1 g0~1 sigma~1    4 -930.73 0.000 0.5692\nfit2 D~Session g0~1 sigma~1    5 -930.07 0.680 0.4051\nfit3 D~session g0~1 sigma~1    8 -929.82 6.192 0.0257\n\n\nSession covariates are readily applied. The covariate for the second session predicts \\lambda_1 = D_2/D_1, for the third session predicts \\lambda_2 = D_3/D_2, etc. The covariate for the first session is discarded (remember D_1 is constant). This all may be confusing, but you can work it out, and it saves extra coding.\n\ncovs &lt;- data.frame(acov = c(0,2,1,1,2))  # fabricated covariate\nfit4  &lt;- secr.fit(ovenCH, model = D~acov, mask = msk, \n    trace = FALSE, details = list(Dlambda = TRUE), \n    sessioncov = covs)\n predictDlambda (fit4)\n\n        estimate SE.estimate     lcl    ucl\nD1        1.0281     0.21244 0.68858 1.5349\nlambda1   0.9501     0.21202 0.61677 1.4636\nlambda2   0.9303     0.14528 0.68627 1.2611\nlambda3   0.9303     0.14528 0.68627 1.2611\nlambda4   0.9501     0.21202 0.61677 1.4636",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>Trend revisited</span>"
    ]
  },
  {
    "objectID": "A10-trend.html#fixing-coefficients",
    "href": "A10-trend.html#fixing-coefficients",
    "title": "Appendix J — Trend revisited",
    "section": "\nJ.3 Fixing coefficients",
    "text": "J.3 Fixing coefficients\nAnother possibility is to fit the model with fixed trend (the second beta coefficient corresponds to lambda, before).\n\n fit5 &lt;- secr.fit(ovenCH, model = D~1, mask = msk, trace = FALSE,\n    details = list(Dlambda = TRUE, fixedbeta = \n    c(NA, log(0.9), NA, NA)))\n predictDlambda(fit5)\n\n        estimate SE.estimate     lcl    ucl\nD1        1.1154     0.15381 0.85231 1.4597\nlambda1   0.9000     0.00000 0.90000 0.9000\nlambda2   0.9000     0.00000 0.90000 0.9000\nlambda3   0.9000     0.00000 0.90000 0.9000\nlambda4   0.9000     0.00000 0.90000 0.9000",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>Trend revisited</span>"
    ]
  },
  {
    "objectID": "A10-trend.html#sec-trendreview",
    "href": "A10-trend.html#sec-trendreview",
    "title": "Appendix J — Trend revisited",
    "section": "\nJ.4 Technical notes and tips",
    "text": "J.4 Technical notes and tips\nDfn2 performs some tricky manipulations. You can see the code by typing secr:::Dfn2. A column is pre-pended to the density design matrix specifically to model the initial density; this takes the value one in Session 1 and is otherwise zero. Other columns in the design matrix are set to zero for the first session. Session-specific density on the link (log) scale is computed as the cumulative sum across sessions of the initial log density and the modelled log-lambda values.\nNote –\n\nThe model allows detector locations and habitat masks to vary between sessions.\nThe coding of Dfn2 relies on a log link function for density.\nDlambda is ignored for single-session data and conditional-likelihood (CL) models.\nThe method is not (yet) suitable for group models.\nThe default start values for D in secr.fit work well: all lambda are initially 1.0 (\\mathrm{log}(\\lambda_t) = 0 for all t).\nIf session covariates are used in any model, AICcompatible() expects the argument ‘sessioncov’ to be included in all models.\n\n\n\n\n\n\n\nTip\n\n\n\nD for session 1 is constant over space. It is not possible in the present version of secr to model simultaneous spatial variation in density or \\lambda, and using Dlambda with a density model that includes spatial covariates will cause an error.\n\n\n\n\n\n\n\n\n\n\nKendeigh, S. C. (1944). Measurement of bird populations. Ecological Monographs, 14, 67–106. https://doi.org/10.2307/1961632",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>Trend revisited</span>"
    ]
  },
  {
    "objectID": "A10-trend.html#footnotes",
    "href": "A10-trend.html#footnotes",
    "title": "Appendix J — Trend revisited",
    "section": "",
    "text": "Their indices are listed in component ‘D’ of the ‘parindx’ component of the fitted model (e.g. fit1$parindx$D), but you are unlikely to need this.↩︎",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>J</span>  <span class='chapter-title'>Trend revisited</span>"
    ]
  },
  {
    "objectID": "A11-expected-counts.html",
    "href": "A11-expected-counts.html",
    "title": "Appendix K — Expected counts",
    "section": "",
    "text": "K.1 Number of individuals n\nThe expected number of individuals detected at least once is E(n) = \\int [1 - \\exp\\{-\\Lambda(\\mathbf x) \\} ] \\times  D(\\mathbf x) \\; d \\mathbf x. This is the same for all detector types in which individuals are detected independently of each other (‘multi’, ‘binary proximity’ or ‘Poisson proximity’). Integration is over all locations in the plane from which an individual might be detected. The region of integration is represented in practice by a discretized ‘habitat mask’, and integration is performed by summing over cells.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Expected counts</span>"
    ]
  },
  {
    "objectID": "A11-expected-counts.html#number-of-detections-c",
    "href": "A11-expected-counts.html#number-of-detections-c",
    "title": "Appendix K — Expected counts",
    "section": "K.2 Number of detections C",
    "text": "K.2 Number of detections C\nThe total number of detections C depends on the detector type, as follows.\n\nK.2.1 Detector type ‘Poisson proximity’\nThis is the simplest case – E(C) = \\int \\Lambda(\\mathbf x) \\times D(\\mathbf x) \\; d\\mathbf x.\n\n\nK.2.2 Detector type ‘multi-catch trap’\nData from ‘multi’ detectors are binary at the level of each animal \\times occasion, with Bernoulli probability p_s = 1 - \\exp\\{- \\Lambda_s(\\mathbf x)\\}. This leads to the overall number of detections – E(C) = \\int \\sum_s p_s(\\mathbf x) \\times D(\\mathbf x) \\; d \\mathbf x.\n\n\nK.2.3 Detector type ‘binary proximity’\nData from binary proximity detectors are binary at the level of each animal \\times detector \\times occasion, with Bernoulli probability p_{ks}(\\mathbf x) = 1 - \\exp\\{- \\lambda(d_k(\\mathbf x))\\}. This leads to the overall number of detections – E(C) = \\int \\sum_s \\sum_k p_{ks}(\\mathbf x) \\times D(\\mathbf x) \\; d\\mathbf x.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Expected counts</span>"
    ]
  },
  {
    "objectID": "A11-expected-counts.html#number-of-recaptures-r",
    "href": "A11-expected-counts.html#number-of-recaptures-r",
    "title": "Appendix K — Expected counts",
    "section": "K.3 Number of recaptures r",
    "text": "K.3 Number of recaptures r\nFor all detector types the expected number of recaptures is simply E(r) = E(C) - E(n).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Expected counts</span>"
    ]
  },
  {
    "objectID": "A11-expected-counts.html#number-of-movements-m",
    "href": "A11-expected-counts.html#number-of-movements-m",
    "title": "Appendix K — Expected counts",
    "section": "K.4 Number of movements m",
    "text": "K.4 Number of movements m\nA movement is a recapture (redetection) at a site other than the previous one. Movements are a subset of recaptures. We calculate the expected number of movements by considering each recapture event in turn and calculating the conditional probability that it is at the same site as before. This is a sum of squared detector-wise conditional probabilities.\nConditional on detection somewhere, the probability of detection in detector k is q_k(\\mathbf x) = \\lambda(d_k(\\mathbf x)) / \\sum_k \\lambda(d_k(\\mathbf x)). For clarity in the following detector-specific expressions we use a(\\mathbf x) = 1 - \\exp\\{-\\Lambda(\\mathbf x)\\}) and b(\\mathbf x) = 1-\\sum_k q_k(\\mathbf x)^2.\n\nK.4.1 Detector type ‘Poisson proximity’\nE(m) = \\int \\{ \\Lambda(\\mathbf x) - a(\\mathbf x)\\} \\times b(\\mathbf x) \\times D(\\mathbf x) \\; d\\mathbf x.\n\n\nK.4.2 Detector type ‘multi-catch trap’ \nE(m) = \\int \\{\\sum_s p_s(\\mathbf x) - a(\\mathbf x)\\} \\times b(\\mathbf x) \\times D(\\mathbf x) \\; d\\mathbf x.\n\n\nK.4.3 Detector type ‘binary proximity’\nE(m) = \\int \\{ \\sum_s \\sum_k p_{ks}(\\mathbf x)  - a(\\mathbf x) \\} \\times b(\\mathbf x) \\times D(\\mathbf x) \\; d\\mathbf x.\n\n\nK.4.4 Caveat\nIf an animal may be detected more than once on one occasion (as with ‘proximity’ and ‘count’ detector types) and time of detection is not recorded within each occasion (the norm in secr) then the temporal sequence of detections is not fully observed. The number of observed (apparent) movements is then less than or equal to the true number. Results from the moves function in secr are also not to be trusted: they effectively assume any repeat detections at the same site precede other redetections rather than being interspersed in time. Precise formulae are not available for the expected number of observed movements among proximity and count detectors. There should be little discrepancy between observed and true numbers when detections are sparse. The predicted number of movements is close to the apparent number in simulations (see later section; this deserves further investigation).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Expected counts</span>"
    ]
  },
  {
    "objectID": "A11-expected-counts.html#individuals-detected-at-two-or-more-detectors-n_2",
    "href": "A11-expected-counts.html#individuals-detected-at-two-or-more-detectors-n_2",
    "title": "Appendix K — Expected counts",
    "section": "K.5 Individuals detected at two or more detectors n_2",
    "text": "K.5 Individuals detected at two or more detectors n_2\nThis count is related to the optimization criterion Q_{p_m} of Dupont et al. (2021). The value is simply the total count \\mathrm{E}(n) minus the number detected at only one detector \\mathrm{E}(n_1). For independent detectors (proximity detectors of any sort) the calculation follows from Dupont et al. (2021): setting p_0(\\mathbf x) = \\exp (-S\\Lambda(\\mathbf x)) and p_k(\\mathbf x) = \\exp\\{-S \\lambda[d_k(\\mathbf x)]\\},\nE(n_1) = \\int p_0(\\mathbf x) \\sum_k \\frac{p_k(\\mathbf x)}{1 - p_k(\\mathbf x)} \\times D(\\mathbf x) \\; d \\mathbf x. Then \\mathrm{E}(n_2) = \\mathrm{E}(n) - \\mathrm{E}(n_1).\nThe calculation of \\mathrm{E}(n_1) is more messy for non-independent detectors, specifically multi-catch traps. Using p_{ks}(\\mathbf x) = [1 - \\exp(-\\Lambda(\\mathbf x))] \\; \\lambda(d_k(\\mathbf x)) / \\Lambda(\\mathbf x) for the probability an individual at \\mathbf x is caught at k on a particular occasion, and p^*_{ks}(\\mathbf x) = [1 - \\exp(-\\Lambda(\\mathbf x))] \\; (1 - \\lambda(d_k(\\mathbf x)) / \\Lambda(\\mathbf x)) for the probability it is caught elsewhere:\nE(n_1) = \\int \\sum_k \\left( 1 - [1-p_{ks}(\\mathbf x)]^S \\right) \\;  [1-p^*_{ks}(\\mathbf x)]^{S-1} \\times D(\\mathbf x) \\; d \\mathbf x.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Expected counts</span>"
    ]
  },
  {
    "objectID": "A11-expected-counts.html#single-catch-traps",
    "href": "A11-expected-counts.html#single-catch-traps",
    "title": "Appendix K — Expected counts",
    "section": "K.6 Single-catch traps ",
    "text": "K.6 Single-catch traps \nAll the preceding calculations assume independence among animals. If traps can catch only one animal at a time then animals effectively compete for access (the first arrival is most likely to be caught). This depresses the realised hazard of detection \\lambda(d_k(\\mathbf x); \\theta); the effect increases with density. No closed-form expressions exist for this case. The computed \\mathrm{E}(n), \\mathrm{E}(r) and \\mathrm{E}(m) for multi-catch traps (detector ‘multi’) will exceed the true values for the single-catch traps (detector ‘single’) given the same detection parameters. That final caveat is significant because a pilot value of \\hat \\lambda_0 from fitting a multi-catch model to single-catch data will be an underestimate (Efford et al., 2009).\n\n\n\n\n\n\nDupont, G., Royle, J. A., Nawaz, M. A., & Sutherland, C. (2021). Optimal sampling design for spatial capture-recapture. Ecology, 102, e03262.\n\n\nEfford, M. G., Borchers, D. L., & Byrom, A. E. (2009). Density estimation by spatially explicit capture-recapture: Likelihood-based methods. In D. L. Thomson, E. G. Cooch, & M. J. Conroy (Eds.), Modeling demographic processes in marked populations (pp. 255–269). Springer.\n\n\nEfford, M. G., & Boulanger, J. (2019). Fast evaluation of study designs for spatially explicit capture-recapture. Methods in Ecology and Evolution, 10, 1529–1535. https://doi.org/10.1111/2041-210X.13239\n\n\nKendeigh, S. C. (1944). Measurement of bird populations. Ecological Monographs, 14, 67–106. https://doi.org/10.2307/1961632",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Expected counts</span>"
    ]
  },
  {
    "objectID": "A11-expected-counts.html#footnotes",
    "href": "A11-expected-counts.html#footnotes",
    "title": "Appendix K — Expected counts",
    "section": "",
    "text": "\\lambda(d_k(\\mathbf x)) = -\\log[1 - g(d_k(\\mathbf x))].↩︎",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>K</span>  <span class='chapter-title'>Expected counts</span>"
    ]
  },
  {
    "objectID": "A12-datasets.html",
    "href": "A12-datasets.html",
    "title": "Appendix L — Datasets",
    "section": "",
    "text": "These datasets are included in secr. See each linked help page for details. Code for model fitting is in secr-version4.pdf.\n\n\n\nTable L.1: Datasets included in secr\n\n\n\n\n\n\n\n\n\nDataset\nDescription\n\n\n\n\nblackbear \nUrsus americanus hair snag DNA data from Tennessee (Laufenberg et al., 2013; Settlage et al., 2008)\n\n\ndeermouse \nPeromyscus maniculatus live-trapping data of V. H. Reid published as a CAPTURE example by Otis et al. (1978)\n\n\nhornedlizard \nRepeated searches of a quadrat in Arizona for flat-tailed horned lizards Phrynosoma mcallii (Royle & Young, 2008)\n\n\nhousemouse \nMus musculus live-trapping data of H. N. Coulombe published as a CAPTURE example by Otis et al. (1978)\n\n\novenbird \nMulti-year mist-netting study of ovenbirds Seiurus aurocapilla at a site in Maryland, USA (Dawson & Efford, 2009)\n\n\novensong\nAcoustic detections of ovenbirds (Dawson & Efford, 2009)\n\n\nOVpossum \nBrushtail possum Trichosurus vulpecula live trapping in the Orongorongo Valley, Wellington, New Zealand 1996–1997 (Efford & Cowan, 2004)\n\n\npossum\nBrushtail possum Trichosurus vulpecula live trapping at Waitarere, North Island, New Zealand April 2002 (Efford et al., 2005)\n\n\ncaptdata\nSimulated data and some fitted models (secrdemo.0, secrdemo.CL)\n\n\nskink \nMulti-session lizard (Oligosoma infrapunctatum and O. lineoocellatum) pitfall trapping data from Lake Station, Upper Buller Valley, South Island, New Zealand (Efford et al. in prep)\n\n\nstoatDNA\nStoat Mustela erminea hair tube DNA data from Matakitaki Valley, South Island, New Zealand (Efford et al., 2009)\n\n\n\n\n\n\n\n\n\n\nDawson, D. K., & Efford, M. G. (2009). Bird population density estimated from acoustic signals. Journal of Applied Ecology, 46, 1201–1209.\n\n\nEfford, M. G., Borchers, D. L., & Byrom, A. E. (2009). Density estimation by spatially explicit capture-recapture: Likelihood-based methods. In D. L. Thomson, E. G. Cooch, & M. J. Conroy (Eds.), Modeling demographic processes in marked populations (pp. 255–269). Springer.\n\n\nEfford, M. G., & Cowan, P. E. (2004). Long-term population trend of trichosurus vulpecula in the orongorongo valley, new zealand. In R. L. Goldingay & S. M. Jackson (Eds.), The biology of australian possums and gliders (pp. 471–483). Surrey Beatty & Sons.\n\n\nEfford, M. G., Warburton, B., Coleman, M. C., & Barker, R. J. (2005). A field test of two methods for density estimation. Wildlife Society Bulletin, 33, 731–738.\n\n\nKendeigh, S. C. (1944). Measurement of bird populations. Ecological Monographs, 14, 67–106. https://doi.org/10.2307/1961632\n\n\nLaufenberg, J. S., Van Manen, F. T., & Clark, J. D. (2013). Effects of sampling conditions on DNA-based estimates of american black bear abundance: Sampling for bear population studies. Journal of Wildlife Management, 77, 1010–1020. https://doi.org/10.1002/jwmg.534\n\n\nOtis, D. L., Burnham, K. P., White, G. C., & Anderson, D. R. (1978). Statistical inference from capture data on closed animal populations. Wildlife Monographs, 62.\n\n\nRoyle, J. A., & Young, K. V. (2008). A hierarchical model for spatial capture-recapture data. Ecology, 89, 2281–2289.\n\n\nSettlage, K. E., Van Manen, F. T., Clark, J. D., & King, T. L. (2008). Challenges of DNA‐based mark‐-recapture studies of american black bears. Journal of Wildlife Management, 72, 1035–1042. https://doi.org/10.2193/2006-472",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>L</span>  <span class='chapter-title'>Datasets</span>"
    ]
  },
  {
    "objectID": "A99-references.html",
    "href": "A99-references.html",
    "title": "References",
    "section": "",
    "text": "Akima, H., & Gebhardt, A. (2022). akima: Interpolation of irregularly and regularly\nspaced data. https://CRAN.R-project.org/package=akima\n\n\nAllaire, J., Francois, R., Ushey, K., Vandenbrouck, G., Geelnard, M.,\n& Intel. (2023). RcppParallel: Parallel programming tools for\n’rcpp’. https://CRAN.R-project.org/package=RcppParallel\n\n\nAugustine, B. C., Royle, J. A., Kelly, M. J., Satter, C. B., Alonso, R.\nS., Boydston, E. E., & Crooks, K. R. (2018). Spatial\ncapture–recapture with partial identity: An application to camera traps.\nThe Annals of Applied Statistics, 12(1). https://doi.org/10.1214/17-aoas1091\n\n\nAugustine, B. C., Royle, J. A., Linden, D. W., & Fuller, A. K.\n(2020). Spatial proximity moderates genotype uncertainty in genetic\ntagging studies. Proceedings of the National Academy of\nSciences, 117(30), 17903–17912. https://doi.org/10.1073/pnas.2000247117\n\n\nBaddeley, A., Rubak, E., & Turner, R. (2015). Spatial point\npatterns. Chapman; Hall/CRC. https://doi.org/10.1201/b19708\n\n\nBischof, R., Dupont, P., Milleret, C., Chipperfield, J., & Royle, J.\nA. (2020). Consequences of ignoring group association in spatial\ncapture–recapture analysis. Wildlife Biology, 2020(1),\n1–10. https://doi.org/10.2981/wlb.00649\n\n\nBorchers, D. L., Buckland, S. T., & Zucchini, W. (2002). Estimating\nanimal abundance. In Statistics for Biology and Health.\nSpringer London. https://doi.org/10.1007/978-1-4471-3708-5\n\n\nBorchers, D. L., & Efford, M. G. (2007). Supplements to\nbiometrics paper. https://www.otago.ac.nz/density\n\n\nBorchers, D. L., & Efford, M. G. (2008). Spatially explicit maximum\nlikelihood methods for capture-recapture studies. Biometrics,\n64, 377–385.\n\n\nBorchers, D. L., & Fewster, R. (2016). Spatial capture–recapture\nmodels. Statistical Science, 31(2). https://doi.org/10.1214/16-sts557\n\n\nBorchers, D. L., & Kidney, D. (2014). Flexible density surface\nestimation for spatially explicit capture–recapture surveys.\nUniversity of St Andrews. https://research-repository.st-andrews.ac.uk/bitstream/handle/10023/9147/secrgam_techrep.pdf?sequence=1\n\n\nBorchers, D. L., Stevenson, B. C., Kidney, D., Thomas, L., &\nMarques, T. A. (2015). A unifying model for capture–recapture and\ndistance sampling surveys of wildlife populations. Journal of the\nAmerican Statistical Association, 110(509), 195–204. https://doi.org/10.1080/01621459.2014.893884\n\n\nBoutin, S. (1984). Home range size and methods of estimating snowshoe\nhare densities. Acta Zoologica Fennica, 171, 275–278.\n\n\nBoyce, M. S., Vernier, P. R., Nielsen, S. E., & Schmiegelow, F. K.\nA. (2002). Evaluating resource selection functions. Ecological\nModelling, 157, 281–300.\n\n\nBrooks, C., S. P., & Morgan, B. J. T. (2000). Bayesian animal\nsurvival estimation. Statistical Science, 15, 357–376.\n\n\nBrooks, S. P., Morgan, B. J. T., Ridout, M. S., & Pack, S. E.\n(1997). Finite mixture models for proportions. Biometrics,\n53(3), 1097. https://doi.org/10.2307/2533567\n\n\nBuckland, S. T., Anderson, D. R., Burnham, K. P., Laake, J. L.,\nBorchers, D. L., & Thomas, L. (2001). Introduction to distance\nsampling. Oxford University Press.\n\n\nBuckland, S. T., Burnham, K. P., & Augustin, N. H. (1997). Model\nselection: An integral part of inference. Biometrics,\n53(2), 603. https://doi.org/10.2307/2533961\n\n\nBuckland, S. T., Rexstad, E. A., Marques, T. A., & Oedekoven, C. S.\n(2015). Distance sampling: Methods and applications. Springer.\n\n\nBurnham, K. P., & Anderson, D. R. (2002). Model selection and\nmultimodel inference: A practical information-theoretic approach.\nSpringer.\n\n\nBurnham, K. P., Anderson, D. R., White, G. C., Brownie, C., &\nPollock, K. H. (1987). Design and analysis methods for fish survival\nexperiments based on release-recapture. American Fisheries Society.\n\n\nCalhoun, J. B., & Casby, J. U. (1958). Calculation of home range and\ndensity of small mammals. Public Health Monograph, 55.\n\n\nChandler, R. B., Crawford, D. A., Garrison, E. P., Miller, K. V., &\nCherry, M. J. (2021). Modeling abundance, distribution, movement and\nspace use with camera and telemetry data. Ecology,\n103(10). https://doi.org/10.1002/ecy.3583\n\n\nChandler, R. B., & Royle, J. A. (2013). Spatially explicit models\nfor inference about density in unmarked or partially marked populations.\nAnnals of Applied Statistics, 7, 936–954.\n\n\nChandler, R., & Hepinstall-Cymerman, J. (2016). Estimating the\nspatial scales of landscape effects on abundance. Landscape\nEcology, 31, 1383–1394. https://doi.org/10.1007/s10980-016-0380-z\n\n\nChao, A. (1989). Estimating population size for sparse data in\ncapture-recapture experiments. Biometrics, 45(2), 427.\nhttps://doi.org/10.2307/2531487\n\n\nChoo, Y. R., Sutherland, C., & Johnston, A. (2024). A monte carlo\nresampling framework for implementing goodness‐of‐fit tests in spatial\ncapture‐recapture models. Methods in Ecology and Evolution. https://doi.org/10.1111/2041-210x.14386\n\n\nClark, J. D. (2019). Comparing clustered sampling designs for spatially\nexplicit estimation of population density. Population Ecology,\n61, 93–101.\n\n\nCochran, W. G. (1977). Sampling techniques. (3rd ed.). John\nWiley; Sons.\n\n\nCooch, E., & White, G. (Eds.). (2023). Program MARK: A gentle\nintroduction. (23rd ed.). http://www.phidot.org/software/mark/docs/book/\n\n\nCsardi, G., & Nepusz, T. (2006). The igraph software package for\ncomplex network research. InterJournal, 1695. http://igraph.org\n\n\nDawson, D. K., & Efford, M. G. (2009). Bird population density\nestimated from acoustic signals. Journal of Applied Ecology,\n46, 1201–1209.\n\n\nDesprés‐Einspenner, M., Howe, E. J., Drapeau, P., & Kühl, H. S.\n(2017). An empirical evaluation of camera trapping and spatially\nexplicit capture‐recapture models for estimating chimpanzee density.\nAmerican Journal of Primatology, 79(7). https://doi.org/10.1002/ajp.22647\n\n\nDey, S., Moqanaki, E., Milleret, C., Dupont, P., Tourani, M., &\nBischof, R. (2023). Modelling spatially autocorrelated detection\nprobabilities in spatial capture-recapture using random effects.\nEcological Modelling, 479, 110324. https://doi.org/10.1016/j.ecolmodel.2023.110324\n\n\nDistiller, G., & Borchers, D. L. (2015). A spatially explicit\ncapture–recapture estimator for single‐catch traps. Ecology and\nEvolution, 5(21), 5075–5087. https://doi.org/10.1002/ece3.1748\n\n\nDoherty, P. F., White, G. C., & Burnham, K. P. (2010). Comparison of\nmodel building and selection strategies. Journal of\nOrnithology, 152(S2), 317–323. https://doi.org/10.1007/s10336-010-0598-5\n\n\nDorazio, R. M. (2013). Bayes and empirical bayes estimators of abundance\nand density from spatial capture-recapture data. PLoS ONE,\n8, e84017.\n\n\nDumelle, M., Kincaid, T. M., Olsen, A. R., & Weber, M. H. (2024).\nSpsurvey: Spatial sampling design and analysis. In R package version\n5.5.1. https://CRAN.R-project.org/package=spsurvey\n\n\nDumelle, M., Kincaid, T., Olsen, A. R., & Weber, M. (2023).\nSpsurvey: Spatial sampling design and analysis in r. Journal of\nStatistical Software, 105. https://doi.org/10.18637/jss.v105.i03\n\n\nDunn, J. E., & Gipson, P. S. (1977). Analysis of radio telemetry\ndata in studies of home range. Biometrics, 33(1), 85.\nhttps://doi.org/10.2307/2529305\n\n\nDupont, G., Royle, J. A., Nawaz, M. A., & Sutherland, C. (2021).\nOptimal sampling design for spatial capture-recapture. Ecology,\n102, e03262.\n\n\nDupont, P., Milleret, C., Gimenez, O., & Bischof, R. (2019).\nPopulation closure and the bias‐precision trade‐off in spatial\ncapture–recapture. Methods in Ecology and Evolution,\n10(5), 661–672. https://doi.org/10.1111/2041-210x.13158\n\n\nDurbach, I., Borchers, D., Sutherland, C., & Sharma, K. (2021).\nFast, flexible alternatives to regular grid designs for spatial\ncapture-recapture. Methods in Ecology and Evolution,\n12(298-310), 2041–2210. https://doi.org/10.1111/2041-210X.13517\n\n\nDurbach, I., Chopara, R., Borchers, D. L., Phillip, R., Sharma, K.,\n& Stevenson, B. C. (2024). That’s not the mona lisa! How to\ninterpret spatial capture-recapture density surface estimates.\nBiometrics, 80. https://doi.org/10.1093/biomtc/ujad020\n\n\nEddelbuettel, D., Francois, R., Allaire, J., Ushey, K., Kou, Q.,\nRussell, N., Ucar, I., Bates, D., & Chambers, J. (2023). Rcpp:\nSeamless r and c++ integration. https://CRAN.R-project.org/package=Rcpp\n\n\nEdzer, E., & Bivand, R. (2023). Spatial\nData Science: With applications in R (p. 352). Chapman;\nHall/CRC. https://doi.org/10.1201/9780429459016\n\n\nEfford, M. G. (2004). Density estimation in live-trapping studies.\nOikos, 106, 598–610.\n\n\nEfford, M. G. (2011). Estimation of population density by spatially\nexplicit capture-recapture analysis of data from area searches.\nEcology, 92, 2202–2207.\n\n\nEfford, M. G. (2012). DENSITY 5.0: Software for spatially explicit\ncapture-recapture. Department of Mathematics; Statistics,\nUniversity of Otago. https://www.otago.ac.nz/density\n\n\nEfford, M. G. (2014). Bias from heterogeneous usage of space in\nspatially explicit capture-recapture analyses. Methods in Ecology\nand Evolution, 5, 599–602.\n\n\nEfford, M. G. (2019). Non‐circular home ranges and the estimation of\npopulation density. Ecology, 100(2). https://doi.org/10.1002/ecy.2580\n\n\nEfford, M. G. (2023a). ipsecr: An R package for\nawkward spatial capture–recapture data. Methods in Ecology\nand Evolution, 14(5), 1182–1189. https://doi.org/10.1111/2041-210x.14088\n\n\nEfford, M. G. (2023b). secrdesign: Sampling\nDesign for Spatially Explicit Capture-Recapture. https://CRAN.R-project.org/package=secrdesign\n\n\nEfford, M. G. (2024). secrlinear: Spatially\nExplicit Capture-Recapture for Linear Habitats. https://CRAN.R-project.org/package=secrlinear\n\n\nEfford, M. G. (2025a). secr: Spatially explicit\ncapture-recapture models. https://CRAN.R-project.org/package=secr\n\n\nEfford, M. G. (2025b). Spatially explicit capture–recapture models for\nrelative density. BioRxiv. https://doi.org/10.1101/2025.01.22.634401\n\n\nEfford, M. G., Borchers, D. L., & Byrom, A. E. (2009). Density\nestimation by spatially explicit capture-recapture: Likelihood-based\nmethods. In D. L. Thomson, E. G. Cooch, & M. J. Conroy (Eds.),\nModeling demographic processes in marked populations (pp.\n255–269). Springer.\n\n\nEfford, M. G., Borchers, D. L., & Mowat, G. (2013). Varying effort\nin capture-recapture studies. Methods in Ecology and Evolution,\n4, 629–636.\n\n\nEfford, M. G., & Boulanger, J. (2019). Fast evaluation of study\ndesigns for spatially explicit capture-recapture. Methods in Ecology\nand Evolution, 10, 1529–1535. https://doi.org/10.1111/2041-210X.13239\n\n\nEfford, M. G., & Cowan, P. E. (2004). Long-term population trend of\ntrichosurus vulpecula in the\norongorongo valley, new zealand. In R. L. Goldingay & S. M. Jackson\n(Eds.), The biology of australian possums and gliders (pp.\n471–483). Surrey Beatty & Sons.\n\n\nEfford, M. G., Dawson, D. K., & Borchers, D. L. (2009). Population\ndensity estimated from locations of individuals on a passive detector\narray. Ecology, 90, 2676–2682.\n\n\nEfford, M. G., Dawson, D. K., Jhala, Y. V., & Qureshi, Q. (2016).\nDensity-dependent home-range size revealed by spatially explicit\ncapture-recapture. Ecography, 39, 676–688.\n\n\nEfford, M. G., Dawson, D. K., & Robbins, C. S. (2004). DENSITY:\nSoftware for analysing capture-recapture data from passive detector\narrays. Animal Biodiversity and Conservation, 27,\n217–228.\n\n\nEfford, M. G., & Fewster, R. M. (2013). Estimating population size\nby spatially explicit capture-recapture. Oikos, 122,\n918–928.\n\n\nEfford, M. G., & Fletcher, D. (2024). Effect of spatial\noverdispersion on confidence intervals for population density estimated\nby spatial capture-recapture. bioRxiv. https://doi.org/10.1101/2024.03.12.584742\n\n\nEfford, M. G., & Hunter, C. M. (2018). Spatial capture-mark-resight\nestimation of animal population density. Biometrics,\n74, 411–420. https://doi.org/10.1111/biom.12766\n\n\nEfford, M. G., & Mowat, G. (2014). Compensatory heterogeneity in\nspatially explicit capture-recapture data. Ecology,\n95, 1341–1348.\n\n\nEfford, M. G., & Schofield, M. R. (2020). A spatial open-population\ncapture-recapture model. Biometrics, 76, 392–402.\n\n\nEfford, M. G., Warburton, B., Coleman, M. C., & Barker, R. J.\n(2005). A field test of two methods for density estimation. Wildlife\nSociety Bulletin, 33, 731–738.\n\n\nErgon, T., & Gardner, B. (2013). Separating mortality and\nemigration: Modelling space use, dispersal and survival with\nrobust‐design spatial capture–recapture data. Methods in Ecology and\nEvolution, 5(12), 1327–1336. https://doi.org/10.1111/2041-210x.12133\n\n\nEvans, M. A., Kim, H.-M., & O’Brien, T. E. (1996). An application of\nprofile-likelihood based confidence interval to capture: Recapture\nestimators. Journal of Agricultural, Biological, and Environmental\nStatistics, 1(1), 131. https://doi.org/10.2307/1400565\n\n\nFewster, R. M., & Buckland, S. T. (2004). Assessment of distance\nsampling estimators. In S. T. Buckland, D. R. Anderson, K. P. Burnham,\nJ. L. Laake, D. L. Borchers, & L. Thomas (Eds.), Advanced\ndistance sampling (pp. 281–306). Oxford University Press.\n\n\nFleming, J., Grant, E. H. C., Sterrett, S. C., & Sutherland, C.\n(2021). Experimental evaluation of spatial capture–recapture study\ndesign. Ecological Applications, 31(7). https://doi.org/10.1002/eap.2419\n\n\nFletcher, D. (2019). Model averaging. Springer.\n\n\nGardner, B., Royle, J. A., & Wegan, M. T. (2009). Hierarchical\nmodels for estimating density from DNA mark-recapture studies.\nEcology, 90, 1106–1115.\n\n\nGelman, M., A., & Stern, H. (1996). Posterior predictive assessment\nof model fitness via realized disrepancies. Statistica Sinica,\n6, 733–807.\n\n\nGerber, B. D., & Parmenter, R. R. (2015). Spatial capture–recapture\nmodel performance with known small‐mammal densities. Ecological\nApplications, 25(3), 695–705. https://doi.org/10.1890/14-0960.1\n\n\nGimenez, O., Viallefont, A., Catchpole, E. A., Choquet, R., &\nMorgan, B. J. T. (2004). Methods for investigating parameter redundancy.\nAnimal Biodiversity and Conservation, 27, 561–572.\n\n\nGopalaswamy, A. M., Royle, J. A., Delampady, M., Nichols, J. D.,\nKaranth, K. U., & Macdonald, D. W. (2012). Density estimation in\ntiger populations: Combining information for strong inference.\nEcology, 93(7), 1741–1751. https://doi.org/10.1890/11-2110.1\n\n\nHahsler, M., & Hornik, K. (2007). TSP- infrastructure for the\ntraveling salesperson problem. Journal of Statistical Software,\n23(2). https://doi.org/10.18637/jss.v023.i02\n\n\nHarihar, A., Chanchani, P., Pariwakam, M., Noon, B. R., & Goodrich,\nJ. (2017). Defensible inference: Questioning global trends in tiger\npopulations. Conservation Letters, 10(5), 502–505. https://doi.org/10.1111/conl.12406\n\n\nHayes, R. J., & Buckland, S. T. (1983). Radial-distance models for\nthe line-transect method. Biometrics, 39, 29–42.\n\n\nHijmans, R. J. (2023a). Raster: Geographic data analysis and modeling.\nIn R package version 3.6-26. https://CRAN.R-project.org/package=raster\n\n\nHijmans, R. J. (2023b). Terra: Spatial data analysis. https://CRAN.R-project.org/package=terra\n\n\nHooten, M. B., Johnson, D. S., McClintock, B. T., & Morales, J. M.\n(2017). Animal movement: Statistical models for telemetry data.\nCRC Press. https://doi.org/10.1201/9781315117744\n\n\nHooten, M. B., Schwob, M. R., Johnson, D. S., & Ivan, J. S. (2024).\nGeostatistical capture–recapture models. Spatial Statistics,\n59, 100817. https://doi.org/10.1016/j.spasta.2024.100817\n\n\nHuggins, R. M. (1989). On the statistical analysis of capture\nexperiments. Biometrika, 76, 133–140.\n\n\nHurvich, C. M., & Tsai, C.-L. (1989). Regression and time series\nmodel selection in small samples. Biometrika, 76(2),\n297–307. https://doi.org/10.1093/biomet/76.2.297\n\n\nHuxley, J. S. (1934). A natural experiment on the territorial instinct.\nBritish Birds, 27, 270–277.\n\n\nIllian, J., Penttinen, A., Stoyan, H., & Stoyan, D. (2008).\nStatistical analysis and modelling of spatial point patterns.\nWiley.\n\n\nIvan, J. S., White, G. C., & Shenk, T. M. (2013). Using auxiliary\ntelemetry information to estimate animal density from capture–recapture\ndata. Ecology, 94(4), 809–816. https://doi.org/10.1890/12-0101.1\n\n\nJackson, H. B., & Fahrig, L. (2014). Are ecologists conducting\nresearch at the optimal scale? Global Ecology and Biogeography,\n24, 52–63. https://doi.org/10.1111/geb.12233\n\n\nJennrich, R. I., & Turner, F. B. (1969). Measurement of non-circular\nhome range. Journal of Theoretical Biology, 22,\n227–237. https://doi.org/10.1016/0022-5193(69)90002-2\n\n\nJohnson, D. H. (1980). The comparison of usage and availability\nmeasurements for evaluating resource preference. Ecology,\n61, 65–71.\n\n\nJohnson, D. S., Laake, J. L., & Ver Hoef, J. M. (2010). A\nmodel‐based approach for making ecological inference from distance\nsampling data. Biometrics, 66(1), 310–318. https://doi.org/10.1111/j.1541-0420.2009.01265.x\n\n\nJohnson, D. S., London, J. M., Lea, M.-A., & Durban, J. W. (2008).\nContinuous-time correlated random walk model for animal telemetry data.\nEcology, 89(5), 1208–1215. https://doi.org/10.1890/07-1032.1\n\n\nKaranth, K. U., & Nichols, J. D. (1998). Estimation of tiger\ndensities in india using photographic captures and recaptures.\nEcology, 79, 2852–2862.\n\n\nKendall, W. L. (1999). Robustness of closed capture–recapture methods to\nviolations of the closure assumption. Ecology, 80(8),\n2517–2525. https://doi.org/10.1890/0012-9658(1999)080[2517:roccrm]2.0.co;2\n\n\nKendeigh, S. C. (1944). Measurement of bird populations. Ecological\nMonographs, 14, 67–106. https://doi.org/10.2307/1961632\n\n\nKéry, M., & Royle, J. A. (2020). Applied hierarchical modeling\nin ecology. Volume 2. Elsevier Science.\n\n\nKing, R., McClintock, B. T., Kidney, D., & Borchers, D. (2016).\nCapture–recapture abundance estimation using a semi-complete data\nlikelihood approach. The Annals of Applied Statistics,\n10, 264–285. https://doi.org/10.1214/15-aoas890\n\n\nKodi, A. R., Howard, J., Borchers, D. L., Worthington, H., Alexander, J.\nS., Lkhagvajav, P., Bayandonoi, G., Ochirjav, M., Erdenebaatar, S.,\nByambasuren, C., Battulga, N., Johansson, Ö., & Sharma, K. (2024).\nGhostbusting - reducing bias due to identification errors in spatial\ncapture‐recapture histories. Methods in Ecology and Evolution.\nhttps://doi.org/10.1111/2041-210x.14326\n\n\nLaake, J. L., & Collier, B. A. (2024). Understanding implications of\ndetection heterogeneity in wildlife abundance estimation. Journal of\nWildlife Management, 88, e22516. https://doi.org/10.1002/jwmg.22516\n\n\nLampa, S., Henle, K., Klenke, R., Hoehn, M., & Gruber, B. (2013).\nHow to overcome genotyping errors in non-invasive genetic mark-recapture\npopulation size estimation-a review of available methods illustrated by\na case study: Genotyping errors in CMR-a review. The Journal of\nWildlife Management, 77(8), 1490–1511. https://doi.org/10.1002/jwmg.604\n\n\nLaufenberg, J. S., Van Manen, F. T., & Clark, J. D. (2013). Effects\nof sampling conditions on DNA-based estimates of american black bear\nabundance: Sampling for bear population studies. Journal of Wildlife\nManagement, 77, 1010–1020. https://doi.org/10.1002/jwmg.534\n\n\nLinden, D. W., Sirén, A. P. K., & Pekins, P. J. (2018). Integrating\ntelemetry data into spatial capture–recapture modifies inferences on\nmulti‐scale resource selection. Ecosphere, 9(4). https://doi.org/10.1002/ecs2.2203\n\n\nLópez-Bao, J. V., Godinho, R., Pacheco, C., Lema, F. J., García, E.,\nLlaneza, L., Palacios, V., & Jiménez, J. (2018). Toward reliable\npopulation estimates of wolves by combining spatial capture-recapture\nmodels and non-invasive DNA monitoring. Scientific Reports,\n8(1). https://doi.org/10.1038/s41598-018-20675-9\n\n\nLukacs, P. M., & Burnham, K. P. (2005). Estimating population size\nfrom DNA-based closed capture-recapture data incorporating genotyping\nerror. Journal of Wildlife Management, 69, 396–403.\n\n\nMargenau, L. L. S., Cherry, M. J., Miller, K. V., Garrison, E. P., &\nChandler, R. B. (2022). Monitoring partially marked populations using\ncamera and telemetry data. Ecological Applications,\n32(4). https://doi.org/10.1002/eap.2553\n\n\nMarques, T. A., Thomas, L., & Royle, J. A. (2011). A hierarchical\nmodel for spatial capture-recapture data: comment. Ecology,\n92, 526–528.\n\n\nMatechou, E., Morgan, B. J. T., Pledger, S., Collazo, J. A., &\nLyons, J. E. (2013). Integrated analysis of capture-recapture-resighting\ndata and counts of unmarked birds at stopover sites. Journal of\nAgricultural, Biological and Environmental Statistics, 18,\n120–135.\n\n\nMcClintock, B. T., & White, G. C. (2012). From NOREMARK to MARK:\nSoftware for estimating demographic parameters using mark-resight\nmethodology. Journal of Ornithology, Supplement 2,\n152, S641–S650.\n\n\nMcCrea, R. S., & Morgan, B. J. T. (2011). Multistate mark-recapture\nmodel selection using score tests. Biometrics, 67,\n234–241. https://doi.org/10.1111/j.1541-0420.2010.01421.x\n\n\nMcLaughlin, P., & Bar, H. (2020). A spatial capture–recapture model\nwith attractions between individuals. Environmetrics,\n32(1). https://doi.org/10.1002/env.2653\n\n\nMcLellan, B. A., Howe, E., Marrotte, R. R., & Northrup, J. M.\n(2023). Accounting for heterogeneous density and detectability in\nspatially explicit capture–recapture studies of carnivores.\nEcosphere, 14(10). https://doi.org/10.1002/ecs2.4669\n\n\nMilleret, C., Dupont, P., Brøseth, H., Kindberg, J., Royle, J. A., &\nBischof, R. (2018). Using partial aggregation in spatial capture\nrecapture. Methods in Ecology and Evolution, 9(8),\n1896–1907. https://doi.org/10.1111/2041-210x.13030\n\n\nMills, L. S., Ciotta, J. C., Lair, K. P., Schwartz, M. K., &\nTallmon, D. A. (2000). Estimating animal abundance using noninvasive\ngenetic sampling: Promise and pitfalls. Ecological\nApplications, 10, 283–294.\n\n\nMoqanaki, E. M., Milleret, C., Tourani, M., Dupont, P., & Bischof,\nR. (2021). Consequences of ignoring variable and spatially\nautocorrelated detection probability in spatial capture-recapture.\nLandscape Ecology, 36(10), 2879–2895. https://doi.org/10.1007/s10980-021-01283-x\n\n\nMowat, G., & Strobeck, C. (2000). Estimating population size of\ngrizzly bears using hair capture, DNA profiling, and mark-recapture\nanalysis. Journal of Wildlife Management, 64, 183–193.\n\n\nMurphy, S. M., Cox, J. J., Augustine, B. C., Hast, J. T., Guthrie, J.\nM., Wright, J., McDermott, J., Maehr, S. C., & Plaxico, J. H.\n(2016). Characterizing recolonization by a reintroduced bear population\nusing genetic spatial capture-recapture: Recolonization by reintroduced\nbear population. The Journal of Wildlife Management,\n80(8), 1390–1407. https://doi.org/10.1002/jwmg.21144\n\n\nMurphy, S. M., & Luja, V. H. (2025). Jaguar density estimation in\nmexico: The conservation importance of considering home range\norientation in spatial capture–recapture. Conservation Science and\nPractice. https://doi.org/10.1111/csp2.13301\n\n\nNoss, A. J., Gardner, B., Maffei, L., Cuéllar, E., Montaño, R.,\nRomero‐Muñoz, A., Sollman, R., & O’Connell, A. F. (2012). Comparison\nof density estimation methods for mammal populations with camera traps\nin the &lt;scp&gt;k&lt;/scp&gt;aa‐&lt;scp&gt;i&lt;/scp&gt;ya del\n&lt;scp&gt;g&lt;/scp&gt;ran &lt;scp&gt;c&lt;/scp&gt;haco landscape.\nAnimal Conservation, 15(5), 527–535. https://doi.org/10.1111/j.1469-1795.2012.00545.x\n\n\nOtis, D. L., Burnham, K. P., White, G. C., & Anderson, D. R. (1978).\nStatistical inference from capture data on closed animal populations.\nWildlife Monographs, 62.\n\n\nOtis, D. L., & White, G. C. (1999). Autocorrelation of location\nestimates and the analysis of radiotracking data. The Journal of\nWildlife Management, 63(3), 1039. https://doi.org/10.2307/3802819\n\n\nPaetkau, D. (2003). An empirical exploration of data quality in\nDNA‐based population inventories. Molecular Ecology,\n12(6), 1375–1387. https://doi.org/10.1046/j.1365-294x.2003.01820.x\n\n\nPalmero, S., Premier, J., Kramer‐Schadt, S., Monterroso, P., &\nHeurich, M. (2023). Sampling variables and their thresholds for the\nprecise estimation of wild felid population density with camera traps\nand spatial capture–recapture methods. Mammal Review,\n53(4), 223–237. https://doi.org/10.1111/mam.12320\n\n\nParmenter, R., Yates, T., Anderson, D., Burnham, K., Dunnum, J.,\nFranklin, A., Friggens, M., Lubow, B., Miller, M., Olson, G., Parmenter,\nC., Pollard, J., Rexstad, E., Shenk, T., Stanley, T., & White, G.\n(2003). Small-mammal density estimation: A field comparison of\ngrid-based vs web-based density estimators. Ecological\nMonographs, 73(1), 1–26.\n\n\nPaterson, J. T., Proffitt, K., Jimenez, B., Rotella, J., & Garrott,\nR. (2019). Simulation-based validation of spatial capture-recapture\nmodels: A case study using mountain lions. PLOS ONE,\n14(4), e0215458. https://doi.org/10.1371/journal.pone.0215458\n\n\nPebesma, E. J. (2018). Simple Features for R:\nStandardized Support for Spatial Vector Data. The R\nJournal, 10(1), 439–446. https://doi.org/10.32614/RJ-2018-009\n\n\nPebesma, E. J., & Bivand, R. (2005). Classes and methods for spatial\ndata in R. R News, 5(2), 9–13. https://CRAN.R-project.org/doc/Rnews/\n\n\nPledger, S. (2000). Unified maximum likelihood estimates for closed\ncapture-recapture models using mixtures. Biometrics,\n56, 434–442.\n\n\nQiu, Y., Balan, S., Beall, M., Sauder, M., Okazaki, N., & Hahn, T.\n(2023). RcppNumerical: ’Rcpp’ integration for numerical computing\nlibraries. https://CRAN.R-project.org/package=RcppNumerical\n\n\nR Core Team. (2024). R: A language and environment for statistical\ncomputing. R Foundation for Statistical Computing. https://www.R-project.org/\n\n\nReich, B. J., & Gardner, B. (2014). A spatial capture‐recapture\nmodel for territorial species. Environmetrics, 25(8),\n630–637. https://doi.org/10.1002/env.2317\n\n\nRich, L. N., Kelly, M. J., Sollmann, R., Noss, A. J., Maffei, L.,\nArispe, R. L., Paviolo, A., De Angelo, C. D., Blanco, D., E., Y., &\nDi Bitetti, M. S. (2014). Comparing capture-recapture, mark-resight, and\nspatial mark-resight models for estimating puma densities via camera\ntraps. Journal of Mammalogy, 95, 382–391.\n\n\nRobertson, B., McDonald, T., Price, C., & Brown, J. (2018). Halton\niterative partitioning: Spatially balanced sampling via partitioning.\nEnvironmental and Ecological Statistics, 25(3),\n305–323. https://doi.org/10.1007/s10651-018-0406-6\n\n\nRoyle, J. A., Chandler, R. B., Gazenski, K. D., & Graves, T. A.\n(2013). Spatial capture-recapture models for jointly estimating\npopulation density and landscape connectivity. Ecology,\n94, 287–294.\n\n\nRoyle, J. A., Chandler, R. B., Sollmann, R., & Gardner, B. (2014).\nSpatial capture-recapture. Academic Press.\n\n\nRoyle, J. A., Chandler, R. B., Sun, C. C., & Fuller, A. K. (2013).\nIntegrating resource selection information with spatial\ncapture–recapture. Methods in Ecology and Evolution,\n4(6), 520–530. https://doi.org/10.1111/2041-210x.12039\n\n\nRoyle, J. A., Fuller, A. K., & Sutherland, C. (2015). Spatial\ncapture–recapture models allowing markovian transience or dispersal.\nPopulation Ecology, 58(1), 53–62. https://doi.org/10.1007/s10144-015-0524-z\n\n\nRoyle, J. A., & Gardner, B. (2011). Hierarchical spatial\ncapture-recapture models for estimating density from trapping arrays. In\nA. F. O’Connell, J. D. Nichols, & K. U. Karanth (Eds.), Camera\ntraps in animal ecology: Methods and analyses (pp. 163–190).\nSpringer.\n\n\nRoyle, J. A., Karanth, K. U., Gopalaswamy, A. M., & Kumar, N. S.\n(2009). Bayesian inference in camera trapping studies for a class of\nspatial capture–recapture models. Ecology, 90(11),\n3233–3244. https://doi.org/10.1890/08-1481.1\n\n\nRoyle, J. A., Magoun, A. J., Gardner, B., Valkenburg, P., & Lowell,\nR. E. (2011). Density estimation in a wolverine population using spatial\ncapture-recapture models. Journal of Wildlife Management,\n75, 604–611.\n\n\nRoyle, J. A., Nichols, J. D., Karanth, K. U., & Gopalaswamy, A. M.\n(2009). A hierarchical model for estimating density in camera‐trap\nstudies. Journal of Applied Ecology, 46(1), 118–127.\nhttps://doi.org/10.1111/j.1365-2664.2008.01578.x\n\n\nRoyle, J. A., & Young, K. V. (2008). A hierarchical model for\nspatial capture-recapture data. Ecology, 89,\n2281–2289.\n\n\nRuprecht, J. S., Eriksson, C. E., Forrester, T. D., Clark, D. A.,\nWisdom, M. J., Rowland, M. M., Johnson, B. K., & Levi, T. (2021).\nEvaluating and integrating spatial capture–recapture models with data of\nvariable individual identifiability. Ecological Applications,\n31(7). https://doi.org/10.1002/eap.2405\n\n\nRussell, R. E., Royle, J. A., Desimone, R., Schwartz, M. K., Edwards, V.\nL., Pilgrim, K. P., & Mckelvey, K. S. (2012). Estimating abundance\nof mountain lions from unstructured spatial sampling. The Journal of\nWildlife Management, 76(8), 1551–1561. https://doi.org/10.1002/jwmg.412\n\n\nRutledge, M. E., Sollmann, R., Washburn, B. E., Moorman, C. E., &\nDePerno, C. S. (2015). Using novel spatial mark-resight techniques to\nmonitor resident canada geese in a suburban environment. Wildlife\nResearch, 41, 447–453.\n\n\nSchlather, M., Malinowski, A., Menck, P. J., Oesting, M., &\nStrokorb, K. (2015). Analysis, simulation and prediction of multivariate\nrandom fields with package RandomFields. Journal of Statistical\nSoftware, 63(8). https://doi.org/10.18637/jss.v063.i08\n\n\nSeber, G. A. F. (1982). The estimation of animal abundance and\nrelated parameters. (2nd ed.). Griffin.\n\n\nSethi, S. A., Cook, G. M., Lemons, P., & Wenburg, J. (2014).\nGuidelines for MSAT and SNP panels that lead to high-quality data for\ngenetic mark–recapture studies. Canadian Journal of Zoology,\n92(6), 515–526. https://doi.org/10.1139/cjz-2013-0302\n\n\nSettlage, K. E., Van Manen, F. T., Clark, J. D., & King, T. L.\n(2008). Challenges of DNA‐based mark‐-recapture studies of american\nblack bears. Journal of Wildlife Management, 72,\n1035–1042. https://doi.org/10.2193/2006-472\n\n\nSollmann, R. (2024). Mt or not mt: Temporal variation in detection\nprobability in spatial capture-recapture and occupancy models. Peer\nCommunity Journal, 4. https://doi.org/10.24072/pcjournal.357\n\n\nSollmann, R., Gardner, B., & Belant, J. L. (2012). How does spatial\nstudy design influence density estimates from spatial capture-recapture\nmodels? PLoS ONE, 7(4), e34575. https://doi.org/10.1371/journal.pone.0034575\n\n\nSollmann, R., Gardner, B., Parsons, A. W., Stocking, J. J., McClintock,\nB. T., Simons, T. R., Pollock, K. H., & O’Connell, A. F. (2013). A\nspatial mark-resight model augmented with telemetry data.\nEcology, 94, 553–559.\n\n\nStephens, M. (2000). Dealing with label switching in mixture models.\nJournal of the Royal Statistical Society Series B, 62,\n795–809.\n\n\nStevenson, B. C., Fewster, R. M., & Sharma, K. (2021). Spatial\ncorrelation structures for detections of individuals in spatial\ncapture–recapture models. Biometrics, 78(3), 963–973.\nhttps://doi.org/10.1111/biom.13502\n\n\nSun, C. C., Fuller, A. K., & Royle, J. A. (2014). Trap configuration\nand spacing influences parameter estimates in spatial capture-recapture\nmodels. PLoS ONE, 9(2), e88025. https://doi.org/10.1371/journal.pone.0088025\n\n\nSutherland, C., Fuller, A. K., & Royle, J. A. (2015). Modelling\nnon-euclidean movement and landscape connectivity in highly structured\necological networks. Methods in Ecology and Evolution,\n6, 169–177.\n\n\nSutherland, C., Royle, J. A., & Linden, D. W. (2019). oSCR: A\nspatial capture-recapture R package for inference about\nspatial ecological processes. Ecography, 42,\n1459–1469. https://doi.org/10.1111/ecog.04551\n\n\nThompson, S. K. (2012). Sampling. In Wiley Series in Probability and\nStatistics. Wiley. https://doi.org/10.1002/9781118162934\n\n\nTobler, M. W., & Powell, G. V. N. (2013). Estimating jaguar\ndensities with camera traps: Problems with current designs and\nrecommendations for future studies. Biological Conservation,\n159, 109–118.\n\n\nTurek, D., Milleret, C., Ergon, T., Brøseth, H., Dupont, P., Bischof,\nR., & Valpine, P. de. (2021). Efficient estimation of large‐scale\nspatial capture–recapture models. Ecosphere, 12(2). https://doi.org/10.1002/ecs2.3385\n\n\nTwining, J. P., McFarlane, C., O’Meara, D., O’Reilly, C., Reyne, M.,\nMontgomery, W. I., Helyar, S., Tosh, D. G., & Augustine, B. C.\n(2022). A comparison of density estimation methods for monitoring marked\nand unmarked animal populations. Ecosphere, 13(10). https://doi.org/10.1002/ecs2.4165\n\n\nvan Etten, J. (2023). Gdistance: Distances and routes on\ngeographical grids. https://CRAN.R-project.org/package=gdistance\n\n\nVan Katwyk, K. E. (2014). Empirical validation of closed population\nabundance estimates and spatially explicit density estimates using a\ncensused population of north american red squirrels [Master’s\nthesis, University of Alberta]. https://redsquirrel.biology.ualberta.ca/wp-content/uploads/sites/32/2014/07/Van-Katwyk_Kristin_Spring-2014.pdf\n\n\nvan Winkle, W. (1975). Comparison of several probabilistic home-range\nmodels. Journal of Wildlife Management, 39, 118–123.\n\n\nWaits, L. P., & Paetkau, D. (2005). Noninvasive genetic sampling\ntools forwildlife biologists: A review of applications and\nrecommendations foraccurate data collection. Journal of Wildlife\nManagement, 69, 1419–1433.\n\n\nWalvoort, D. J. J., Brus, D. J., & Gruijter, J. J. de. (2010). An\nR package for spatial coverage sampling and random sampling\nfrom compact geographical strata by k-means. Computers &\nGeosciences, 36, 1261–1267. https://doi.org/10.1016/j.cageo.2010.04.005\n\n\nWhittington, J., Hebblewhite, M., & Chandler, R. (2018). Generalized\nspatial mark-resight models with an application to grizzly bears.\nJournal of Applied Ecology, 55, 157–168.\n\n\nWhoriskey, K., Martins, E. G., Auger‐Méthé, M., Gutowsky, L. F. G.,\nLennox, R. J., Cooke, S. J., Power, M., & Mills Flemming, J. (2019).\nCurrent and emerging statistical techniques for aquatic telemetry data:\nA guide to analysing spatially discrete animal detections. Methods\nin Ecology and Evolution, 10(7), 935–948. https://doi.org/10.1111/2041-210x.13188\n\n\nWilliams, B. K., Nichols, J. D., & Conroy, M. J. (2002).\nAnalysis and management of animal populations. Academic Press.\n\n\nWolters, M. A. (2015). A genetic algorithm for selection of fixed-size\nsubsets with application to design problems. Journal of Statistical\nSoftware, Code Snippets, 68, 1–18. https://doi.org/10.18637/jss.v068.c01\n\n\nWood, S. N. (2006). Generalized additive models: An introduction\nwith R. Chapman; Hall/CRC.\n\n\nWoodruff, S. P., Johnson, T. R., & Waits, L. P. (2014). Evaluating\nthe interaction of faecal pellet deposition rates and\n&lt;scp&gt;DNA&lt;/scp&gt; degradation rates to optimize sampling design\nfor &lt;scp&gt;DNA&lt;/scp&gt;‐based mark–recapture analysis of sonoran\npronghorn. Molecular Ecology Resources, 15(4),\n843–854. https://doi.org/10.1111/1755-0998.12362",
    "crumbs": [
      "Appendices",
      "References"
    ]
  }
]