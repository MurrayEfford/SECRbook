---
editor: 
  markdown: 
    wrap: 72
---

# Concepts and terminology {#sec-basics}

This chapter briefly introduces the concepts and terminology of SECR.
The following chapter gives a [simple example](#02-example.qmd).
Technical details are provided in the [Theory](#03-theory.qmd) chapters.

<br>    

## Motivation

<br>

:::: {layout="[ 57, 43 ]"}

::: {#first-column}

"Measurement of the size of animal populations is a full-time job and
should be treated as a worthy end in itself."

S. Charles Kendeigh (1944)

:::

::: {#second-column}
![](images/Kendeigh.png)  
*Photo: L.L.Getz*
:::

::::

Only a brave ecologist would make this claim today. Measuring animal populations 
is now justified in more practical terms - the urgent need for evidence to track 
biodiversity decline or to manage endangered or pest species or those we wish to
harvest sustainably. But we rather like Kendeigh's formulation, to which could
be added the lure of the statistical challenges. This book has no more to say 
about the diverse reasons for measuring populations: we will focus instead
on a particular toolkit.

Populations of some animal species can be censused by direct
observation, but many species are elusive or cryptic.
Surveying these species requires indirect methods, often using passive
devices such as traps or cameras that accumulate records over time.
Passive devices encounter animals as they move around. The scale of that 
movement is unknown, which creates uncertainty regarding the population that is
sampled. Furthermore,the number of observed individuals increases indefinitely 
as more and more peripheral individuals are encountered.

SECR cuts through this problem by modelling the spatial scale of detection
to obtain an unbiased estimate of the stationary density of individuals.

## Spatial population

\index{Activity centres} An animal population in SECR is a spatial point
pattern. Each point represents the location of an animal's activity
centre, often abbreviated AC\index{Activity centre}. Density is a property 
of the spatial point process of which the pattern
is one instance. The usual process model is a 2-dimensional Poisson
process\index{Poisson process (2-D)} (@fig-example). By fitting the SECR
model we can estimate the intensity surface of the Poisson process that
we represent by $D(\mathbf x)$ where $\mathbf x$ stands for the x,y
coordinates of a point. Density may be *homogeneous* (a flat surface,
with constant $D(\mathbf x)$) or *inhomogeneous* (varying).

An inhomogeneous intensity surface is considered to depend on a vector
of parameters $\phi$, hence $D(\mathbf x; \phi)$. For constant density
$\phi$ is a single value.

::: {.callout-note}
## Activity centre vs home range centre
'Activity centre' is often used in preference to 'home
range centre' because it appears more neutral: 'home range' implies
a particular pattern of behaviour and spatial familiarity. In
reality, SECR assumes the very pattern of behaviour (persistent
use) that distinguishes a home range, so it is safe to use the terms
interchangeably in this context.
:::

## Detectors

\index{Detectors} SECR uses sampling devices placed at known locations.
We need to recognise individuals whenever they are detected. The
accumulated detections of each known individual are its 'detection
history'. Device types differ according to how they affect animal
behaviour and the data they collect; each type corresponds to a
probability model, as covered later.

Detection may be entirely passive and non-invasive if individuals carry
unique natural marks (e.g., pelage patterns) or their DNA can be
sampled. Devices that record detections passively are "proximity
detectors". Proximity detectors may be split according to the
distribution of the number of detections per animal per occasion
(binary, Poisson, or binomial), with binary being the most common.

Animals without natural marks must be marked individually on their first
detection. This implies capture and handling. Only devices that hold an
animal until it is released can truly be called 'traps'. The probability
model for trapping must allow for exclusivity: an animal can be found at
only one trap on any occasion, and some traps ('single-catch' traps)
also exclude other individuals after the first. \index{Traps!single-catch}

Confusingly, **secr** uses 'traps' as a generic name for R objects holding
detector coordinates and other information. This software-specific
jargon should be avoided in publications.

| Device           | Marks                                    | Detector type               | Example                   |
|:---------------|:---------------|:---------------|:-----------------------|
| Automatic camera | natural marks: stripes and spots | proximity | tiger, @Royle2009         |
| Hair snag        | microsatellite DNA                       | proximity | grizzly bear, @ms00       |
| Cage trap        | numbered ear tag                         | single-catch trap  | brushtail possum, @ewcb05 |
| Ugglan trap      | numbered ear tag                         | multi-catch trap   | field vole, @Ergon2013    |
| Mist net         | numbered leg band                        | multi-catch trap   | red-eyed vireo, @be08     |

: Examples of SECR sampling devices {#tbl-deviceexamples tbl-colwidths="[15,23,20,42]" .sm}

## Sampling across time

For most vertebrates we expect population turnover (births, deaths, AC
movement) on a time scale of months or years. Population change is often
negligible over shorter spans (days or weeks, depending on the species).
Sampling over shorter spans can therefore treat the size and composition
of a population as fixed: it is said to be 'closed'. This greatly
simplifies analysis. We assume closure except when considering [breaches
of assumptions](#popnclosed). \index{Population closure}

A 'session' in **secr** parlance is a set of samples from a closed
population on one or more 'occasions' (possibly consecutive days).

For trap-type detectors there must be multiple occasions within a
session to obtain recaptures. For proximity-type detectors the role of
'occasions' is more subtle, and data may usually be collapsed to animal-
and detector-specific counts. The spatial pattern of binary detections
alone is sufficient to obtain an estimate of density [@edb09].

It is not necessary for all detectors to be used on all occasions.
Incomplete usage [and other variation in effort per occasion -- @ebm13]
may be recorded for each detector as an attribute of 'traps' objects.

Data collected across multiple sessions potentially include losses of
some individuals and gains of others. An open population model is the
natural way to go, e.g., @es20. However, the complexity of
open-population models can be avoided if sessions are treated as
independent in a 'multi-session' closed population analysis.

## Data structure

\index{Data entry} Data for a single SECR session comprise a
3-dimensional rectangular array with dimensions corresponding to known
animals, sampling occasions, and detectors. Data in each cell of the
array are usually binary (0/1) but may be integer counts \> 1 (e.g., if
binary data have been collapsed by occasion). In **secr**, an R Object of
class 'capthist' holds data in this form, along with the coordinates of
the detectors in its 'traps' attribute. The user constructs a capthist
object from text or spreadsheet input using data entry functions
described
[here](https://cran.r-project.org/web/packages/secr/vignettes/secr-datainput.pdf).

## Model fitting

A SECR model combines a model for the point process (the state model)
and a model for distance-dependent detection (the observation model).
Unbiased estimates of population density (and other parameters) by
jointly fitting the state and observation models.

### Distance-dependent detection

In order to estimate density from a sample we must account for the
sampling process. The process is inherently spatial: each animal is more
likely to be detected near its AC, and less likely to be detected far
away. Sampling filters the geographic locations of animals as indicated
in @fig-example.

```{r}
#| label: fig-example
#| eval: true
#| echo: false
#| out-width: 70%
#| fig-cap: Distance-dependent detection of uniformly distributed activity centres (open
#|   circles; filled if captured)
knitr::include_graphics('figures/example.png')
```

\index{Detection model!functions}

The true locations of animals are not known, and therefore the
distance-dependent probabilities cannot be calculated directly. The
model is fitted by marginalising (integrating over) animal locations.

Distance-dependent detection is represented by a 'detection function'
with intercept, scale, and possibly shape, determined by parameters to
be estimated[^01-basics-2].

[^01-basics-2]: All detection functions have intercept ($g_0$,
    $\lambda_0$) and scale ($\sigma$) parameters; some such as the
    hazard rate function have a further parameter that controls some
    aspect of shape.

```{r}
#| label: fig-detectionfunctions
#| echo: false
#| message: false
#| fig-width: 6
#| fig-height: 4.5
#| out-width: 75%
#| fig-cap: Some detection functions
par(mar=c(6,4,4,4), mgp=c(2.6,0.7, 0))
par(lwd = 1.5, bty = 'l', mar = c(4,4,2,6))
detectfnplot (detectfn = 0, c(0.7, 80), ylim=c(0,0.75), ylab = 'Probability', col = 'black',
              yaxs = "i", xlab = "Distance from AC (m)")
detectfnplot (detectfn = 2, c(0.7, 80), add = TRUE, col = 'blue')
detectfnplot (detectfn = 1, c(0.7, 80, 4 ), add = TRUE, col = 'orange')
legend (140, 0.7, legend = c('halfnormal','exponential','hazard rate'), lty=rep(1,3),
        col = c('black','blue','orange'), xpd = TRUE)
```

### Habitat

\index{Habitat mask}

SECR models include a map of potential habitat near the detectors. Here
'potential' means 'feasible locations for the AC of detected animals'.
Excluded are sites that are known *a priori* to be unoccupied, and sites
that are so distant that an animal centred there has negligible chance
of detection.

The habitat map is called a 'habitat mask' in **secr** and a 'state space'
in the various Bayesian implementations. It is commonly formed by adding
a constant-width buffer around the detectors. For computational
convenience the map is discretized as many small pixels. Spatial
covariates (vegetation type, elevation, etc.) may be attached to each
pixel for use in density models. The choice of buffer width and pixel
size are considered later.

### Link functions {#sec-link}

\index{Link function} \index{Link function!detection}
\index{Link function!density} A simple SECR model has three parameters:
density $D$, and the intercept $g_0$ and spatial scale $\sigma$ of the
detection function. Valid values of each parameter are restricted to
part of the number line (positive values for $D$ and $\sigma$, values
between zero and one for $g_0$). A straightforward way to constrain
estimates to valid values is to conduct maximization of the likelihood
on a transformed ('link') scale: at each evaluation the parameter value
is back transformed to the natural scale. The link function for all
commonly used parameters defaults to 'log' (for positive values) except
for g0 which defaults to 'logit' (for values between zero and one).

| Name     | Function               | Inverse              |
|:---------|:-----------------------|:---------------------|
| log      | $y = \log(x)$          | $\exp(y)$            |
| logit    | $y = \log[p/(1-p)]$    | $1 / [1 + \exp(-y)]$ |
| identity | $y=x$                  | $y$                  |
| cloglog  | $y = \log(-\log(1-p))$ | $1 -\exp(-\exp(y))$  |

: Link functions {#tbl-links .sm}

Working on a link scale is especially useful when the parameter is
itself a function of covariates. For example,
$\log (D) = \beta_0 + \beta_1 c \;$ for a log-linear function of a
spatially varying covariate $c$. The coefficients $\beta_0$ and
$\beta_1$ are estimated in place of $D$ per se.

We sometimes follow MARK [e.g., @cw] and use 'beta parameters' for
coefficients on the link scale and 'real parameters' for the core
parameters ($D, g_0, \lambda_0, \sigma$) on the natural scale.

### Estimation

\index{Likelihood!maximization}

There are several ways to estimate the parameters of the SECR
probability model, all of them computer-intensive. We focus on numerical
maximization of the log likelihood (@be08 and @sec-theory). The
likelihood integrates over the unknown locations of the animals'
activity centres. This is achieved in practice by summing over cells in
a discretized map of the [habitat](08-habitat.qmd).

In outline, a function to compute the log likelihood from a vector of
beta parameters is passed, along with the data, to an optimization
function. Optimization is iterative. For illustration, @fig-MLE shows
the sequence of likelihood evaluations with two maximization algorithms
when the parameter vector consists of only the intercept and spatial
scale of detection. Optimization returns the maximized log likelihood, a
vector of parameter values at the maximum, and the Hessian matrix from
which the variance-covariance matrix of the estimates may be obtained.

```{r}
#| label: fig-MLE
#| cache: true
#| message: false
#| echo: false
#| warning: false
#| fig-width: 7
#| fig-height: 3.5
#| out-width: 100%
#| fig-cap: Numerical maximization of conditional likelihood by two methods; triangle
#|   -- initial values; filled circle -- estimates. Both converge on the same estimates
#|   (dashed lines). Newton-Raphson is the default method in secr.
getL <- function(start = list(g0 = 0.2, sigma = 20), method = 'Newton-Raphson', 
                 skip = 3, ...) {
 tf <- tempfile()
 capture.output(fit <- secr.fit(captdata, CL = TRUE, start = start, method = method), file = tf)
 output <- readLines(tf)
 writeLines(output[(skip+1):(length(output)-1)], con = tf)  # drop lines
 df <- read.table(tf, header = TRUE)
 ne <- nrow(df)
 contour(x = x, y = y, z = LL, col = 'grey', nlevels = 20, add = TRUE, drawlabels = FALSE)
 g0 <- invlogit(df$g0)
 sigma <- exp(df$sigma)
 points(g0, sigma, type = 'o', pch = 21, bg = 'white', col = 'blue', cex = 1.3)
 points(g0[1], sigma[1], pch = 24, bg = 'white', col = 'blue', cex = 1.5)
 points(g0[ne], sigma[ne], pch = 16, col = 'blue', cex = 1.5)
 segments(g0[ne], 1, g0[ne], sigma[ne], lty = 2)
 segments(0.01, sigma[ne], g0[ne], sigma[ne], lty = 2)
 mtext(side = 3, line = 0.5, method)
 invisible(df)
}
x <- seq(0.15,0.40,0.01)
y <- seq(16,80,2)
LL <- LLsurface(secr.fit(captdata, CL = TRUE, trace = FALSE), 
                xval = x, yval = y, nlevels = 20, plot = FALSE)
par(mfrow = c(1,2), pty = 's', mar = c(4,4,2,2), mgp=c(2.4,0.7,0))
plot (1,1, type='n', xlim=c(0.15,0.4), ylim=c(16,80), log='xy', xlab = 'g0', ylab = 'sigma')
df <- getL(method = 'Newton-Raphson')
plot (1,1, type='n', xlim=c(0.15,0.4), ylim=c(16,80), log='xy', xlab = 'g0', ylab = 'sigma')
getL(method = 'Nelder-Mead')
```

Bayesian methods make use of algorithms that sample from a Markov chain
(MCMC) to approximate the posterior distribution of the parameters. MCMC
for abundance estimation faces the special problem that an unknown
number of individuals, at unknown locations, remain undetected. The
issue is addressed by data augmentation [@ry08] or using a semi-complete
data likelihood [@King2016]. \index{Bayesian methods}

Early anxiety about the suitability of MLE and asymptotic variances for
SECR with small samples appears to have been mistaken. @Royle2009
believed that "... the practical validity of these procedures cannot be
asserted in most situations involving small samples". This has not been
borne out by subsequent simulations. The concluding discussion of
@Gerber2015 is pertinent. @Palmero2023 reported that Bayesian methods
provide more precise estimates of density, but this appears to be an
artefact: it is a mistake to compare estimates from MLE models with
random $N(A)$ with estimates from Bayesian models with fixed $N(A)$. 
There is a further risk that the chosen Bayesian priors constrain the 
estimates.

<!-- : "Based on 20 studies that applied both frameworks, the performance of Bayesian methods was significantly better than MLE (P < 0.001)." -->

<!-- "Our results indicated that Bayesian methods performed better than MLE. This finding is consistent with Royle et al. (2009), who demonstrated that Bayesian methods cope better with small sample sizes." -->

The choice between Bayesian and frequentist (MLE) methods is now an
issue of convenience for most users:

* MLE provides fast and repeatable results for established models with a small 
to medium number of parameters. 
* Bayesian methods have an advantage for novel models and possibly for those 
with many parameters.

<!-- "As the underlying point process model is made more complex (e.g., containing interactions and conditional dependencies) the integration required for inference by integrated likelihood may become computationally prohibitive. [...] we feel that the biggest practical advantage apparent at this time has to do with the validity of inferences achieved by Bayesian analysis. In particular, classical inference procedures are asymptotic and as such their relevance to small sample situations is questionable. Conversely, Bayesian inferences do not rely on asymptotic arguments and are valid regardless of the sample size. There seems to be a prevailing view in statistical ecology that classical likelihood-based procedures are virtuous because of the availability of simple formulas and procedures for carrying out inference, such as calculating standard errors, doing model selection by AIC, and assessing goodness of fit. In large samples, this is an important practical benefit of classical likelihood-based inference. However, the practical validity of these procedures cannot be asserted in most situations involving small samples." -->

A further method, simulation and inverse prediction, has a niche use for
data from single-catch traps [@edr04; @Efford2023]. \index{Traps!single-catch}
